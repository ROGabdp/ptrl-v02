#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
================================================================================
US Tech Stock Buy Agent Training Script
================================================================================
è¨“ç·´ 10 éš»ç¾è‚¡ç§‘æŠ€è‚¡çš„ Buy Agentï¼Œç”¨æ–¼è¾¨è­˜ã€Œèµ·æ¼²é»ã€ã€‚

éšæ®µè¨“ç·´æµç¨‹ï¼š
- Phase 1 (Pre-training): æ•´åˆ 10 éš»è‚¡ç¥¨çš„æ­·å²æ•¸æ“šé€²è¡Œå¤§è¦æ¨¡é è¨“ç·´
- Phase 2 (Fine-tuning): é‡å°æ¯éš»è‚¡ç¥¨è¼‰å…¥é è¨“ç·´æ¬Šé‡ï¼Œé€²è¡Œå€‹è‚¡å¾®èª¿

ç›®æ¨™æ¨™çš„: NVDA, MSFT, AAPL, AMZN, META, AVGO, GOOGL, TSLA, NFLX, PLTR
åŸºæº–æŒ‡æ•¸: ^IXIC (Nasdaq Composite)

Buy Agent ç›®æ¨™: é æ¸¬æœªä¾† 20 å€‹äº¤æ˜“æ—¥å…§ï¼Œæœ€é«˜åƒ¹å ±é…¬ç‡æ˜¯å¦é”åˆ° +10% ä»¥ä¸Š

ä½œè€…ï¼šPhil Liang (Generated by Gemini)
æ—¥æœŸï¼š2026-01-18
================================================================================
"""

import os
import sys
import json
import glob
import shutil
import pickle
import multiprocessing
from datetime import datetime, date
from pathlib import Path

# Windows çµ‚ç«¯æ©Ÿ UTF-8 ç·¨ç¢¼è¨­å®š
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

import numpy as np
import pandas as pd
import yfinance as yf
import gymnasium as gym
from gymnasium import spaces
from ta.volatility import AverageTrueRange
from ta.momentum import RSIIndicator
from tqdm import tqdm

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, CallbackList
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
from stable_baselines3.common.logger import configure
from stable_baselines3.common.utils import get_schedule_fn

import warnings
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
warnings.filterwarnings('ignore')


# =============================================================================
# å¸¸é‡å®šç¾©
# =============================================================================
TICKERS = ["NVDA", "MSFT", "AAPL", "AMZN", "META", "AVGO", "GOOGL", "TSLA", "NFLX", "PLTR"]
BENCHMARK = "^IXIC"  # Nasdaq Composite

# è¨“ç·´/é©—è­‰æœŸé–“ (èˆ‡ train_v5_models.py ä¸€è‡´)
TRAIN_RANGES = [
    ("2000-01-01", "2017-10-15"),
    ("2023-10-16", "2025-12-31")
]
VAL_RANGE = ("2017-10-16", "2023-10-15")

WARMUP_DAYS = 250  # MA240 æš–æ©ŸæœŸ

# è¨“ç·´æ­¥æ•¸è¨­å®š
PRETRAIN_BUY_STEPS = 1_000_000
FINETUNE_BUY_STEPS = 750_000

# è·¯å¾‘è¨­å®š
PROJECT_PATH = os.path.dirname(os.path.abspath(__file__))
DATA_PATH = os.path.join(PROJECT_PATH, "data", "stocks")
CACHE_DIR = os.path.join(PROJECT_PATH, "data", "processed")
MODELS_PATH = os.path.join(PROJECT_PATH, "models_v5")

# ç‰¹å¾µåˆ—è¡¨ (ç§»é™¤ Volume ç›¸é—œ)
FEATURE_COLS = [
    'Norm_Close', 'Norm_Open', 'Norm_High', 'Norm_Low',
    'Norm_DC_Lower',
    'Norm_HA_Open', 'Norm_HA_High', 'Norm_HA_Low', 'Norm_HA_Close',
    'Norm_SuperTrend_1', 'Norm_SuperTrend_2',
    'Norm_RSI',
    # 'Norm_MFI',  # REMOVED - Volume ç›¸é—œ
    'Norm_ATR_Change',
    'Norm_RS_Ratio',
    'RS_ROC_5', 'RS_ROC_10', 'RS_ROC_20', 'RS_ROC_60', 'RS_ROC_120',
    'Feat_MA20_Slope',
    'Feat_Trend_Gap',
    'Feat_Bias_MA20',
    'Feat_Dist_MA60',
    'Feat_Dist_MA240',
    # 'Feat_Vol_Ratio',  # REMOVED - Volume ç›¸é—œ
    # æ–°å¢æ³¢å‹•ç‡ä»£ç†
    'Feat_ATR_Ratio',    # ATR(5) / ATR(20)
    'Feat_HV20',         # 20æ—¥å¹´åŒ–æ­·å²æ³¢å‹•ç‡
    'Feat_Price_Pos',    # 20æ—¥åƒ¹æ ¼å€é–“ç™¾åˆ†æ¯”
    'Norm_K', 'Norm_D',
    'Norm_DIF', 'Norm_MACD', 'Norm_OSC',
]


# =============================================================================
# 1. æ•¸æ“šè¼‰å…¥æ¨¡çµ„ (åœ°ç«¯ CSV å¢é‡æ›´æ–°)
# =============================================================================
def load_or_update_local_csv(ticker: str, start_date: str = "2000-01-01") -> pd.DataFrame:
    """
    è¼‰å…¥æœ¬åœ° CSV è³‡æ–™ï¼Œè‹¥ä¸å­˜åœ¨æˆ–éæœŸå‰‡å¾ Yahoo Finance ä¸‹è¼‰ä¸¦æ›´æ–°
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç¢¼ (e.g., "NVDA", "^IXIC")
        start_date: è³‡æ–™èµ·å§‹æ—¥æœŸ
    
    Returns:
        pd.DataFrame: è‚¡åƒ¹è³‡æ–™ (OHLCV)
    """
    os.makedirs(DATA_PATH, exist_ok=True)
    
    # æ¸…ç† ticker åç¨±ä½œç‚ºæª”å
    safe_ticker = ticker.replace("^", "").replace(".", "_")
    csv_path = os.path.join(DATA_PATH, f"{safe_ticker}.csv")
    
    today = date.today()
    df = None
    need_update = True
    last_date = None
    
    # 1. å˜—è©¦è®€å–ç¾æœ‰ CSV
    if os.path.exists(csv_path):
        try:
            df = pd.read_csv(csv_path, parse_dates=['Date'], index_col='Date')
            last_date = df.index.max().date()
            
            # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–° (æœ€è¿‘è³‡æ–™è¶…é 1 å¤©)
            days_diff = (today - last_date).days
            if days_diff <= 1:  # é€±æœ«å¯èƒ½ 2 å¤©ï¼Œä½†æˆ‘å€‘æ”¾å¯¬åˆ° 1 å¤©
                need_update = False
                print(f"  âœ… {ticker} (Local, up to {last_date}): {len(df)} rows")
            else:
                print(f"  ğŸ“¥ {ticker} éœ€è¦æ›´æ–° ({last_date} â†’ {today})")
        except Exception as e:
            print(f"  âš ï¸ {ticker} CSV è®€å–éŒ¯èª¤: {e}")
            df = None
    else:
        print(f"  ğŸ“¥ {ticker} é¦–æ¬¡ä¸‹è¼‰...")
    
    # 2. ä¸‹è¼‰ç¼ºå¤±çš„æ•¸æ“š
    if need_update:
        try:
            if df is not None and last_date is not None:
                # å¢é‡æ›´æ–°ï¼šåªä¸‹è¼‰æœ€å¾Œæ—¥æœŸä¹‹å¾Œçš„è³‡æ–™
                download_start = (last_date + pd.Timedelta(days=1)).strftime('%Y-%m-%d')
            else:
                # å…¨æ–°ä¸‹è¼‰
                download_start = start_date
            
            new_data = yf.download(
                ticker, 
                start=download_start, 
                auto_adjust=True, 
                progress=False
            )
            
            if len(new_data) > 0:
                # è™•ç† MultiIndex columns (yfinance æœ‰æ™‚æœƒè¿”å›)
                if isinstance(new_data.columns, pd.MultiIndex):
                    new_data.columns = new_data.columns.get_level_values(0)
                
                new_data.index.name = 'Date'
                
                # åˆä½µèˆŠè³‡æ–™å’Œæ–°è³‡æ–™
                if df is not None:
                    df = pd.concat([df, new_data])
                    df = df[~df.index.duplicated(keep='last')]
                    df = df.sort_index()
                else:
                    df = new_data
                
                # å„²å­˜åˆ° CSV
                df.to_csv(csv_path)
                print(f"  âœ… {ticker} æ›´æ–°å®Œæˆ: {len(df)} rows ({df.index[0].date()} ~ {df.index[-1].date()})")
            elif df is None:
                print(f"  âŒ {ticker} ç„¡æ³•ä¸‹è¼‰ä»»ä½•è³‡æ–™")
                return None
        except Exception as e:
            print(f"  âŒ {ticker} ä¸‹è¼‰éŒ¯èª¤: {e}")
            if df is None:
                return None
    
    return df


def fetch_all_stock_data(start_date: str = "2000-01-01") -> dict:
    """
    ä¸‹è¼‰/æ›´æ–°æ‰€æœ‰è‚¡ç¥¨è³‡æ–™ (10 éš»è‚¡ç¥¨ + åŸºæº–æŒ‡æ•¸)
    
    Returns:
        dict: {ticker: DataFrame}
    """
    print("=" * 60)
    print("ğŸ“¥ è¼‰å…¥/æ›´æ–° è‚¡ç¥¨è³‡æ–™")
    print("=" * 60)
    
    all_data = {}
    
    # 1. è¼‰å…¥åŸºæº–æŒ‡æ•¸
    print(f"\n[åŸºæº–æŒ‡æ•¸] {BENCHMARK}")
    benchmark_df = load_or_update_local_csv(BENCHMARK, start_date)
    if benchmark_df is not None:
        all_data[BENCHMARK] = benchmark_df
    else:
        print(f"  âŒ ç„¡æ³•è¼‰å…¥åŸºæº–æŒ‡æ•¸ {BENCHMARK}ï¼Œçµ‚æ­¢ç¨‹å¼")
        sys.exit(1)
    
    # 2. è¼‰å…¥å€‹è‚¡
    print(f"\n[ç›®æ¨™è‚¡ç¥¨] {len(TICKERS)} éš»")
    for ticker in TICKERS:
        df = load_or_update_local_csv(ticker, start_date)
        if df is not None and len(df) > WARMUP_DAYS:
            all_data[ticker] = df
        else:
            print(f"  âš ï¸ {ticker} è³‡æ–™ä¸è¶³ (éœ€è¦ > {WARMUP_DAYS} å¤©)")
    
    print(f"\n[Summary] æˆåŠŸè¼‰å…¥ {len(all_data)} å€‹æ¨™çš„")
    return all_data


# =============================================================================
# 2. ç‰¹å¾µå·¥ç¨‹
# =============================================================================
def calculate_heikin_ashi(df: pd.DataFrame) -> pd.DataFrame:
    """è¨ˆç®— Heikin Ashi Kç·š"""
    ha_close = (df['Open'] + df['High'] + df['Low'] + df['Close']) / 4
    ha_open = [df['Open'].iloc[0]]
    for i in range(1, len(df)):
        ha_open.append((ha_open[-1] + ha_close.iloc[i-1]) / 2)
    ha_open = pd.Series(ha_open, index=df.index)
    return pd.DataFrame({
        'HA_open': ha_open,
        'HA_high': pd.concat([df['High'], ha_open, ha_close], axis=1).max(axis=1),
        'HA_low': pd.concat([df['Low'], ha_open, ha_close], axis=1).min(axis=1),
        'HA_close': ha_close
    })


def calculate_supertrend(df: pd.DataFrame, length: int = 14, multiplier: float = 3.0) -> pd.DataFrame:
    """è¨ˆç®— SuperTrend æŒ‡æ¨™"""
    atr = AverageTrueRange(df['High'], df['Low'], df['Close'], window=length).average_true_range()
    atr = atr.fillna(method='bfill')
    hl2 = (df['High'] + df['Low']) / 2
    basic_upper = hl2 + multiplier * atr
    basic_lower = hl2 - multiplier * atr
    
    final_upper = basic_upper.copy()
    final_lower = basic_lower.copy()
    trend = np.zeros(len(df))
    
    for i in range(1, len(df)):
        if basic_upper.iloc[i] < final_upper.iloc[i-1] or df['Close'].iloc[i-1] > final_upper.iloc[i-1]:
            final_upper.iloc[i] = basic_upper.iloc[i]
        else:
            final_upper.iloc[i] = final_upper.iloc[i-1]
            
        if basic_lower.iloc[i] > final_lower.iloc[i-1] or df['Close'].iloc[i-1] < final_lower.iloc[i-1]:
            final_lower.iloc[i] = basic_lower.iloc[i]
        else:
            final_lower.iloc[i] = final_lower.iloc[i-1]
        
        if df['Close'].iloc[i] > final_upper.iloc[i-1]:
            trend[i] = 1
        elif df['Close'].iloc[i] < final_lower.iloc[i-1]:
            trend[i] = -1
        else:
            trend[i] = trend[i-1]
    
    return pd.DataFrame({'SUPERT_': np.where(trend == 1, final_lower, final_upper)}, index=df.index)


def calculate_features(df_in: pd.DataFrame, benchmark_df: pd.DataFrame, 
                       ticker: str = "Unknown", use_cache: bool = True) -> pd.DataFrame:
    """
    è¨ˆç®—ç‰¹å¾µ (ç§»é™¤ Volume ç›¸é—œï¼Œæ–°å¢æ³¢å‹•ç‡ä»£ç†)
    
    Args:
        df_in: åŸå§‹ OHLCV è³‡æ–™
        benchmark_df: åŸºæº–æŒ‡æ•¸è³‡æ–™ (^IXIC)
        ticker: è‚¡ç¥¨ä»£ç¢¼
        use_cache: æ˜¯å¦ä½¿ç”¨å¿«å–
    
    Returns:
        pd.DataFrame: åŒ…å«æ‰€æœ‰ç‰¹å¾µçš„ DataFrame
    """
    os.makedirs(CACHE_DIR, exist_ok=True)
    cache_path = os.path.join(CACHE_DIR, f"{ticker.replace('^', '_').replace('.', '_')}_features_ustech.pkl")
    
    if use_cache and os.path.exists(cache_path):
        try:
            with open(cache_path, 'rb') as f:
                cached_df = pickle.load(f)
            
            # æª¢æŸ¥å¿«å–æ˜¯å¦æ¶µè“‹è¼¸å…¥æ•¸æ“šçš„æ—¥æœŸç¯„åœ
            input_last_date = df_in.index.max()
            cached_last_date = cached_df.index.max()
            
            if cached_last_date >= input_last_date:
                print(f"  [Cache] Loading features for {ticker} (up to {cached_last_date.strftime('%Y-%m-%d')})...")
                return cached_df
            else:
                print(f"  [Cache] Invalidating stale cache for {ticker}: {cached_last_date.strftime('%Y-%m-%d')} < {input_last_date.strftime('%Y-%m-%d')}")
        except Exception as e:
            print(f"  [Cache] Error loading cache for {ticker}: {e}")
    
    print(f"  [Compute] Generating features for {ticker}...")
    df = df_in.copy()
    
    # =========================================================================
    # åŸºç¤æŠ€è¡“æŒ‡æ¨™
    # =========================================================================
    df['DC_Upper'] = df['High'].rolling(20).max().shift(1).fillna(method='bfill')
    df['DC_Lower'] = df['Low'].rolling(20).min().shift(1).fillna(method='bfill')
    df['DC_Upper_10'] = df['High'].rolling(10).max().shift(1).fillna(method='bfill')
    
    # ATR (å¤šé€±æœŸ)
    df['ATR'] = AverageTrueRange(df['High'], df['Low'], df['Close'], window=10).average_true_range()
    df['ATR_5'] = AverageTrueRange(df['High'], df['Low'], df['Close'], window=5).average_true_range()
    df['ATR_20'] = AverageTrueRange(df['High'], df['Low'], df['Close'], window=20).average_true_range()
    
    # RSI (ä¸ä½¿ç”¨ MFIï¼Œå› ç‚ºå®ƒä¾è³´ Volume)
    df['RSI'] = RSIIndicator(df['Close'], window=14).rsi()
    
    # Heikin Ashi
    ha = calculate_heikin_ashi(df)
    df['HA_Open'] = ha['HA_open']
    df['HA_High'] = ha['HA_high']
    df['HA_Low'] = ha['HA_low']
    df['HA_Close'] = ha['HA_close']
    
    # SuperTrend
    df['SuperTrend_1'] = calculate_supertrend(df, 14, 2.0).iloc[:, 0]
    df['SuperTrend_2'] = calculate_supertrend(df, 21, 1.0).iloc[:, 0]
    
    # =========================================================================
    # æ­£è¦åŒ– (ä½¿ç”¨ Donchian Channel ä½œç‚ºåˆ†æ¯)
    # =========================================================================
    base_price = df['DC_Upper'].replace(0, np.nan).fillna(method='bfill')
    
    for col in ['Close', 'Open', 'High', 'Low', 'DC_Lower', 
                'HA_Open', 'HA_High', 'HA_Low', 'HA_Close', 
                'SuperTrend_1', 'SuperTrend_2']:
        df[f'Norm_{col}'] = df[col] / base_price
    
    df['Norm_RSI'] = df['RSI'] / 100.0
    df['Norm_ATR_Change'] = (df['ATR'] / df['ATR'].shift(1)).fillna(1.0)
    
    # =========================================================================
    # å‡ç·šèˆ‡è¶¨å‹¢ç‰¹å¾µ
    # =========================================================================
    df['MA20'] = df['Close'].rolling(20).mean()
    df['MA60'] = df['Close'].rolling(60).mean()
    df['MA120'] = df['Close'].rolling(120).mean()
    df['MA240'] = df['Close'].rolling(240).mean()
    
    # (1) MA20 çŸ­æœŸè¶¨å‹¢å‹•èƒ½
    df['Feat_MA20_Slope'] = (df['MA20'] / df['MA20'].shift(1) - 1).fillna(0)
    
    # (2) MA20 vs MA240 å¸‚å ´é«”åˆ¶
    df['Feat_Trend_Gap'] = ((df['MA20'] - df['MA240']) / df['MA240']).fillna(0)
    
    # (3) MA20 çŸ­æœŸä¹–é›¢
    df['Feat_Bias_MA20'] = ((df['Close'] - df['MA20']) / df['MA20']).fillna(0)
    
    # (4) MA60 å­£ç·šæ”¯æ’è·é›¢
    df['Feat_Dist_MA60'] = ((df['Close'] - df['MA60']) / df['MA60']).fillna(0)
    
    # (5) MA240 å¹´ç·šç”Ÿå‘½ç·šä½ç½®
    df['Feat_Dist_MA240'] = ((df['Close'] - df['MA240']) / df['MA240']).fillna(0)
    
    # =========================================================================
    # æ–°å¢æ³¢å‹•ç‡ä»£ç†æŒ‡æ¨™ (æ›¿ä»£ Volume)
    # =========================================================================
    # (1) ATR Ratio (5/20) - åµæ¸¬åƒ¹æ ¼æ³¢å‹•æ“´å¼µ
    df['Feat_ATR_Ratio'] = (df['ATR_5'] / (df['ATR_20'] + 1e-9)).fillna(1.0)
    
    # (2) 20æ—¥å¹´åŒ–æ­·å²æ³¢å‹•ç‡ (HV20)
    log_returns = np.log(df['Close'] / df['Close'].shift(1))
    df['Feat_HV20'] = log_returns.rolling(20).std() * np.sqrt(252)
    # æ­£è¦åŒ–åˆ° 0-1 ç¯„åœ (å‡è¨­ HV ç¯„åœ 0-100%)
    df['Feat_HV20'] = (df['Feat_HV20'] / 1.0).clip(0, 1).fillna(0.3)
    
    # (3) 20æ—¥åƒ¹æ ¼å€é–“ç™¾åˆ†æ¯” (Price_Pos)
    high_20 = df['High'].rolling(20).max()
    low_20 = df['Low'].rolling(20).min()
    df['Feat_Price_Pos'] = ((df['Close'] - low_20) / (high_20 - low_20 + 1e-9)).fillna(0.5)
    
    # =========================================================================
    # KD èˆ‡ MACD ç‰¹å¾µ
    # =========================================================================
    # Stochastic KD (9, 3)
    low_min_9 = df['Low'].rolling(9).min()
    high_max_9 = df['High'].rolling(9).max()
    rsv = ((df['Close'] - low_min_9) / (high_max_9 - low_min_9 + 1e-9)) * 100
    df['K_raw'] = rsv.rolling(3).mean()
    df['D_raw'] = df['K_raw'].rolling(3).mean()
    df['Norm_K'] = (df['K_raw'] / 100.0).fillna(0.5)
    df['Norm_D'] = (df['D_raw'] / 100.0).fillna(0.5)
    
    # MACD (12, 26, 9)
    ema_12 = df['Close'].ewm(span=12, adjust=False).mean()
    ema_26 = df['Close'].ewm(span=26, adjust=False).mean()
    df['DIF'] = ema_12 - ema_26
    df['MACD_Signal'] = df['DIF'].ewm(span=9, adjust=False).mean()
    df['OSC'] = df['DIF'] - df['MACD_Signal']
    
    # æ­£è¦åŒ–ï¼šé™¤ä»¥æ”¶ç›¤åƒ¹
    df['Norm_DIF'] = (df['DIF'] / df['Close']).fillna(0)
    df['Norm_MACD'] = (df['MACD_Signal'] / df['Close']).fillna(0)
    df['Norm_OSC'] = (df['OSC'] / df['Close']).fillna(0)
    
    # =========================================================================
    # ç›¸å°å¼·åº¦ (ä½¿ç”¨ ^IXIC ä½œç‚ºåŸºæº–)
    # =========================================================================
    if benchmark_df is not None:
        bench_close = benchmark_df['Close'].reindex(df.index).fillna(method='ffill')
        df['RS_Raw'] = df['Close'] / bench_close
        rs_min = df['RS_Raw'].rolling(250).min()
        rs_max = df['RS_Raw'].rolling(250).max()
        df['Norm_RS_Ratio'] = ((df['RS_Raw'] - rs_min) / ((rs_max - rs_min).replace(0, np.nan).fillna(1.0) + 1e-9)).fillna(0.5)
        
        for period in [5, 10, 20, 60, 120]:
            df[f'RS_ROC_{period}'] = df['RS_Raw'].pct_change(period).fillna(0)
    else:
        df['Norm_RS_Ratio'] = 0.5
        for period in [5, 10, 20, 60, 120]:
            df[f'RS_ROC_{period}'] = 0.0
    
    # =========================================================================
    # ç›®æ¨™æ¨™ç±¤ (20 å¤©æœ€é«˜åƒ¹å ±é…¬ç‡)
    # =========================================================================
    df['Signal_Buy_Filter'] = df['High'] > df['DC_Upper_10']
    
    # Next_20d_Max: æœªä¾† 20 äº¤æ˜“æ—¥å…§çš„æœ€é«˜åƒ¹å ±é…¬ç‡
    df['Next_20d_Max'] = df['High'].shift(-1).iloc[::-1].rolling(20, min_periods=20).max().iloc[::-1] / df['Close'] - 1
    
    # =========================================================================
    # ç§»é™¤ NaN
    # =========================================================================
    # ç§»é™¤å›  MA240 é€ æˆçš„å‰ 250 å¤©ç©ºå€¼
    df = df.dropna(subset=['MA240'])
    # ç§»é™¤å…¶é¤˜ NaN (æ’é™¤ç›®æ¨™æ¬„ä½)
    feature_cols_check = [c for c in FEATURE_COLS if c in df.columns]
    df = df.dropna(subset=feature_cols_check)
    
    # =========================================================================
    # å¿«å–
    # =========================================================================
    if use_cache:
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(df, f)
        except:
            pass
    
    return df


# =============================================================================
# 3. RL ç’°å¢ƒå®šç¾©
# =============================================================================
class BuyEnvHybridV5US(gym.Env):
    """
    Buy Agent ç’°å¢ƒ - ç¾è‚¡ç§‘æŠ€è‚¡ç‰ˆæœ¬
    
    ç›®æ¨™: é æ¸¬æœªä¾† 20 å¤©å…§æœ€é«˜åƒ¹å ±é…¬ç‡ >= 10%
    çå‹µçµæ§‹ (å°ç¨±):
    - è²·å° (action=1, æ¼²å¹…â‰¥10%): +1.0
    - è²·éŒ¯ (action=1, æ¼²å¹…<10%): 0.0
    - éŒ¯é (action=0, æ¼²å¹…â‰¥10%): 0.0
    - æ­£ç¢ºè¿´é¿ (action=0, æ¼²å¹…<10%): +1.0
    """
    
    def __init__(self, data_dict: dict, is_training: bool = True, balance_tickers: bool = False):
        """
        Args:
            data_dict: {ticker: DataFrame} åŒ…å«ç‰¹å¾µçš„è³‡æ–™
            is_training: æ˜¯å¦ç‚ºè¨“ç·´æ¨¡å¼ (å½±éŸ¿æŠ½æ¨£æ–¹å¼)
            balance_tickers: æ˜¯å¦å¹³è¡¡å„ Ticker çš„æŠ½æ¨£æ©Ÿç‡ (é è¨“ç·´æ™‚æ‡‰ç‚º True)
        """
        super().__init__()
        
        self.is_training = is_training
        self.balance_tickers = balance_tickers
        
        # å„²å­˜æ¨£æœ¬
        self.samples = []           # æ‰€æœ‰æ¨£æœ¬
        self.pos_samples = []       # æ­£æ¨£æœ¬ (>= 10%)
        self.neg_samples = []       # è² æ¨£æœ¬ (< 10%)
        self.ticker_samples = {}    # ä¾ Ticker åˆ†çµ„çš„æ¨£æœ¬
        
        for ticker, df in data_dict.items():
            df = df.dropna(subset=['Next_20d_Max'])
            
            if len(df) == 0:
                continue
            
            states = df[FEATURE_COLS].values.astype(np.float32)
            future_rets = df['Next_20d_Max'].values.astype(np.float32)
            
            ticker_pos = []
            ticker_neg = []
            
            for i in range(len(df)):
                sample = (states[i], future_rets[i], ticker)
                self.samples.append(sample)
                
                if future_rets[i] >= 0.10:
                    self.pos_samples.append(sample)
                    ticker_pos.append(sample)
                else:
                    self.neg_samples.append(sample)
                    ticker_neg.append(sample)
            
            self.ticker_samples[ticker] = {'pos': ticker_pos, 'neg': ticker_neg}
        
        print(f"[BuyEnvV5US] Total: {len(self.samples)} | Pos (â‰¥10%): {len(self.pos_samples)} | Neg: {len(self.neg_samples)}")
        print(f"             Tickers: {list(self.ticker_samples.keys())}")
        
        self.action_space = spaces.Discrete(2)
        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(len(FEATURE_COLS),), dtype=np.float32)
        self.current_sample = None
    
    def reset(self, seed=None, options=None):
        if self.is_training:
            if self.balance_tickers and self.ticker_samples:
                # é è¨“ç·´æ¨¡å¼ï¼šå…ˆéš¨æ©Ÿé¸ Tickerï¼Œå† 50/50 é¡åˆ¥å¹³è¡¡
                ticker = np.random.choice(list(self.ticker_samples.keys()))
                ticker_data = self.ticker_samples[ticker]
                
                if np.random.rand() < 0.5 and ticker_data['pos']:
                    self.current_sample = ticker_data['pos'][np.random.randint(len(ticker_data['pos']))]
                elif ticker_data['neg']:
                    self.current_sample = ticker_data['neg'][np.random.randint(len(ticker_data['neg']))]
                else:
                    self.current_sample = self.samples[np.random.randint(len(self.samples))]
            else:
                # å¾®èª¿æ¨¡å¼ï¼š50/50 é¡åˆ¥å¹³è¡¡
                if np.random.rand() < 0.5 and self.pos_samples:
                    self.current_sample = self.pos_samples[np.random.randint(len(self.pos_samples))]
                elif self.neg_samples:
                    self.current_sample = self.neg_samples[np.random.randint(len(self.neg_samples))]
                else:
                    self.current_sample = self.samples[np.random.randint(len(self.samples))]
        else:
            # é©—è­‰æ¨¡å¼ï¼šä½¿ç”¨çœŸå¯¦åˆ†ä½ˆ
            self.current_sample = self.samples[np.random.randint(len(self.samples))]
        
        return self.current_sample[0], {}
    
    def step(self, action):
        _, max_ret, _ = self.current_sample
        is_success = max_ret >= 0.10  # 20 å¤©å…§æœ€é«˜é” +10%
        
        # å°ç¨±çå‹µçµæ§‹
        if action == 1:  # BUY
            reward = 1.0 if is_success else 0.0
        else:  # WAIT
            reward = 0.0 if is_success else 1.0
        
        return self.current_sample[0], reward, True, False, {}


# =============================================================================
# 4. è¼”åŠ©å‡½å¼
# =============================================================================
def filter_by_ranges(df: pd.DataFrame, ranges: list) -> pd.DataFrame:
    """æ ¹æ“šæ—¥æœŸç¯„åœåˆ—è¡¨éæ¿¾ DataFrame"""
    mask = pd.Series(False, index=df.index)
    for start, end in ranges:
        mask |= (df.index >= pd.Timestamp(start)) & (df.index <= pd.Timestamp(end))
    return df[mask]


def get_valid_train_ranges(ticker: str, df: pd.DataFrame, train_ranges: list) -> list:
    """
    å–å¾—æœ‰æ•ˆçš„è¨“ç·´å€é–“ (æ’é™¤è‚¡ç¥¨å°šæœªä¸Šå¸‚æˆ–æš–æ©ŸæœŸä¸è¶³çš„éƒ¨åˆ†)
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç¢¼
        df: åŸå§‹è³‡æ–™
        train_ranges: é è¨­è¨“ç·´å€é–“åˆ—è¡¨
    
    Returns:
        list: æœ‰æ•ˆçš„è¨“ç·´å€é–“åˆ—è¡¨
    """
    if len(df) == 0:
        return []
    
    first_valid_date = df.index[0] + pd.Timedelta(days=WARMUP_DAYS)
    
    valid_ranges = []
    for start, end in train_ranges:
        start_dt = pd.Timestamp(start)
        end_dt = pd.Timestamp(end)
        
        if end_dt < first_valid_date:
            continue  # æ•´å€‹å€é–“ç„¡æ•ˆ
        
        actual_start = max(start_dt, first_valid_date)
        if actual_start < end_dt:
            valid_ranges.append((actual_start.strftime('%Y-%m-%d'), end))
    
    if len(valid_ranges) < len(train_ranges):
        print(f"  âš ï¸ {ticker}: éƒ¨åˆ†è¨“ç·´å€é–“ç„¡æ•ˆ (ä¸Šå¸‚æ—¥æœŸæ™šæˆ–æš–æ©ŸæœŸä¸è¶³)")
    
    return valid_ranges


def clear_cache():
    """æ¸…é™¤ç‰¹å¾µå¿«å–"""
    print("\n[Cache] Clearing feature cache...")
    if os.path.exists(CACHE_DIR):
        pkl_files = glob.glob(os.path.join(CACHE_DIR, "*_ustech.pkl"))
        for f in pkl_files:
            try:
                os.remove(f)
                print(f"  Deleted: {os.path.basename(f)}")
            except Exception as e:
                print(f"  Failed: {os.path.basename(f)} - {e}")


# =============================================================================
# 5. è¨“ç·´æµç¨‹
# =============================================================================
def run_pretraining(train_data: dict, models_path: str):
    """
    Phase 1: Pre-training on combined 10-stock data
    
    Args:
        train_data: {ticker: DataFrame} è¨“ç·´è³‡æ–™
        models_path: æ¨¡å‹å„²å­˜è·¯å¾‘
    """
    print("\n" + "=" * 60)
    print("Phase 1: Pre-training (Combined 10 US Tech Stocks)")
    print("=" * 60)
    print(f"  Steps: {PRETRAIN_BUY_STEPS:,}")
    print(f"  Tickers: {list(train_data.keys())}")
    print(f"  Output: {models_path}")
    print("=" * 60)
    
    os.makedirs(models_path, exist_ok=True)
    os.makedirs(os.path.join(models_path, "best_pretrain"), exist_ok=True)
    
    tensorboard_log = "./tensorboard_logs/"
    os.makedirs(tensorboard_log, exist_ok=True)
    
    n_envs = min(8, max(1, multiprocessing.cpu_count() - 1))
    print(f"[System] CPU cores: {multiprocessing.cpu_count()}, Using {n_envs} envs")
    
    ppo_params = {
        "learning_rate": 0.0001,
        "n_steps": max(128, 2048 // n_envs),
        "batch_size": 512,
        "ent_coef": 0.01,
        "device": "cpu",
        "policy_kwargs": dict(net_arch=[64, 64, 64]),
        "verbose": 1,
        "tensorboard_log": tensorboard_log
    }
    
    # å»ºç«‹è¨“ç·´ç’°å¢ƒ (balance_tickers=True ç¢ºä¿å„ Ticker æŠ½æ¨£å‡ç­‰)
    print("\nğŸ›’ Training Buy Agent (Pre-training on all tickers)...")
    buy_env = make_vec_env(
        BuyEnvHybridV5US, n_envs=n_envs, vec_env_cls=SubprocVecEnv,
        env_kwargs={'data_dict': train_data, 'is_training': True, 'balance_tickers': True}
    )
    
    # å»ºç«‹è©•ä¼°ç’°å¢ƒ
    eval_buy_env = make_vec_env(
        BuyEnvHybridV5US, n_envs=1, vec_env_cls=DummyVecEnv,
        env_kwargs={'data_dict': train_data, 'is_training': False, 'balance_tickers': False}
    )
    
    buy_model = PPO("MlpPolicy", buy_env, **ppo_params)
    
    # Callbacks
    buy_callbacks = CallbackList([
        CheckpointCallback(save_freq=100000, save_path=models_path, name_prefix="ppo_buy_base_ustech"),
        EvalCallback(eval_buy_env, 
                     best_model_save_path=os.path.join(models_path, "best_pretrain"),
                     log_path="./logs/", eval_freq=20000, n_eval_episodes=100, 
                     deterministic=True)
    ])
    
    buy_model.learn(total_timesteps=PRETRAIN_BUY_STEPS, callback=buy_callbacks, 
                    tb_log_name="buy_pretrain_us_tech")
    
    # è¤‡è£½ best model
    best_buy_path = os.path.join(models_path, "best_pretrain", "best_model.zip")
    buy_base_path = os.path.join(models_path, "ppo_buy_base_us_tech.zip")
    if os.path.exists(best_buy_path):
        shutil.copy(best_buy_path, buy_base_path)
        print(f"[Pre-train] âœ… Buy Agent: Copied BEST model to {buy_base_path}")
    else:
        buy_model.save(buy_base_path.replace('.zip', ''))
        print(f"[Pre-train] âš ï¸ Buy Agent: Best model not found, saved last step model")
    
    buy_env.close()
    eval_buy_env.close()
    
    print("[System] Pre-training Completed.")


def run_finetuning(ticker: str, ticker_train_data: dict, ticker_eval_data: dict, 
                   models_path: str, manifest: dict):
    """
    Phase 2: Fine-tuning for individual ticker
    
    Args:
        ticker: è‚¡ç¥¨ä»£ç¢¼
        ticker_train_data: {ticker: DataFrame} è©²è‚¡ç¥¨çš„è¨“ç·´è³‡æ–™
        ticker_eval_data: {ticker: DataFrame} è©²è‚¡ç¥¨çš„é©—è­‰è³‡æ–™
        models_path: æ¨¡å‹å„²å­˜è·¯å¾‘
        manifest: æ¨¡å‹æ¸…å–®å­—å…¸ (æœƒè¢«æ›´æ–°)
    """
    print(f"\n[Fine-tune] {ticker}")
    print("-" * 40)
    
    ticker_model_dir = os.path.join(models_path, "finetuned", ticker)
    os.makedirs(ticker_model_dir, exist_ok=True)
    os.makedirs(os.path.join(ticker_model_dir, "best"), exist_ok=True)
    
    tensorboard_log = "./tensorboard_logs/"
    
    # è¼‰å…¥é è¨“ç·´æ¬Šé‡
    buy_base_path = os.path.join(models_path, "ppo_buy_base_us_tech.zip")
    if not os.path.exists(buy_base_path):
        print(f"  âŒ Base model not found: {buy_base_path}")
        return
    
    n_envs = min(4, max(1, multiprocessing.cpu_count() - 1))
    
    # å»ºç«‹ç’°å¢ƒ
    buy_env = make_vec_env(
        BuyEnvHybridV5US, n_envs=n_envs, vec_env_cls=SubprocVecEnv,
        env_kwargs={'data_dict': ticker_train_data, 'is_training': True, 'balance_tickers': False}
    )
    
    eval_env = make_vec_env(
        BuyEnvHybridV5US, n_envs=1, vec_env_cls=DummyVecEnv,
        env_kwargs={'data_dict': ticker_eval_data, 'is_training': False}
    )
    
    # è¼‰å…¥æ¨¡å‹ä¸¦èª¿æ•´è¶…åƒæ•¸
    buy_model = PPO.load(buy_base_path, env=buy_env, device="cpu")
    
    # Fine-tune è¶…åƒæ•¸ (LR = pretrain çš„ 1/10)
    buy_model.learning_rate = 1e-5
    buy_model.lr_schedule = get_schedule_fn(buy_model.learning_rate)
    buy_model.ent_coef = 0.01
    
    # ç¨ç«‹ TensorBoard æ—¥èªŒ
    log_name = f"buy_finetune_{ticker}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    logger = configure(os.path.join(tensorboard_log, log_name), ["tensorboard", "stdout"])
    buy_model.set_logger(logger)
    
    # Callbacks
    callbacks = CallbackList([
        CheckpointCallback(save_freq=100000, save_path=ticker_model_dir, name_prefix=f"ppo_buy_{ticker}"),
        EvalCallback(eval_env, 
                     best_model_save_path=os.path.join(ticker_model_dir, "best"),
                     log_path=os.path.join(ticker_model_dir, "logs"),
                     eval_freq=10000, n_eval_episodes=100, deterministic=True)
    ])
    
    print(f"  Training {FINETUNE_BUY_STEPS:,} steps (LR: 1e-5)...")
    buy_model.learn(total_timesteps=FINETUNE_BUY_STEPS, callback=callbacks, reset_num_timesteps=False)
    
    # å„²å­˜æœ€çµ‚æ¨¡å‹
    final_path = os.path.join(ticker_model_dir, f"ppo_buy_{ticker}_final.zip")
    best_path = os.path.join(ticker_model_dir, "best", "best_model.zip")
    
    if os.path.exists(best_path):
        shutil.copy(best_path, final_path)
        print(f"  âœ… Copied BEST model to {final_path}")
    else:
        buy_model.save(final_path.replace('.zip', ''))
        print(f"  âš ï¸ Best model not found, saved last step model")
    
    # æ›´æ–° manifest
    train_df = ticker_train_data[ticker]
    eval_df = ticker_eval_data[ticker]
    
    # è¨ˆç®—é©—è­‰é›†å‹ç‡ (ç°¡åŒ–ç‰ˆ)
    eval_samples = eval_df.dropna(subset=['Next_20d_Max'])
    if len(eval_samples) > 0:
        pos_ratio = (eval_samples['Next_20d_Max'] >= 0.10).mean()
    else:
        pos_ratio = 0.0
    
    manifest["tickers"][ticker] = {
        "model_path": final_path,
        "train_end_date": train_df.index[-1].strftime('%Y-%m-%d'),
        "val_win_rate": round(pos_ratio, 3),
        "actual_training_days": len(train_df),
        "base_model_version": "ppo_buy_base_us_tech"
    }
    
    buy_env.close()
    eval_env.close()
    
    print(f"  âœ… {ticker} Fine-tuning Complete (Train days: {len(train_df)})")


def run_all_finetuning(all_train_data: dict, all_eval_data: dict, models_path: str):
    """
    å°æ‰€æœ‰è‚¡ç¥¨åŸ·è¡Œ Fine-tuning
    """
    print("\n" + "=" * 60)
    print("Phase 2: Fine-tuning (Individual Tickers)")
    print("=" * 60)
    print(f"  Steps per ticker: {FINETUNE_BUY_STEPS:,}")
    print(f"  Tickers: {list(all_train_data.keys())}")
    print("=" * 60)
    
    manifest = {
        "version": "v5_us_tech",
        "base_model": "ppo_buy_base_us_tech.zip",
        "created_at": datetime.now().isoformat(),
        "tickers": {}
    }
    
    for ticker in all_train_data.keys():
        if ticker == BENCHMARK:
            continue  # è·³éåŸºæº–æŒ‡æ•¸
        
        ticker_train = {ticker: all_train_data[ticker]}
        ticker_eval = {ticker: all_eval_data.get(ticker, all_train_data[ticker])}
        
        run_finetuning(ticker, ticker_train, ticker_eval, models_path, manifest)
    
    # å„²å­˜ manifest
    manifest_path = os.path.join(models_path, "model_manifest.json")
    with open(manifest_path, 'w', encoding='utf-8') as f:
        json.dump(manifest, f, indent=2, ensure_ascii=False)
    
    print(f"\n[System] Model manifest saved to: {manifest_path}")
    print("[System] Fine-tuning Completed for all tickers.")


# =============================================================================
# 6. ä¸»ç¨‹å¼
# =============================================================================
def main():
    print("=" * 70)
    print("  US Tech Stock Buy Agent Training Script")
    print("  ç›®æ¨™æ¨™çš„: " + ", ".join(TICKERS))
    print("  åŸºæº–æŒ‡æ•¸: " + BENCHMARK)
    print("=" * 70)
    
    # =========================================================================
    # Step 1: è¼‰å…¥è³‡æ–™
    # =========================================================================
    raw_data = fetch_all_stock_data()
    benchmark_df = raw_data.get(BENCHMARK)
    
    # =========================================================================
    # Step 2: è¨ˆç®—ç‰¹å¾µ
    # =========================================================================
    print("\n" + "=" * 60)
    print("ğŸ“Š è¨ˆç®—ç‰¹å¾µ")
    print("=" * 60)
    
    all_features = {}
    for ticker, df in raw_data.items():
        try:
            processed = calculate_features(df, benchmark_df, ticker, use_cache=True)
            if len(processed) > 100:
                all_features[ticker] = processed
                print(f"  âœ… {ticker}: {len(processed)} rows")
        except Exception as e:
            print(f"  âŒ {ticker}: {e}")
    
    # =========================================================================
    # Step 3: åˆ†å‰²è¨“ç·´/é©—è­‰é›†
    # =========================================================================
    print("\n" + "=" * 60)
    print("ğŸ“‚ åˆ†å‰²è¨“ç·´/é©—è­‰é›†")
    print("=" * 60)
    
    train_data = {}
    eval_data = {}
    
    val_start, val_end = VAL_RANGE
    
    for ticker, df in all_features.items():
        if ticker == BENCHMARK:
            continue
        
        # å–å¾—æœ‰æ•ˆçš„è¨“ç·´å€é–“
        valid_ranges = get_valid_train_ranges(ticker, raw_data[ticker], TRAIN_RANGES)
        
        if not valid_ranges:
            print(f"  âš ï¸ {ticker}: ç„¡æœ‰æ•ˆè¨“ç·´å€é–“ï¼Œè·³é")
            continue
        
        # éæ¿¾è¨“ç·´è³‡æ–™
        train_df = filter_by_ranges(df, valid_ranges)
        
        # é©—è­‰è³‡æ–™
        eval_df = df[(df.index >= pd.Timestamp(val_start)) & (df.index <= pd.Timestamp(val_end))]
        
        if len(train_df) > 100:
            train_data[ticker] = train_df
            eval_data[ticker] = eval_df if len(eval_df) > 50 else train_df
            print(f"  âœ… {ticker}: Train {len(train_df)} | Eval {len(eval_df)}")
        else:
            print(f"  âš ï¸ {ticker}: è¨“ç·´è³‡æ–™ä¸è¶³ ({len(train_df)} rows)")
    
    print(f"\n[Summary] æœ‰æ•ˆæ¨™çš„: {len(train_data)} / {len(TICKERS)}")
    
    # =========================================================================
    # Step 4: Pre-training
    # =========================================================================
    buy_base_path = os.path.join(MODELS_PATH, "ppo_buy_base_us_tech.zip")
    
    if os.path.exists(buy_base_path):
        print("\n" + "=" * 60)
        print("[Skip] Pre-training (Base model exists)")
        print(f"       {buy_base_path}")
        print("=" * 60)
    else:
        run_pretraining(train_data, MODELS_PATH)
    
    # =========================================================================
    # Step 5: Fine-tuning
    # =========================================================================
    run_all_finetuning(train_data, eval_data, MODELS_PATH)
    
    # =========================================================================
    # å®Œæˆ
    # =========================================================================
    print("\n" + "=" * 70)
    print("  âœ… è¨“ç·´å®Œæˆï¼")
    print("=" * 70)
    print(f"""
ğŸ“ æ¨¡å‹å„²å­˜ä½ç½®:
   {MODELS_PATH}/ppo_buy_base_us_tech.zip (Base Model)
   {MODELS_PATH}/finetuned/{{TICKER}}/ppo_buy_{{TICKER}}_final.zip

ğŸ“Š æ¨¡å‹æ¸…å–®:
   {MODELS_PATH}/model_manifest.json

ğŸ“ˆ TensorBoard ç›£æ§:
   tensorboard --logdir ./tensorboard_logs/
""")


if __name__ == "__main__":
    main()
