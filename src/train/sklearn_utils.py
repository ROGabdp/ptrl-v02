#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
================================================================================
Sklearn Training Utilities
================================================================================
æœ¬æ¨¡çµ„æä¾›å…±ç”¨çš„æ©Ÿå™¨å­¸ç¿’è¨“ç·´ã€è©•ä¼°èˆ‡é¡åˆ¥å¹³è¡¡è¼”åŠ©å‡½å¼ï¼Œ
è¢« train_sklearn_classifier.py èˆ‡ train_rolling_hgb.py ç­‰è…³æœ¬æ‰€å¼•ç”¨ã€‚
================================================================================
"""

import sys
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, HistGradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, 
                             roc_auc_score, average_precision_score, confusion_matrix)
from sklearn.inspection import permutation_importance


def get_positive_proba(model, X, positive_label=1) -> tuple:
    """
    å–å¾—é æ¸¬ç‚º positive_label (é è¨­ç‚º 1) çš„æ©Ÿç‡é™£åˆ—ã€‚
    é¿å…ä¾è³´ hard-coded çš„ [:, 1]ï¼Œæ”¹ç”± model.classes_ å‹•æ…‹å°‹æ‰¾ã€‚
    """
    if not hasattr(model, "predict_proba"):
        raise ValueError(f"æ¨¡å‹ {type(model).__name__} ä¸æ”¯æ´ predict_proba()")
        
    proba_all = model.predict_proba(X)
    classes = list(model.classes_)
    
    if positive_label not in classes:
        raise ValueError(f"æ¨™ç±¤ {positive_label} ä¸å­˜åœ¨æ–¼ model.classes_ {classes} ä¸­ã€‚")
        
    pos_idx = classes.index(positive_label)
    return proba_all[:, pos_idx], classes, pos_idx


def apply_class_balancing(df_train, balance_method, seed):
    """æ ¹æ“š balance_method è™•ç†è¨“ç·´é›†çš„ Class Imbalance"""
    if balance_method == 'undersample_50_50':
        pos_df = df_train[df_train['y'] == 1]
        neg_df = df_train[df_train['y'] == 0]
        min_len = min(len(pos_df), len(neg_df))
        if min_len == 0:
            return df_train
            
        pos_sample = pos_df.sample(n=min_len, random_state=seed)
        neg_sample = neg_df.sample(n=min_len, random_state=seed)
        
        # ç¢ºä¿é †åºä¸è¢«æ‰“äº‚æˆ–è€…é‡æ’
        balanced_df = pd.concat([pos_sample, neg_sample]).sort_index()
        print(f"\nâš–ï¸  [Undersample 50/50] é‡æ–°å–æ¨£å¾Œ Train Size: {len(balanced_df)} (Pos: {len(pos_sample)}, Neg: {len(neg_sample)})")
        return balanced_df
        
    return df_train


def get_model(model_name, balance_method, seed):
    """å›å‚³æŒ‡å®šæ¨¡å‹èˆ‡æ˜¯å¦éœ€è¦åœ¨ fit() ä¸­ä½¿ç”¨ sample_weight"""
    class_weight = 'balanced' if balance_method == 'class_weight_balanced' else None
    
    if model_name == 'rf':
        model = RandomForestClassifier(n_estimators=100, max_depth=10, 
                                       random_state=seed, class_weight=class_weight, n_jobs=-1)
        return model, False # RF built-in handles class_weight

    elif model_name == 'adaboost':
        # DecisionTreeClassifier æ”¯æ´ class_weight
        base = DecisionTreeClassifier(max_depth=2, class_weight=class_weight, random_state=seed)
        model = AdaBoostClassifier(estimator=base, n_estimators=100, random_state=seed)
        return model, False

    elif model_name == 'hgb':
        # HistGradientBoostingClassifier é›–ç„¶ä¸ç›´æ¥æ”¯æ´ class_weight='balanced'
        # åœ¨ sklearn ä¸­å¯ä»¥æ”¹ç”± class_weight parameter (åœ¨ 1.3+) æˆ–æ˜¯ä½¿ç”¨ fit å‚³é sample_weight
        try:
            model = HistGradientBoostingClassifier(max_iter=100, max_depth=10, 
                                                   random_state=seed, class_weight=class_weight)
            return model, False
        except TypeError:
            # Fallback for older scikit-learn versions
            model = HistGradientBoostingClassifier(max_iter=100, max_depth=10, random_state=seed)
            return model, (class_weight == 'balanced')
            
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹ç¨®é¡: {model_name}")


def calc_metrics(y_true, y_proba, y_pred, prefix="Overall"):
    """è¨ˆç®—ä¸¦å›å‚³é©—è­‰é›†çš„å„ç¨®æŒ‡æ¨™"""
    metrics = {}
    
    # é¿å… y_true å…¨ 0 æˆ–å…¨ 1 å°è‡´ auc å¤±æ•—
    has_mixed_classes = len(np.unique(y_true)) > 1
    
    metrics['Accuracy'] = float(accuracy_score(y_true, y_pred))
    metrics['Precision'] = float(precision_score(y_true, y_pred, zero_division=0))
    metrics['Recall'] = float(recall_score(y_true, y_pred, zero_division=0))
    metrics['F1'] = float(f1_score(y_true, y_pred, zero_division=0))
    
    metrics['ROC-AUC'] = float(roc_auc_score(y_true, y_proba)) if has_mixed_classes else None
    metrics['PR-AUC'] = float(average_precision_score(y_true, y_proba)) if has_mixed_classes else None
    
    metrics['Confusion Matrix'] = confusion_matrix(y_true, y_pred).tolist()
    
    # Precision@k (Top 1%, 5%, 10%)
    sort_idx = np.argsort(y_proba)[::-1]
    sorted_y_true = np.array(y_true)[sort_idx]
    
    for k_pct in [0.01, 0.05, 0.10]:
        k = max(1, int(len(y_true) * k_pct))
        top_k_y_true = sorted_y_true[:k]
        metrics[f'Precision@{int(k_pct*100)}%'] = float(np.mean(top_k_y_true))
        
    # Threshold sweep
    metrics['Threshold Sweep'] = {}
    for th in [0.5, 0.6, 0.7, 0.8, 0.9]:
        y_pred_th = (y_proba >= th).astype(int)
        metrics['Threshold Sweep'][f'Threshold={th}'] = {
            'Precision': float(precision_score(y_true, y_pred_th, zero_division=0)),
            'Recall': float(recall_score(y_true, y_pred_th, zero_division=0)),
            'F1': float(f1_score(y_true, y_pred_th, zero_division=0))
        }
        
    return metrics


def get_feature_importances(model, model_name, X_val, y_val, feature_cols):
    """è¨ˆç®—ä¸¦å›å‚³ç‰¹å¾µé‡è¦æ€§"""
    importances_dict = {}
    print("\nğŸ” æ­£åœ¨è¨ˆç®— Feature Importances (Top 30)...")
    
    if model_name == 'rf':
        try:
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1]
            for i in indices[:30]:
                importances_dict[feature_cols[i]] = float(importances[i])
        except AttributeError:
            pass
    
    # å°å…¶ä»–æ¨¡å‹ä½¿ç”¨ permutation importance (é‡å° Validation Subset å–æ¨£ä»¥æ±‚æ•ˆç‡)
    if not importances_dict:
        n_samples = min(50000, len(X_val))
        idx = np.random.choice(len(X_val), n_samples, replace=False)
        X_sub = X_val.iloc[idx]
        y_sub = y_val.iloc[idx]
        
        result = permutation_importance(model, X_sub, y_sub, n_repeats=5, random_state=42, n_jobs=-1)
        importances = result.importances_mean
        indices = np.argsort(importances)[::-1]
        
        for i in indices[:30]:
            importances_dict[feature_cols[i]] = float(importances[i])
            
    return importances_dict
