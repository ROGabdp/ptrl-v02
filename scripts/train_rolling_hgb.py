#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import sys
import json
import argparse
import numpy as np
import pandas as pd
from datetime import datetime

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ åˆ° sys.pathï¼Œä»¥ä¾¿ import å…±ç”¨æ¨¡çµ„
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

from src.train.sklearn_utils import get_positive_proba, apply_class_balancing, get_model, calc_metrics

try:
    from train_us_tech_buy_agent import fetch_all_stock_data, calculate_features, FEATURE_COLS, BENCHMARK
except ImportError:
    print("âŒ æ‰¾ä¸åˆ° train_us_tech_buy_agent æ¨¡çµ„ï¼Œè«‹ç¢ºå®šæ‚¨åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹åŸ·è¡Œã€‚")
    sys.exit(1)


def parse_args():
    parser = argparse.ArgumentParser(description="Rolling HGB Walk-Forward è¨“ç·´è…³æœ¬ (Regime Shift é˜²ç¦¦èµ·æ‰‹å¼)")
    
    # ç›®æ¨™èˆ‡è¼¸å‡º
    parser.add_argument('--tickers', nargs='+', default=['GOOGL'], help="ç›®æ¨™è‚¡ç¥¨ (é è¨­: GOOGL)")
    parser.add_argument('--output-dir', type=str, default='output_rolling_hgb', help="è¼¸å‡ºæ ¹ç›®éŒ„")
    
    # ç›®æ¨™å®šç¾©åƒæ•¸
    parser.add_argument('--target-days', type=int, default=120, help="ç›®æ¨™é æ¸¬å¤©æ•¸ (é è¨­: 120)")
    parser.add_argument('--target-return', type=float, default=0.20, help="ç›®æ¨™å ±é…¬ç‡é–€æª» (é è¨­: 0.20)")
    
    # è¨“ç·´/é©—è­‰é‚Šç•Œèˆ‡æ™‚é–“çª—è¨­å®š
    parser.add_argument('--window-years', type=int, default=5, help="Train window çš„é•·åº¦(ä»¥å¹´ç‚ºå–®ä½) (é è¨­: 5)")
    parser.add_argument('--val-years', nargs='+', type=int, 
                        help="æ¬²é©—è­‰çš„å¹´åº¦ï¼Œè‹¥æœªçµ¦å®šå‰‡æœƒè‡ªå‹•æƒæå¯ç”¨çš„æ‰€æœ‰å¹´ä»½ã€‚ç¯„ä¾‹: --val-years 2018 2019 2020")
    parser.add_argument('--start-year', type=int, default=None, help="èˆ‡ --end-year æ­é…ç”¨æ–¼ç¯„åœè¨­å®š")
    parser.add_argument('--end-year', type=int, default=None, help="èˆ‡ --start-year æ­é…ç”¨æ–¼ç¯„åœè¨­å®š")
    
    # æ¨¡å‹è¶…åƒæ•¸èˆ‡è¡Œç‚º
    parser.add_argument('--model', type=str, default='hgb', choices=['hgb'], help="ç›®å‰å¯¦ä½œå°ˆæ³¨æ–¼ HGB")
    parser.add_argument('--seed', type=int, default=42, help="äº‚æ•¸ç¨®å­")
    parser.add_argument('--balance-train', type=str, default='none', 
                        choices=['none', 'undersample_50_50', 'class_weight_balanced'],
                        help="Train Set çš„å¹³è¡¡ç­–ç•¥ï¼ŒVal Set ä¸€å¾‹ä¸å¹³è¡¡ä»¥åæ˜ çœŸå¯¦åˆ†ä½ˆ")
    
    # å·¥å…·æ§åˆ¶
    parser.add_argument('--no-cache', action='store_true', help="å¼·åˆ¶é‡æ–°è¨ˆç®—ç‰¹å¾µä¸ä½¿ç”¨å¿«å–")
    parser.add_argument('--dry-run', action='store_true', help="åƒ…è¼¸å‡ºè¨­å®šèˆ‡åˆ‡åˆ†çš„é‚Šç•Œèˆ‡æ¨£æœ¬æ•¸ï¼Œä¸é€²è¡Œè¨“ç·´")
    
    return parser.parse_args()


def get_sanity_reversal_metrics(y_true, y_proba, k_pct=0.05):
    """
    è¨ˆç®—ä¸¦åˆ¤æ–·æ˜¯å¦æœ‰åå‘ï¼ˆRegime Shift åˆ°é€£ä½åˆ†ç¾¤éƒ½æ¯”é«˜åˆ†ç¾¤æº–ï¼‰çš„å•é¡Œã€‚
    å›å‚³ top k é æ¸¬çš„ç²¾åº¦ã€æ˜¯å¦è§¸ç™¼ reversal warning ä»¥åŠ marginã€‚
    """
    n_samples = len(y_true)
    k = max(1, int(n_samples * k_pct))
    
    # High Confidence (Proba) Hit Rate
    sort_idx_proba = np.argsort(y_proba)[::-1]
    top_k_y_true = y_true.iloc[sort_idx_proba[:k]] if isinstance(y_true, pd.Series) else y_true[sort_idx_proba[:k]]
    top5_hit_rate_by_proba = float(np.mean(top_k_y_true))
    
    # Low Confidence (Inv Proba) Hit Rate
    inv_proba = 1.0 - y_proba
    sort_idx_inv = np.argsort(inv_proba)[::-1]
    bottom_k_y_true = y_true.iloc[sort_idx_inv[:k]] if isinstance(y_true, pd.Series) else y_true[sort_idx_inv[:k]]
    top5_hit_rate_by_invproba = float(np.mean(bottom_k_y_true))
    
    margin = top5_hit_rate_by_invproba - top5_hit_rate_by_proba
    # å¦‚æœæœ€çœ‹è¡°çš„æ—ç¾¤å‹ç‡è¶…è¶Šæœ€çœ‹å¥½æ—ç¾¤ 10% æˆ–ä»¥ä¸Šï¼Œå‰‡ç™¼å‡ºåš´é‡è­¦å‘Š
    is_reversal = (margin >= 0.10)
    
    return {
        'topk_pct': k_pct * 100,
        'topk_n_by_year': k,
        'top5_hit_rate_by_proba': top5_hit_rate_by_proba,
        'top5_hit_rate_by_invproba': top5_hit_rate_by_invproba,
        'reversal_margin': float(margin),
        'reversal_warning': is_reversal
    }


def prepare_dataset_for_ticker(ticker, target_days, target_return, use_cache):
    """å–å¾—è³‡æ–™ä¸¦æ ¹æ“š Target å‹•æ…‹å»ºç«‹ y æ¨™ç±¤ï¼Œç„¶å¾Œå›å‚³å®Œæ•´æ¸…ç†éçš„ DataFrame"""
    print(f"\nğŸ“¦ æ­£åœ¨æº–å‚™ {ticker} çš„è³‡æ–™é›†ä¸¦è¨ˆç®—ç‰¹å¾µ...")
    all_raw_data = fetch_all_stock_data()
    
    if ticker not in all_raw_data:
        raise ValueError(f"Ticker {ticker} ç„¡æ³•å–å¾—æ•¸æ“š")
        
    raw_df = all_raw_data[ticker]
    benchmark_df = all_raw_data.get(BENCHMARK)
    
    df_features = calculate_features(raw_df, benchmark_df, ticker=ticker, use_cache=use_cache)
    
    target_col = f'Next_{target_days}d_Max'
    if target_col not in df_features.columns:
        raise ValueError(f"ç‰¹å¾µæ¬„ä½ä¸­æ‰¾ä¸åˆ° {target_col}ï¼Œè«‹ç¢ºèª calculate_features çš„æ”¯æ´ã€‚")
    
    # å»ºç«‹æ¨™ç±¤
    df_dataset = df_features.dropna(subset=FEATURE_COLS + [target_col]).copy()
    df_dataset['y'] = (df_dataset[target_col] >= target_return).astype(int)
    
    # å°‡ datetime index è®Šç‚ºæ¬„ä½æ–¹ä¾¿æ“ä½œä¸¦ç¢ºä¿å…¶å«åš 'date'
    df_dataset = df_dataset.reset_index()
    if 'Date' in df_dataset.columns:
        df_dataset.rename(columns={'Date': 'date'}, inplace=True)
    elif 'index' in df_dataset.columns:
        df_dataset.rename(columns={'index': 'date'}, inplace=True)
        
    df_dataset['ticker'] = ticker
    return df_dataset


def extract_val_years(df_dataset, args):
    """æ±ºå®šéœ€è¦è·‘é©—è­‰çš„å¹´ä»½æ¸…å–®"""
    if args.val_years is not None:
        return sorted([int(y) for y in args.val_years])
    
    df_dataset['year'] = df_dataset['date'].dt.year
    available_years = sorted([int(y) for y in df_dataset['year'].unique()])
    
    # å‡è¨­æœ€çŸ­ window ç‚º Nï¼Œé‚£å¯ä»¥è¢« evaluate çš„ç¬¬ä¸€å¹´è‡³å°‘è¦å¤§æ–¼ min_year + N
    min_year = available_years[0]
    first_viable_val_year = min_year + args.window_years
    
    val_years_candidates = [y for y in available_years if y >= first_viable_val_year]
    
    if args.start_year:
        val_years_candidates = [y for y in val_years_candidates if y >= args.start_year]
    if args.end_year:
        val_years_candidates = [y for y in val_years_candidates if y <= args.end_year]
        
    return val_years_candidates


def main():
    args = parse_args()
    np.random.seed(args.seed)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"run_{args.model}_{args.target_days}d_{timestamp}"
    root_output_dir = os.path.join(args.output_dir, run_name)
    
    if not args.dry_run:
        os.makedirs(root_output_dir, exist_ok=True)
        print(f"ğŸ“ å»ºç«‹æ ¹è¼¸å‡ºç›®éŒ„: {root_output_dir}")
        
    master_summary = []
    use_cache = not args.no_cache
    
    for ticker in args.tickers:
        print(f"\n{'='*80}\nğŸš€ æ‰“é–‹ Walk-Forward å¼•æ“: Ticker = {ticker}\n{'='*80}")
        try:
            df_full = prepare_dataset_for_ticker(ticker, args.target_days, args.target_return, use_cache)
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ– {ticker} è³‡æ–™å¤±æ•—: {e}")
            continue
            
        val_years = extract_val_years(df_full, args)
        if not val_years:
            print(f"âš ï¸ {ticker} æ‰¾ä¸åˆ°ç¬¦åˆ window size è¦æ±‚çš„å¯ç”¨å¹´åº¦è³‡æ–™ï¼Œè·³éã€‚")
            continue
            
        print(f"ğŸ“… é è¨ˆåŸ·è¡Œ Rolling çš„å¹´åº¦ (Val Years): {val_years}")
        
        for val_y in val_years:
            print(f"\n--- â³ Epoch: é©—è­‰å¹´åº¦ {val_y} ---")
            
            # å®šç¾© Requested range
            req_train_start = f"{val_y - args.window_years}-01-01"
            req_train_end = f"{val_y - 1}-12-31"
            req_val_start = f"{val_y}-01-01"
            req_val_end = f"{val_y}-12-31"
            
            # å¯¦éš›å–å¾—åˆ‡å‰²
            df_train_raw = df_full[(df_full['date'] >= req_train_start) & (df_full['date'] <= req_train_end)].copy()
            df_val = df_full[(df_full['date'] >= req_val_start) & (df_full['date'] <= req_val_end)].copy()
            
            # æª¢æŸ¥ç­†æ•¸
            if len(df_train_raw) == 0:
                print(f"  âš ï¸ Train Set ç­†æ•¸ç‚º 0ï¼Œè·³éæ­¤å¹´åº¦ã€‚")
                continue
            if len(df_val) == 0:
                print(f"  âš ï¸ Val Set ç­†æ•¸ç‚º 0ï¼Œè·³éæ­¤å¹´åº¦ã€‚")
                continue
                
            # å–å¾— Actual Boundary (dropna ä¹‹å¾Œçš„é‚Šç•Œï¼Œç¢ºä¿ç´€éŒ„ä¸å¤±çœŸ)
            actual_tr_min = df_train_raw['date'].min().strftime('%Y-%m-%d')
            actual_tr_max = df_train_raw['date'].max().strftime('%Y-%m-%d')
            actual_va_min = df_val['date'].min().strftime('%Y-%m-%d')
            actual_va_max = df_val['date'].max().strftime('%Y-%m-%d')
            
            tr_n, va_n = len(df_train_raw), len(df_val)
            tr_pos_r = df_train_raw['y'].mean()
            va_pos_r = df_val['y'].mean()
            
            print(f"  [Requested Train] {req_train_start} ~ {req_train_end}")
            print(f"  [Actual    Train] {actual_tr_min} ~ {actual_tr_max} | N: {tr_n} | Pos%: {tr_pos_r*100:.2f}%")
            print(f"  [Requested Val  ] {req_val_start} ~ {req_val_end}")
            print(f"  [Actual    Val  ] {actual_va_min} ~ {actual_va_max} | N: {va_n} | Pos%: {va_pos_r*100:.2f}%")
            
            # å¦‚æœå¹´ä»½ä¸è¶³ï¼ˆçœŸå¯¦æœ‰è³‡æ–™çš„æœŸé–“è·Ÿè¦æ±‚å·®å¤ªå¤šï¼Œä¾‹å¦‚ req train è¦ 5 å¹´ä½†å¯¦éš›è³‡æ–™åªæœ‰åŠå¹´ï¼‰
            # ä¸ä¸€å®šè¦ skip ä½†æé†’æˆ‘å€‘å¯èƒ½ä¸æº–ç¢º
            date_span_days = (df_train_raw['date'].max() - df_train_raw['date'].min()).days
            if date_span_days < (args.window_years * 365) * 0.7:
                 print(f"  âš ï¸ æ³¨æ„: å¯¦éš› Train Window è¦†è“‹å¤©æ•¸ ({date_span_days}) é å°æ–¼è¨­å®šçš„ {args.window_years} å¹´ã€‚")
            
            if args.dry_run:
                continue
                
            y_train_raw = df_train_raw['y']
            
            # åœ¨ Train Set å¯¦æ–½ class balancing (Val Set çµ•å°ä¸å¯å‹•)
            df_train_bal = apply_class_balancing(df_train_raw, args.balance_train, args.seed)
            X_train = df_train_bal[FEATURE_COLS]
            y_train = df_train_bal['y']
            
            X_val = df_val[FEATURE_COLS]
            y_val = df_val['y']
            
            # --- è¨“ç·´èˆ‡æ¨è«– ---
            model, use_sample_weight = get_model(args.model, args.balance_train, args.seed)
            print("  [Train] æ­£åœ¨è¨“ç·´æ¨¡å‹ (Random State å›ºå®š)...")
            
            if use_sample_weight:
                from sklearn.utils.class_weight import compute_class_weight
                classes = np.unique(y_train)
                weight_arr = compute_class_weight('balanced', classes=classes, y=y_train)
                weight_dict = dict(zip(classes, weight_arr))
                sw = np.array([weight_dict[c] for c in y_train])
                model.fit(X_train, y_train, sample_weight=sw)
                used_balancing_method = 'sample_weight (simulated class_weight)'
            else:
                model.fit(X_train, y_train)
                used_balancing_method = 'built-in class_weight / None'
                
            y_proba_val, clz_list, pos_idx = get_positive_proba(model, X_val, positive_label=1)
            y_pred_val = model.predict(X_val)
            
            # --- æŒ‡æ¨™è¨ˆç®—èˆ‡ Sanity Check ---
            metrics = calc_metrics(y_val, y_proba_val, y_pred_val, prefix="Yearly")
            
            # Mean Proba
            mask_pos = (y_val == 1)
            mask_neg = (y_val == 0)
            mean_pos_proba = y_proba_val[mask_pos].mean() if mask_pos.sum() > 0 else 0.0
            mean_neg_proba = y_proba_val[mask_neg].mean() if mask_neg.sum() > 0 else 0.0
            
            # Top-K Reversal Check
            rev_stats = get_sanity_reversal_metrics(y_val, y_proba_val, k_pct=0.05)
            # Combo reversal warning (roc < 0.5 is also strictly bad)
            is_roc_fail = (metrics.get('ROC-AUC') is not None) and (metrics['ROC-AUC'] < 0.5)
            final_reversal_warning = rev_stats['reversal_warning'] or is_roc_fail
            
            print(f"  [Metric] {val_y} ROC-AUC: {metrics.get('ROC-AUC', 'N/A')}")
            print(f"  [Metric] Top5% Hit Rate by Proba: {rev_stats['top5_hit_rate_by_proba']*100:.1f}%")
            print(f"  [Metric] Top5% Hit Rate by Inv. : {rev_stats['top5_hit_rate_by_invproba']*100:.1f}%")
            if final_reversal_warning:
                print(f"  ğŸš¨âš ï¸ [REVERSAL OCURRED IN {val_y}] æ¨¡å‹æ–¹å‘å®Œå…¨éŒ¯äº‚ï¼")
                
            metrics['Sanity Check'] = {
                'mean_pos_proba': float(mean_pos_proba),
                'mean_neg_proba': float(mean_neg_proba),
                'reversal_warning': final_reversal_warning,
                **rev_stats
            }
            
            # --- å»ºç«‹å–®ä»½çµæœçš„ Params dict ---
            epoch_params = {
                'ticker': ticker,
                'run_name': run_name,
                'target_definition': f"Next_{args.target_days}d_Max >= {args.target_return}",
                'val_year': val_y,
                'requested_train_range': [req_train_start, req_train_end],
                'actual_train_range': [actual_tr_min, actual_tr_max],
                'requested_val_range': [req_val_start, req_val_end],
                'actual_val_range': [actual_va_min, actual_va_max],
                'train_samples': tr_n,
                'train_pos_rate': float(tr_pos_r),
                'val_samples': va_n,
                'val_pos_rate': float(va_pos_r),
                'window_years': args.window_years,
                'seed': args.seed,
                'model_class': type(model).__name__,
                'model_params': model.get_params(),
                'balance_train': args.balance_train,
                'used_balancing_method': used_balancing_method
            }
            
            # æº–å‚™ Master é€™ä¸€è¡Œçš„ Data
            row = {
                'ticker': ticker,
                'val_year': val_y,
                'window': args.window_years,
                'train_n': tr_n,
                'val_n': va_n,
                'val_pos_rate': va_pos_r,
                'roc_auc': metrics.get('ROC-AUC', None),
                'pr_auc': metrics.get('PR-AUC', None),
                'precision@5%': metrics.get('Precision@5%', None),
                'precision@10%': metrics.get('Precision@10%', None),
                'th0.5_precision': metrics.get('Threshold Sweep', {}).get('Threshold=0.5', {}).get('Precision', None),
                'th0.5_recall': metrics.get('Threshold Sweep', {}).get('Threshold=0.5', {}).get('Recall', None),
                'th0.5_f1': metrics.get('Threshold Sweep', {}).get('Threshold=0.5', {}).get('F1', None),
                'top5_n': rev_stats['topk_n_by_year'],
                'top5_hit_proba': rev_stats['top5_hit_rate_by_proba'],
                'top5_hit_invproba': rev_stats['top5_hit_rate_by_invproba'],
                'mean_pos_proba': mean_pos_proba,
                'mean_neg_proba': mean_neg_proba,
                'reversal_warning': final_reversal_warning
            }
            master_summary.append(row)
            
            # --- Output åˆ°å¹´ä»½ç¨ç«‹è³‡æ–™å¤¾ ---
            year_dir = os.path.join(root_output_dir, f"{ticker}_{val_y}")
            os.makedirs(year_dir, exist_ok=True)
            
            with open(os.path.join(year_dir, "params.json"), "w", encoding="utf-8") as f:
                json.dump(epoch_params, f, indent=4, ensure_ascii=False)
                
            with open(os.path.join(year_dir, "metrics.json"), "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=4, ensure_ascii=False)
                
            # Prediction CSV
            df_out = df_val[['date', 'ticker']].copy()
            df_out['y_true'] = y_val.values
            df_out['y_proba'] = y_proba_val
            df_out['y_pred'] = y_pred_val
            df_out.to_csv(os.path.join(year_dir, "val_predictions.csv"), index=False)
            
    # å…¨å±€ç¸½çµèˆ‡å¯«æª”
    if not args.dry_run and master_summary:
        print(f"\n{'='*80}\nâœ… æ‰€æœ‰ Rolling Epochs æ¸¬è©¦å®Œç•¢ï¼Œæ­£åœ¨ç”¢ç”Ÿçµç®—å ±å‘Š...")
        df_summary = pd.DataFrame(master_summary)
        csv_path = os.path.join(root_output_dir, "rolling_summary.csv")
        json_path = os.path.join(root_output_dir, "rolling_summary.json")
        
        df_summary.to_csv(csv_path, index=False)
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(master_summary, f, indent=4, ensure_ascii=False)
            
        print(f"ğŸ“Š ç¸½çµå ±å‘Šï¼š{csv_path}")
        print(df_summary[['val_year', 'val_n', 'val_pos_rate', 'roc_auc', 'precision@5%', 
                          'top5_hit_invproba', 'reversal_warning']].to_string(index=False))


if __name__ == "__main__":
    main()
