#!/usr/bin/env python
# -*- coding: utf-8 -*-
import os
import sys
import json
import argparse
import numpy as np
import pandas as pd
from datetime import datetime

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ åˆ° sys.pathï¼Œä»¥ä¾¿ import å…±ç”¨æ¨¡çµ„
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

from src.train.sklearn_utils import get_positive_proba, apply_class_balancing, get_model, calc_metrics
from src.features.regime_features import compute_regime_features, REGIME_COLS

try:
    from train_us_tech_buy_agent import fetch_all_stock_data, calculate_features, FEATURE_COLS, BENCHMARK
except ImportError:
    print("âŒ æ‰¾ä¸åˆ° train_us_tech_buy_agent æ¨¡çµ„ï¼Œè«‹ç¢ºå®šæ‚¨åœ¨å°ˆæ¡ˆæ ¹ç›®éŒ„ä¸‹åŸ·è¡Œã€‚")
    sys.exit(1)


def parse_args():
    parser = argparse.ArgumentParser(description="Rolling HGB Walk-Forward è¨“ç·´è…³æœ¬ (Regime Shift é˜²ç¦¦èµ·æ‰‹å¼)")
    
    # ç›®æ¨™èˆ‡è¼¸å‡º
    parser.add_argument('--tickers', nargs='+', default=['GOOGL'], help="ç›®æ¨™è‚¡ç¥¨ (é è¨­: GOOGL)")
    parser.add_argument('--output-dir', type=str, default='output_rolling_hgb', help="è¼¸å‡ºæ ¹ç›®éŒ„")
    
    # ç›®æ¨™å®šç¾©åƒæ•¸
    parser.add_argument('--target-days', type=int, default=120, help="ç›®æ¨™é æ¸¬å¤©æ•¸ (é è¨­: 120)")
    parser.add_argument('--target-return', type=float, default=0.20, help="ç›®æ¨™å ±é…¬ç‡é–€æª» (é è¨­: 0.20)")
    
    # è¨“ç·´/é©—è­‰é‚Šç•Œèˆ‡æ™‚é–“çª—è¨­å®š
    parser.add_argument('--window-years', type=int, default=5, help="Train window çš„é•·åº¦(ä»¥å¹´ç‚ºå–®ä½) (é è¨­: 5)")
    parser.add_argument('--val-years', nargs='+', type=int, 
                        help="æ¬²é©—è­‰çš„å¹´åº¦ï¼Œè‹¥æœªçµ¦å®šå‰‡æœƒè‡ªå‹•æƒæå¯ç”¨çš„æ‰€æœ‰å¹´ä»½ã€‚ç¯„ä¾‹: --val-years 2018 2019 2020")
    parser.add_argument('--start-year', type=int, default=None, help="èˆ‡ --end-year æ­é…ç”¨æ–¼ç¯„åœè¨­å®š")
    parser.add_argument('--end-year', type=int, default=None, help="èˆ‡ --start-year æ­é…ç”¨æ–¼ç¯„åœè¨­å®š")
    
    # æ¨¡å‹è¶…åƒæ•¸èˆ‡è¡Œç‚º
    parser.add_argument('--model', type=str, default='hgb', choices=['hgb'], help="ç›®å‰å¯¦ä½œå°ˆæ³¨æ–¼ HGB")
    parser.add_argument('--seed', type=int, default=42, help="äº‚æ•¸ç¨®å­")
    parser.add_argument('--balance-train', type=str, default='none', 
                        choices=['none', 'undersample_50_50', 'class_weight_balanced'],
                        help="Train Set çš„å¹³è¡¡ç­–ç•¥ï¼ŒVal Set ä¸€å¾‹ä¸å¹³è¡¡ä»¥åæ˜ çœŸå¯¦åˆ†ä½ˆ")
    parser.add_argument('--use-regime-features', action='store_true', 
                        help="æ˜¯å¦è¦åˆä½µ Benchmark Regime Features (ä¾‹å¦‚ MA200, HV20) ä¸€èµ·ä¸Ÿçµ¦æ¨¡å‹è©•ä¼°")
    
    # Reversal åˆ¤å®šé˜²å‘†
    parser.add_argument('--reversal-gap-margin', type=float, default=0.10, 
                        help="å®šç¾©å·®è· (Hit Proba - Inv Proba) å°æ–¼è² å¤šå°‘æ™‚ç™¼å‡ºè­¦å‘Š (é è¨­: 0.10)")
    parser.add_argument('--reversal-use-top10', type=str, default='true', choices=['true', 'false'],
                        help="æ˜¯å¦åˆä½µç´å…¥ Top 10% æ¨£æœ¬é€²è¡Œåå‘é›™é‡ç¢ºèª (é è¨­: true)")
    
    # å·¥å…·æ§åˆ¶
    parser.add_argument('--no-cache', action='store_true', help="å¼·åˆ¶é‡æ–°è¨ˆç®—ç‰¹å¾µä¸ä½¿ç”¨å¿«å–")
    parser.add_argument('--dry-run', action='store_true', help="åƒ…è¼¸å‡ºè¨­å®šèˆ‡åˆ‡åˆ†çš„é‚Šç•Œèˆ‡æ¨£æœ¬æ•¸ï¼Œä¸é€²è¡Œè¨“ç·´")
    
    return parser.parse_args()


def get_sanity_reversal_metrics(y_true, y_proba, margin_threshold=0.10, use_top10=True):
    """
    è¨ˆç®—ä¸¦åˆ¤æ–·æ˜¯å¦æœ‰åå‘ï¼ˆRegime Shift åˆ°é€£ä½åˆ†ç¾¤éƒ½æ¯”é«˜åˆ†ç¾¤æº–ï¼‰çš„å•é¡Œã€‚
    å›å‚³ top5 èˆ‡ top10 çš„ç²¾åº¦ã€gapï¼Œä»¥åŠåŸºæ–¼ gap_margin ç™¼å¸ƒçš„ warningã€‚
    """
    n_samples = len(y_true)
    k5 = max(1, int(n_samples * 0.05))
    k10 = max(1, int(n_samples * 0.10))
    
    sort_idx_proba = np.argsort(y_proba)[::-1]
    inv_proba = 1.0 - y_proba
    sort_idx_inv = np.argsort(inv_proba)[::-1]
    
    def _calc_hit_rate(sort_idx, k):
        top_k_y_true = y_true.iloc[sort_idx[:k]] if isinstance(y_true, pd.Series) else y_true[sort_idx[:k]]
        return float(np.mean(top_k_y_true))
        
    # Top 5%
    top5_proba_hr = _calc_hit_rate(sort_idx_proba, k5)
    top5_inv_hr = _calc_hit_rate(sort_idx_inv, k5)
    gap5 = top5_proba_hr - top5_inv_hr
    warn5 = (gap5 <= -margin_threshold)
    
    # Top 10%
    top10_proba_hr = _calc_hit_rate(sort_idx_proba, k10)
    top10_inv_hr = _calc_hit_rate(sort_idx_inv, k10)
    gap10 = top10_proba_hr - top10_inv_hr
    warn10 = (gap10 <= -margin_threshold)
    
    final_warning = warn5 or warn10 if use_top10 else warn5

    return {
        'top5_n': k5,
        'top5_hit_proba': top5_proba_hr,
        'top5_hit_invproba': top5_inv_hr,
        'top5_gap': float(gap5),
        'top10_n': k10,
        'top10_hit_proba': top10_proba_hr,
        'top10_hit_invproba': top10_inv_hr,
        'top10_gap': float(gap10),
        'reversal_warning_top5': warn5,
        'reversal_warning_top10': warn10,
        'reversal_warning': final_warning
    }


def prepare_dataset_for_ticker(ticker, target_days, target_return, use_cache):
    """å–å¾—è³‡æ–™ä¸¦æ ¹æ“š Target å‹•æ…‹å»ºç«‹ y æ¨™ç±¤ï¼Œç„¶å¾Œå›å‚³å®Œæ•´æ¸…ç†éçš„ DataFrame"""
    print(f"\nğŸ“¦ æ­£åœ¨æº–å‚™ {ticker} çš„è³‡æ–™é›†ä¸¦è¨ˆç®—ç‰¹å¾µ...")
    all_raw_data = fetch_all_stock_data()
    
    if ticker not in all_raw_data:
        raise ValueError(f"Ticker {ticker} ç„¡æ³•å–å¾—æ•¸æ“š")
        
    raw_df = all_raw_data[ticker]
    benchmark_df = all_raw_data.get(BENCHMARK)
    
    df_features = calculate_features(raw_df, benchmark_df, ticker=ticker, use_cache=use_cache)
    
    target_col = f'Next_{target_days}d_Max'
    if target_col not in df_features.columns:
        raise ValueError(f"ç‰¹å¾µæ¬„ä½ä¸­æ‰¾ä¸åˆ° {target_col}ï¼Œè«‹ç¢ºèª calculate_features çš„æ”¯æ´ã€‚")
    
    # å»ºç«‹æ¨™ç±¤
    df_dataset = df_features.dropna(subset=FEATURE_COLS + [target_col]).copy()
    df_dataset['y'] = (df_dataset[target_col] >= target_return).astype(int)
    
    # å°‡ datetime index è®Šç‚ºæ¬„ä½æ–¹ä¾¿æ“ä½œä¸¦ç¢ºä¿å…¶å«åš 'date'
    df_dataset = df_dataset.reset_index()
    if 'Date' in df_dataset.columns:
        df_dataset.rename(columns={'Date': 'date'}, inplace=True)
    elif 'index' in df_dataset.columns:
        df_dataset.rename(columns={'index': 'date'}, inplace=True)
        
    df_dataset['date_str'] = pd.to_datetime(df_dataset['date']).dt.strftime('%Y-%m-%d')
    df_dataset['ticker'] = ticker
    
    return df_dataset, benchmark_df


def extract_val_years(df_dataset, args):
    """æ±ºå®šéœ€è¦è·‘é©—è­‰çš„å¹´ä»½æ¸…å–®"""
    if args.val_years is not None:
        return sorted([int(y) for y in args.val_years])
    
    df_dataset['year'] = df_dataset['date'].dt.year
    available_years = sorted([int(y) for y in df_dataset['year'].unique()])
    
    # å‡è¨­æœ€çŸ­ window ç‚º Nï¼Œé‚£å¯ä»¥è¢« evaluate çš„ç¬¬ä¸€å¹´è‡³å°‘è¦å¤§æ–¼ min_year + N
    min_year = available_years[0]
    first_viable_val_year = min_year + args.window_years
    
    val_years_candidates = [y for y in available_years if y >= first_viable_val_year]
    
    if args.start_year:
        val_years_candidates = [y for y in val_years_candidates if y >= args.start_year]
    if args.end_year:
        val_years_candidates = [y for y in val_years_candidates if y <= args.end_year]
        
    return val_years_candidates


def run_rolling_training(args):
    """
    åŸ·è¡Œ Walk-Forward æ»¾å‹•è¨“ç·´çš„æ ¸å¿ƒé‚è¼¯ã€‚
    å¯ç”±åŸæœ¬ CLI çš„ main() æˆ–å¤–éƒ¨ wrapper (å¦‚ run_rolling_grid.py) å‚³å…¥ args å‘¼å«ã€‚
    å›å‚³å€¼ï¼š
      master_summary (list of dict): æ”¶éŒ„æ‰€æœ‰å¹´åº¦ã€æ‰€æœ‰ ticker çš„åŸ·è¡Œçµ±è¨ˆæŒ‡æ¨™ã€‚
    """
    if hasattr(args, 'seed') and args.seed is not None:
        np.random.seed(args.seed)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"run_hgb_{args.target_days}d_{timestamp}"
    root_output_dir = args.output_dir
    
    if getattr(args, 'dry_run', False):
        pass
    else:
        os.makedirs(root_output_dir, exist_ok=True)
        print(f"ğŸ“ å»ºç«‹è¼¸å‡ºç›®éŒ„: {root_output_dir}")
        
    master_summary = []
    use_cache = not getattr(args, 'no_cache', False)
    
    for ticker in args.tickers:
        print(f"\n{'='*80}\nğŸš€ æ‰“é–‹ Walk-Forward å¼•æ“: Ticker = {ticker}\n{'='*80}")
        try:
            df_full, benchmark_df = prepare_dataset_for_ticker(ticker, args.target_days, args.target_return, use_cache)
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ– {ticker} è³‡æ–™å¤±æ•—: {e}")
            continue
            
        # è™•ç† Regime Features æ•´åˆ
        active_feature_cols = FEATURE_COLS.copy()
        if getattr(args, 'use_regime_features', False):
            print("ğŸ§² å•Ÿå‹• Regime Features (HGB è‡ªç ”é˜²ç¦¦), æº–å‚™çµåˆå¤§ç›¤ç‰¹å¾µ...")
            df_regime = compute_regime_features(benchmark_df)
            
            # å»ºç«‹ Date Str ä»¥ä¾› Merge
            if 'date_str' not in df_full.columns:
                df_full['date_str'] = pd.to_datetime(df_full['date']).dt.strftime('%Y-%m-%d')
                
            # å°‡ df_regime (å·²ç¶“æœ‰ date string) Merge èµ·ä¾†
            df_full = pd.merge(df_full, df_regime, left_on='date_str', right_on='date', how='inner', suffixes=('', '_regime'))
            # é‡æ–° Dropna ä¿éšœæ–°ç‰¹å¾µæ²’æœ‰æ´ (å¤§ç›¤æœ€å‰é¢æœƒæœ‰æ­·å²é•·åº¦çš„æ´)
            df_full = df_full.dropna(subset=REGIME_COLS).copy()
            active_feature_cols += REGIME_COLS
            print(f"   => åˆä½µå®Œæˆ, X è®Šæ•¸å¾ {len(FEATURE_COLS)} å¢é•·ç‚º {len(active_feature_cols)} å€‹ã€‚")
            
        val_years = extract_val_years(df_full, args)
        if not val_years:
            print(f"âš ï¸ {ticker} æ‰¾ä¸åˆ°ç¬¦åˆ window size è¦æ±‚çš„å¯ç”¨å¹´åº¦è³‡æ–™ï¼Œè·³éã€‚")
            continue
            
        print(f"ğŸ“… é è¨ˆåŸ·è¡Œ Rolling çš„å¹´åº¦ (Val Years): {val_years}")
        
        for val_y in val_years:
            print(f"\n--- â³ Epoch: é©—è­‰å¹´åº¦ {val_y} ---")
            
            # å®šç¾© Requested range
            req_train_start = f"{val_y - args.window_years}-01-01"
            req_train_end = f"{val_y - 1}-12-31"
            req_val_start = f"{val_y}-01-01"
            req_val_end = f"{val_y}-12-31"
            
            # å¯¦éš›å–å¾—åˆ‡å‰²
            df_train_raw = df_full[(df_full['date'] >= req_train_start) & (df_full['date'] <= req_train_end)].copy()
            df_val = df_full[(df_full['date'] >= req_val_start) & (df_full['date'] <= req_val_end)].copy()
            
            # æª¢æŸ¥ç­†æ•¸
            if len(df_train_raw) == 0:
                print(f"  âš ï¸ Train Set ç­†æ•¸ç‚º 0ï¼Œè·³éæ­¤å¹´åº¦ã€‚")
                continue
            if len(df_val) == 0:
                print(f"  âš ï¸ Val Set ç­†æ•¸ç‚º 0ï¼Œè·³éæ­¤å¹´åº¦ã€‚")
                continue
                
            # å–å¾— Actual Boundary (dropna ä¹‹å¾Œçš„é‚Šç•Œï¼Œç¢ºä¿ç´€éŒ„ä¸å¤±çœŸ)
            actual_tr_min = df_train_raw['date'].min().strftime('%Y-%m-%d')
            actual_tr_max = df_train_raw['date'].max().strftime('%Y-%m-%d')
            actual_va_min = df_val['date'].min().strftime('%Y-%m-%d')
            actual_va_max = df_val['date'].max().strftime('%Y-%m-%d')
            
            tr_n, va_n = len(df_train_raw), len(df_val)
            tr_pos_r = df_train_raw['y'].mean()
            va_pos_r = df_val['y'].mean()
            
            print(f"  [Requested Train] {req_train_start} ~ {req_train_end}")
            print(f"  [Actual    Train] {actual_tr_min} ~ {actual_tr_max} | N: {tr_n} | Pos%: {tr_pos_r*100:.2f}%")
            print(f"  [Requested Val  ] {req_val_start} ~ {req_val_end}")
            print(f"  [Actual    Val  ] {actual_va_min} ~ {actual_va_max} | N: {va_n} | Pos%: {va_pos_r*100:.2f}%")
            
            # å¦‚æœå¹´ä»½ä¸è¶³ï¼ˆçœŸå¯¦æœ‰è³‡æ–™çš„æœŸé–“è·Ÿè¦æ±‚å·®å¤ªå¤šï¼Œä¾‹å¦‚ req train è¦ 5 å¹´ä½†å¯¦éš›è³‡æ–™åªæœ‰åŠå¹´ï¼‰
            # ä¸ä¸€å®šè¦ skip ä½†æé†’æˆ‘å€‘å¯èƒ½ä¸æº–ç¢º
            date_span_days = (df_train_raw['date'].max() - df_train_raw['date'].min()).days
            if date_span_days < (args.window_years * 365) * 0.7:
                 print(f"  âš ï¸ æ³¨æ„: å¯¦éš› Train Window è¦†è“‹å¤©æ•¸ ({date_span_days}) é å°æ–¼è¨­å®šçš„ {args.window_years} å¹´ã€‚")
            
            if args.dry_run:
                continue
                
            y_train_raw = df_train_raw['y']
            
            # åœ¨ Train Set å¯¦æ–½ class balancing (Val Set çµ•å°ä¸å¯å‹•)
            df_train_bal = apply_class_balancing(df_train_raw, args.balance_train, args.seed)
            X_train = df_train_bal[active_feature_cols]
            y_train = df_train_bal['y']
            
            X_val = df_val[active_feature_cols]
            y_val = df_val['y']
            
            # --- è¨“ç·´èˆ‡æ¨è«– ---
            model, use_sample_weight = get_model(args.model, args.balance_train, args.seed)
            print("  [Train] æ­£åœ¨è¨“ç·´æ¨¡å‹ (Random State å›ºå®š)...")
            
            if use_sample_weight:
                from sklearn.utils.class_weight import compute_class_weight
                classes = np.unique(y_train)
                weight_arr = compute_class_weight('balanced', classes=classes, y=y_train)
                weight_dict = dict(zip(classes, weight_arr))
                sw = np.array([weight_dict[c] for c in y_train])
                model.fit(X_train, y_train, sample_weight=sw)
                used_balancing_method = 'sample_weight (simulated class_weight)'
            else:
                model.fit(X_train, y_train)
                used_balancing_method = 'built-in class_weight / None'
                
            y_proba_val, clz_list, pos_idx = get_positive_proba(model, X_val, positive_label=1)
            y_pred_val = model.predict(X_val)
            
            # --- æŒ‡æ¨™è¨ˆç®—èˆ‡ Sanity Check ---
            metrics = calc_metrics(y_val, y_proba_val, y_pred_val, prefix="Yearly")
            
            # Mean Proba
            mask_pos = (y_val == 1)
            mask_neg = (y_val == 0)
            mean_pos_proba = y_proba_val[mask_pos].mean() if mask_pos.sum() > 0 else 0.0
            mean_neg_proba = y_proba_val[mask_neg].mean() if mask_neg.sum() > 0 else 0.0
            
            # Top-K Reversal Check
            rev_stats = get_sanity_reversal_metrics(
                y_val, y_proba_val, 
                margin_threshold=args.reversal_gap_margin, 
                use_top10=(args.reversal_use_top10 == 'true')
            )
            
            # Combo reversal warning (roc < 0.5 is also strictly bad)
            is_roc_fail = (metrics.get('ROC-AUC') is not None) and (metrics['ROC-AUC'] < 0.5)
            final_reversal_warning = rev_stats['reversal_warning'] or is_roc_fail
            
            print(f"  [Metric] {val_y} ROC-AUC: {metrics.get('ROC-AUC', 'N/A')}")
            print(f"  [Metric] Top5% Hit Rate by Proba: {rev_stats['top5_hit_proba']*100:.1f}%")
            print(f"  [Metric] Top5% Hit Rate by Inv. : {rev_stats['top5_hit_invproba']*100:.1f}%")
            print(f"  [Metric] Top5% Gap              : {rev_stats['top5_gap']*100:.1f}%")
            if args.reversal_use_top10 == 'true':
                 print(f"  [Metric] Top10% Hit Rate by Pro: {rev_stats['top10_hit_proba']*100:.1f}% | Gap: {rev_stats['top10_gap']*100:.1f}%")
            
            if final_reversal_warning:
                print(f"  ğŸš¨âš ï¸ [REVERSAL OCURRED IN {val_y}] è§¸ç™¼åå‘è­¦å‘Šæ©Ÿåˆ¶ï¼")
                
            metrics['Sanity Check'] = {
                'reversal_rule_version': 'v2',
                'reversal_gap_margin': args.reversal_gap_margin,
                'reversal_check_top10': args.reversal_use_top10,
                'mean_pos_proba': float(mean_pos_proba),
                'mean_neg_proba': float(mean_neg_proba),
                'reversal_warning_final': final_reversal_warning,
                **rev_stats
            }
            
            # --- å»ºç«‹å–®ä»½çµæœçš„ Params dict ---
            epoch_params = {
                'ticker': ticker,
                'run_name': run_name,
                'target_definition': f"Next_{args.target_days}d_Max >= {args.target_return}",
                'val_year': val_y,
                'requested_train_range': [req_train_start, req_train_end],
                'actual_train_range': [actual_tr_min, actual_tr_max],
                'requested_val_range': [req_val_start, req_val_end],
                'actual_val_range': [actual_va_min, actual_va_max],
                'train_samples': tr_n,
                'train_pos_rate': float(tr_pos_r),
                'val_samples': va_n,
                'val_pos_rate': float(va_pos_r),
                'window_years': args.window_years,
                'seed': args.seed,
                'model_class': type(model).__name__,
                'model_params': model.get_params(),
                'balance_train': args.balance_train,
                'used_balancing_method': used_balancing_method,
                'use_regime_features': getattr(args, 'use_regime_features', False),
                'regime_cols': REGIME_COLS if getattr(args, 'use_regime_features', False) else [],
                'reversal_rule_version': 'v2',
                'reversal_gap_margin': args.reversal_gap_margin,
                'reversal_check_top10': args.reversal_use_top10
            }
            
            # --- æ”¶é›† Regime Summary (ç•¶å¹´å¸‚å ´ç‹€æ³) ---
            regime_dict = {}
            if getattr(args, 'use_regime_features', False):
                # çµ±è¨ˆè©²å¹´åº¦ (Val Set) ä¸­ï¼Œé€™äº›å¤§ç›¤ç‰¹å¾µçš„è¡¨ç¾æ¦‚æ³ï¼Œç”¨ä¾†é—œè¯æ˜¯å¦é€ æˆæ¨¡å‹å´©å£
                regime_dict = {
                    'regime_above_ma200_rate': df_val['REGIME_BM_ABOVE_MA200'].mean(),
                    'regime_hv20_mean': df_val['REGIME_BM_HV20'].mean(),
                    'regime_hv20_p50': df_val['REGIME_BM_HV20'].median(),
                    'regime_hv20_p90': df_val['REGIME_BM_HV20'].quantile(0.90),
                    'regime_hv20_pctl_mean': df_val['REGIME_BM_HV20_PCTL'].mean(),
                    'regime_hv20_pctl_p50': df_val['REGIME_BM_HV20_PCTL'].median(),
                    'regime_ret_120_mean': df_val['REGIME_BM_RET_120'].mean(),
                    'regime_ret_60_mean': df_val['REGIME_BM_RET_60'].mean(),
                }
            
            # æº–å‚™ Master é€™ä¸€è¡Œçš„ Data
            row = {
                'ticker': ticker,
                'val_year': val_y,
                'window': args.window_years,
                'train_n': tr_n,
                'val_n': va_n,
                'val_pos_rate': va_pos_r,
                'roc_auc': metrics.get('ROC-AUC', None),
                'pr_auc': metrics.get('PR-AUC', None),
                'precision@5%': metrics.get('Precision@5%', None),
                'precision@10%': metrics.get('Precision@10%', None),
                'th0.5_precision': metrics.get('Threshold Sweep', {}).get('Threshold=0.5', {}).get('Precision', None),
                'th0.5_recall': metrics.get('Threshold Sweep', {}).get('Threshold=0.5', {}).get('Recall', None),
                'th0.5_f1': metrics.get('Threshold Sweep', {}).get('Threshold=0.5', {}).get('F1', None),
                'top5_n': rev_stats['top5_n'],
                'top5_hit_proba': rev_stats['top5_hit_proba'],
                'top5_hit_invproba': rev_stats['top5_hit_invproba'],
                'top5_gap': rev_stats['top5_gap'],
                'top10_n': rev_stats['top10_n'],
                'top10_hit_proba': rev_stats['top10_hit_proba'],
                'top10_hit_invproba': rev_stats['top10_hit_invproba'],
                'top10_gap': rev_stats['top10_gap'],
                'mean_pos_proba': mean_pos_proba,
                'mean_neg_proba': mean_neg_proba,
                'reversal_warning_top5': rev_stats['reversal_warning_top5'],
                'reversal_warning_top10': rev_stats['reversal_warning_top10'],
                'reversal_warning': final_reversal_warning
            }
            # å¦‚æœæœ‰å•Ÿå‹• Regimeï¼Œå°±æŠŠé‚£äº›çµ±è¨ˆæŒ‡æ¨™å¡å…¥ Master
            row.update(regime_dict)
            
            master_summary.append(row)
            
            # --- Output åˆ°å¹´ä»½ç¨ç«‹è³‡æ–™å¤¾ ---
            year_dir = os.path.join(root_output_dir, f"{ticker}_{val_y}")
            os.makedirs(year_dir, exist_ok=True)
            
            with open(os.path.join(year_dir, "params.json"), "w", encoding="utf-8") as f:
                json.dump(epoch_params, f, indent=4, ensure_ascii=False)
                
            with open(os.path.join(year_dir, "metrics.json"), "w", encoding="utf-8") as f:
                json.dump(metrics, f, indent=4, ensure_ascii=False)
                
            # Prediction CSV
            df_out = df_val[['date', 'ticker']].copy()
            df_out['y_true'] = y_val.values
            df_out['y_proba'] = y_proba_val
            df_out['y_pred'] = y_pred_val
            df_out.to_csv(os.path.join(year_dir, "val_predictions.csv"), index=False)
            
    # å…¨å±€ç¸½çµèˆ‡å¯«æª”
    if not getattr(args, 'dry_run', False) and master_summary:
        print(f"\n{'='*80}\nâœ… æ‰€æœ‰ Rolling Epochs æ¸¬è©¦å®Œç•¢ï¼Œæ•´ç†çµæœ...")
        df_summary = pd.DataFrame(master_summary)
        csv_path = os.path.join(root_output_dir, "rolling_summary.csv")
        json_path = os.path.join(root_output_dir, "rolling_summary.json")
        
        df_summary.to_csv(csv_path, index=False)
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(master_summary, f, indent=4, ensure_ascii=False)
            
        print(f"ğŸ“Š å¹´åº¦ç¸½çµå ±å‘Šå·²å¯«å‡ºï¼š{csv_path}")
        
    return master_summary


def main():
    """åŸæœ¬ä½œç‚ºç¨ç«‹ CLI æ™‚çš„é€²å…¥é»"""
    args = parse_args()
    
    # å–®ç´”åŸ·è¡Œè…³æœ¬æ™‚ï¼Œè‡ªå‹•è£œä¸Šç›®æ¨™ç›®éŒ„çš„ä¸€å±¤ timestamp 
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_name = f"run_{args.model}_{args.target_days}d_{timestamp}"
    args.output_dir = os.path.join(args.output_dir, run_name)
    
    master_summary = run_rolling_training(args)
    if not args.dry_run and master_summary:
        df_summary = pd.DataFrame(master_summary)
        print(df_summary[['val_year', 'val_n', 'val_pos_rate', 'roc_auc', 'precision@5%', 
                          'top5_hit_invproba', 'reversal_warning']].to_string(index=False))


if __name__ == "__main__":
    main()
