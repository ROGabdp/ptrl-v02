#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
================================================================================
Feature Regime Shift Diagnostics
================================================================================
é‡å°ç‰¹å®šçš„ ticker èˆ‡æ¨è«–çµæœ (val_predictions.csv)ï¼Œè‡ªå‹•åˆ‡å‡ºæ¯å¹´çš„
ã€Œæ¨¡å‹æœ€æœ‰ä¿¡å¿ƒæœƒæ¼² (Top K% by p)ã€èˆ‡ã€Œæ¨¡å‹æœ€æ²’ä¿¡å¿ƒæœƒæ¼² (Top K% by 1-p)ã€å­é›†ã€‚
æ¥è‘—ï¼Œè¨ˆç®—é€™äº›æ¥µç«¯æ¨£æœ¬ç¾¤å„è‡ªåœ¨ç‰¹å¾µåˆ†ä½ˆä¸Šçš„çµ±è¨ˆæ•¸æ“šï¼Œä¸¦ä¸”æ¯”å°å·®ç•° (Feature Shift)ã€‚
é€™æœ‰åŠ©æ–¼è¨ºæ–·ç‚ºä½•æŸå¹¾å¹´æ¨¡å‹çš„é æ¸¬æ–¹å‘ç™¼ç”Ÿäº†å€’å (ROC-AUC < 0.5)ã€‚

ä½¿ç”¨ç¯„ä¾‹:
python scripts/analyze_topk_feature_shifts.py --val-predictions output_sklearn/run_hgb_120d_20260221_083838/val_predictions.csv --ticker GOOGL --topk-pct 5 --output-dir output_analysis
================================================================================
"""

import os
import sys
import json
import argparse
from datetime import datetime
import numpy as np
import pandas as pd
import warnings

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ åˆ° sys.pathï¼Œä»¥ä¾¿ import å…±ç”¨æ¨¡çµ„
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

try:
    from train_us_tech_buy_agent import fetch_all_stock_data, calculate_features, FEATURE_COLS, BENCHMARK
except ImportError as e:
    print(f"âŒ ç„¡æ³•å¾ train_us_tech_buy_agent.py è¼‰å…¥å…±ç”¨é‚è¼¯: {e}")
    sys.exit(1)


def parse_args():
    parser = argparse.ArgumentParser(description="Analyze Feature Shifts for False Positives / Regime Changes")
    parser.add_argument("--val-predictions", type=str, required=True, 
                        help="å¿…å¡«ï¼šåŒ…å«æ¨è«–çµæœçš„ val_predictions.csv æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--ticker", type=str, required=True, 
                        help="å¿…å¡«ï¼šè¦è¨ºæ–·çš„è‚¡ç¥¨ä»£ç¢¼ (e.g. GOOGL)")
    parser.add_argument("--topk-pct", type=float, default=5.0, 
                        help="Top K ç™¾åˆ†æ¯”é–¾å€¼ï¼Œé è¨­ 5 (ä»£è¡¨ 5%)")
    parser.add_argument("--output-dir", type=str, default="output_analysis", 
                        help="åˆ†æå ±å‘Šçš„è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--years", nargs="*", type=int, 
                        help="æ¬²åˆ†æçš„å¹´ä»½æ¸…å–® (é è¨­: è‡ªå‹•åµæ¸¬ csv åŒ…å«çš„æ‰€æœ‰å¹´ä»½)")
    parser.add_argument("--no-cache", action="store_true", 
                        help="é—œé–‰ç‰¹å¾µå¿«å–è®€å–ï¼Œå¼·åˆ¶é‡æ–°é‹ç®—")
    parser.add_argument("--seed", type=int, default=42, 
                        help="æ’åºç™¼ç”Ÿå¹³æ‰‹æ™‚çš„éš¨æ©Ÿç¨®å­")
    return parser.parse_args()


def compute_feature_stats(df_subset, features):
    """è¨ˆç®—æŒ‡å®šç‰¹å¾µåœ¨çµ¦å®šå­é›†å…§çš„çµ±è¨ˆæ‘˜è¦"""
    if len(df_subset) == 0:
        return pd.DataFrame(columns=['feature', 'mean', 'median', 'std', 'p10', 'p90', 'n'])
        
    stats = []
    for f in features:
        if f not in df_subset.columns:
            continue
        vals = df_subset[f].dropna()
        n = len(vals)
        if n == 0:
            stats.append({'feature': f, 'mean': np.nan, 'median': np.nan, 'std': np.nan, 'p10': np.nan, 'p90': np.nan, 'n': 0})
        else:
            stats.append({
                'feature': f,
                'mean': vals.mean(),
                'median': vals.median(),
                'std': vals.std(),
                'p10': vals.quantile(0.10),
                'p90': vals.quantile(0.90),
                'n': n
            })
    return pd.DataFrame(stats)


def main():
    args = parse_args()
    
    # é˜²æ­¢ pandas print è¢«çœç•¥
    pd.set_option('display.max_rows', 100)
    
    print("============================================================")
    print("ğŸ” Feature Regime Shift Analytics")
    print("============================================================")
    print(f"  Ticker       : {args.ticker}")
    print(f"  Predictions  : {args.val_predictions}")
    print(f"  Top K%       : {args.topk_pct}%")
    print(f"  Output Dir   : {args.output_dir}")
    print("============================================================")
    
    # 1. è®€å– Predictions CSV
    if not os.path.exists(args.val_predictions):
        print(f"âŒ æ‰¾ä¸åˆ°é æ¸¬æª”: {args.val_predictions}")
        sys.exit(1)
        
    df_pred_all = pd.read_csv(args.val_predictions)
    
    # é©—è­‰å¿…å‚™æ¬„ä½
    req_cols = ['date', 'ticker', 'y_true', 'y_proba']
    if not all(c in df_pred_all.columns for c in req_cols):
        print(f"âŒ val_predictions ç¼ºå°‘å¿…å‚™æ¬„ä½ï¼Œè«‹ç¢ºèªåŒ…å«: {req_cols}")
        print(f"ç›®å‰æ¬„ä½: {df_pred_all.columns.tolist()}")
        sys.exit(1)
        
    # 2. ç¯©é¸èˆ‡æ—¥æœŸè½‰æ›
    df_pred = df_pred_all[df_pred_all['ticker'] == args.ticker].copy()
    if len(df_pred) == 0:
        print(f"âŒ åœ¨é æ¸¬æª”ä¸­æ‰¾ä¸åˆ° Ticker: {args.ticker} çš„ç›¸é—œç´€éŒ„ã€‚")
        sys.exit(1)
        
    df_pred['date'] = pd.to_datetime(df_pred['date'])
    df_pred['year'] = df_pred['date'].dt.year
    df_pred['inv_proba'] = 1.0 - df_pred['y_proba'] # åå‘åˆ†æ•¸
    
    years_to_analyze = args.years if args.years else sorted(df_pred['year'].unique())
    print(f"ğŸ‘‰ æ¶µè“‹çš„ç›®æ¨™å¹´ä»½: {years_to_analyze}")
    
    # 3. ç²å–ä¸¦å»ºæ§‹ Feature DataFrame
    print(f"ğŸ“¥ æ­£åœ¨ç”¢ç”Ÿ {args.ticker} çš„ç‰¹å¾µæ­·å²åºåˆ—...")
    all_raw_data = fetch_all_stock_data()
    benchmark_df = all_raw_data.get(BENCHMARK)
    if benchmark_df is None:
        print(f"âŒ ç„¡æ³•è¼‰å…¥åŸºæº–æŒ‡æ•¸ {BENCHMARK}ã€‚")
        sys.exit(1)
        
    raw_df = all_raw_data.get(args.ticker)
    if raw_df is None:
        print(f"âŒ ç„¡æ³•è¼‰å…¥åŸå§‹æ¨™çš„è³‡æ–™: {args.ticker}ã€‚")
        sys.exit(1)
        
    df_features = calculate_features(raw_df, benchmark_df, ticker=args.ticker, use_cache=not args.no_cache)
    
    # è®“ Feature DF çš„ index è½‰æˆ regular column å–å date ä¸¦èª¿æ•´æ ¼å¼ï¼Œä¾¿æ–¼ merge
    df_features = df_features.reset_index()
    df_features.rename(columns={'Date': 'date'}, inplace=True)
    df_features['date'] = pd.to_datetime(df_features['date'])
    
    # 4. Inner Join
    # æª¢æŸ¥æ˜¯å¦æœƒæœ‰å¤§é‡æ¼åˆ‡çš„æƒ…å½¢
    join_test = pd.merge(df_pred, df_features[['date'] + FEATURE_COLS], on='date', how='left')
    missing_mask = join_test[FEATURE_COLS[0]].isna()
    if missing_mask.any():
        missing_count = missing_mask.sum()
        missing_pct = missing_count / len(join_test) * 100
        print(f"âš ï¸ è­¦å‘Š: Join ä¹‹å¾Œç™¼ç¾æœ‰ {missing_count} ç­† ({missing_pct:.2f}%) çš„ç‰¹å¾µç‚ºç©ºï¼")
        print("  å¯èƒ½åŸå› æ˜¯ calculate_features åœ¨æœ€æ–°æ¨è«–è³‡æ–™ä¸Šçš„ NaN è¢«æ¿¾æ‰ï¼Œæˆ–è€…æ™‚é–“å®Œå…¨è„«é‰¤ã€‚")
        print("  å‰ 5 ç­†éºå¤±æ—¥æœŸ:")
        print(join_test[missing_mask].head(5)[['date', 'y_proba']])
    
    # åŸ·è¡Œä¹¾æ·¨çš„ Inner Join (å»é™¤ feature ç©ºå€¼)
    df_merged = pd.merge(df_pred, df_features[['date'] + FEATURE_COLS], on='date', how='inner')
    print(f"âœ… å°é½Šå®Œæˆï¼Œå…±æœ‰ {len(df_merged)} ç­†å¯ç”¨ç‰¹å¾µæ¨£æœ¬ã€‚")
    
    # 5. å»ºç«‹è¼¸å‡ºæ ¹ç›®éŒ„
    run_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    sub_dir = f"shift_{args.ticker}_top{int(args.topk_pct)}_{run_ts}"
    out_dir = os.path.join(args.output_dir, sub_dir)
    per_year_dir = os.path.join(out_dir, "per_year")
    os.makedirs(per_year_dir, exist_ok=True)
    
    # éš¨æ©Ÿæ•¸ç¢ºä¿æ’åºä¸€è‡´æ€§
    rng = np.random.default_rng(args.seed)
    
    # æ•´é«”å½™ç¸½
    summary_data = {
        "ticker": args.ticker,
        "predictions_source": args.val_predictions,
        "topk_pct": args.topk_pct,
        "total_merged_samples": len(df_merged),
        "yearly_performance": {},
        "top_flipped_features_per_year": {}
    }
    
    # =========================================================
    # 6. é–‹å§‹åˆ†å¹´æƒæ
    # =========================================================
    for yr in years_to_analyze:
        df_yr = df_merged[df_merged['year'] == yr].copy()
        
        if len(df_yr) < 30:
            print(f"âš ï¸ [Year {yr}] æ¨£æœ¬æ•¸åªæœ‰ {len(df_yr)} ä¸è¶³ 30 ç­†ï¼Œçµ±è¨ˆå¯èƒ½æ²’æœ‰ä»£è¡¨æ€§ã€‚")
            if len(df_yr) == 0:
                continue
        
        # æ±ºå®š K ç­†æ•¸
        k_sz = max(1, int(len(df_yr) * args.topk_pct / 100.0))
        
        # ç‚ºäº†é¿å…æ©Ÿç‡å®Œå…¨ç›¸åŒå°è‡´æ¬¡åºäº‚è·³ï¼ŒåŠ å…¥å¾®å°çš„ noise ä¾†æ–· tie
        noise = rng.uniform(0, 1e-9, size=len(df_yr))
        df_yr['tie_breaker_p'] = df_yr['y_proba'] + noise
        df_yr['tie_breaker_inv_p'] = df_yr['inv_proba'] + noise
        
        # Açµ„: åˆ†æ•¸æœ€é«˜ Top K%
        df_top_A = df_yr.nlargest(k_sz, 'tie_breaker_p')
        # Bçµ„: åå‘åˆ†æ•¸æœ€é«˜ (æœ€ä¸çœ‹å¥½) Top K%
        df_top_B = df_yr.nlargest(k_sz, 'tie_breaker_inv_p')
        
        # è¨ˆç®—å‘½ä¸­ç‡ (Precision@K)
        prec_A = df_top_A['y_true'].mean()
        prec_B = df_top_B['y_true'].mean()
        
        summary_data["yearly_performance"][str(yr)] = {
            "total_samples": int(len(df_yr)),
            "group_size_k": int(k_sz),
            "baseline_pos_rate": float(df_yr['y_true'].mean()),
            "GroupA_HighConf_PrecAtK": float(prec_A),
            "GroupB_LowConf_PrecAtK": float(prec_B),
            "warning_reversal": bool(prec_B > prec_A) # ä½åˆ†ç¾¤åè€Œæ›´æœƒæ¼²ï¼
        }
        
        # ç”¢å‡º A / B çš„çµ±è¨ˆç‰¹å¾µè¡¨
        stats_A = compute_feature_stats(df_top_A, FEATURE_COLS)
        stats_B = compute_feature_stats(df_top_B, FEATURE_COLS)
        
        stats_A.to_csv(os.path.join(per_year_dir, f"{yr}_topk_by_proba_stats.csv"), index=False)
        stats_B.to_csv(os.path.join(per_year_dir, f"{yr}_topk_by_invproba_stats.csv"), index=False)
        
        # ç”¢å‡ºè©²å¹´ top-k date æ¸…å–®ï¼Œä¾¿æ–¼è¦–è¦ºåŒ–å›æ¸¬
        pd.concat([
            df_top_A[['date', 'y_true', 'y_proba']].assign(Group='HighConf_A'),
            df_top_B[['date', 'y_true', 'y_proba']].assign(Group='LowConf_B')
        ]).to_csv(os.path.join(per_year_dir, f"{yr}_topk_dates.csv"), index=False)
        
        
        # =========================================================
        # è¨ˆç®—ç‰¹å¾µå·®ç•°åº¦é‡
        # =========================================================
        if not stats_A.empty and not stats_B.empty:
            diff_merged = pd.merge(
                stats_A[['feature', 'median', 'mean', 'std']], 
                stats_B[['feature', 'median', 'mean', 'std']], 
                on='feature', suffixes=('_A', '_B')
            )
            
            diff_merged['median_diff'] = diff_merged['median_A'] - diff_merged['median_B']
            
            # Pooled STD Approximation for Standardized Diff
            # (n1-1)*s1^2 + (n2-1)*s2^2 / (n1+n2-2)
            var_A = diff_merged['std_A'] ** 2
            var_B = diff_merged['std_B'] ** 2
            pooled_std = np.sqrt((var_A + var_B) / 2.0) + 1e-9  # é˜²æ­¢é™¤ä»¥ 0
            
            diff_merged['standardized_diff'] = (diff_merged['mean_A'] - diff_merged['mean_B']) / pooled_std
            
            # æ’åºï¼šä»¥æ¨™æº–åŒ–å·®ç•°çš„çµ•å°å€¼æ’åºï¼Œå°‹æ‰¾ã€ŒAçµ„èˆ‡Bçµ„çœ‹æ³•æˆªç„¶ä¸åŒã€çš„é¡›å€’ç‰¹å¾µ
            diff_merged['abs_std_diff'] = diff_merged['standardized_diff'].abs()
            diff_merged = diff_merged.sort_values(by='abs_std_diff', ascending=False)
            diff_merged = diff_merged.drop(columns=['abs_std_diff'])
            
            diff_merged.to_csv(os.path.join(per_year_dir, f"{yr}_feature_diff_A_vs_B.csv"), index=False)
            
            # æŠŠå‰ 20 å€‹æ½›åœ¨ç¿»è½‰ç‰¹å¾µåç¨±å¯«å…¥ Summary
            top_N = min(20, len(diff_merged))
            top_features_yr = diff_merged.head(top_N)[['feature', 'standardized_diff', 'median_diff']].to_dict(orient='records')
            summary_data["top_flipped_features_per_year"][str(yr)] = top_features_yr
            
            # å°å‡ºç°¡å–®å ±è¡¨
            print(f"\n[{yr}] N={len(df_yr)}, k={k_sz} ({args.topk_pct}%)")
            print(f"   Baseline Pos% : {df_yr['y_true'].mean()*100:5.2f}%")
            print(f"   Group A Prec@k: {prec_A*100:5.2f}%  (Top Proba)")
            print(f"   Group B Prec@k: {prec_B*100:5.2f}%  (Low Proba)")
            if prec_B > prec_A:
                print("   âš ï¸ ç™¼ç¾åå‘é æ¸¬ (åˆ†æ•¸è¶Šä½è¶Šå®¹æ˜“ä¸­)ï¼")
                
            print("   â–º Top 3 å·®ç•°æœ€å¤§çš„ç‰¹å¾µ:")
            for row in diff_merged.head(3).itertuples():
                print(f"      - {row.feature:<20} | StdDiff: {row.standardized_diff:>7.3f} | MedianDiff: {row.median_diff:>7.3f}")

    # =========================================================
    # 7. åŒ¯å‡ºæ•´é«” Summary
    # =========================================================
    summary_path = os.path.join(out_dir, "summary.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump(summary_data, f, indent=4, ensure_ascii=False)
        
    print("\n============================================================")
    print(f"âœ… è¨ºæ–·å ±å‘Šç”¢ç”Ÿå®Œç•¢ï¼Œè«‹æŸ¥é–±: {out_dir}")
    print("============================================================")

if __name__ == "__main__":
    main()
