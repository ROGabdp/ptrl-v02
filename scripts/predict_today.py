#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
================================================================================
US Tech Stock - Daily Train & Predictor (Single Ticker Approach)
================================================================================
æ¯å¤©ä¾æ“šæœ€æ–°æŠ“å–çš„å¸‚å ´è³‡æ–™ç‚ºæ¯å€‹è‚¡ç¥¨ã€Œç¨ç«‹ã€å»ºæ§‹æ»¾å‹•ç‰¹å¯«æ¨¡å‹ï¼Œä¸¦ä¾æ“šè©²æ¨™çš„ä»Šæ—¥åˆ†æ•¸
åœ¨å…¶æœ€è¿‘ 252 äº¤æ˜“æ—¥ï¼ˆæ­·å²åˆ†ä½æ•¸ï¼‰é–“çš„ç›¸å°å¼·åº¦ï¼Œèˆ‡ä»Šæ—¥å¤§ç›¤é¢¨æ§æŒ‡æ¨™çµåˆï¼Œé€²è¡Œè©•ç´šèˆ‡ä½ˆå±€åˆ¤æ–·ã€‚
================================================================================
"""

import os
import sys
import argparse
import joblib
import json
import pandas as pd
import numpy as np
import warnings
from datetime import datetime, timedelta
from scipy.stats import percentileofscore

warnings.filterwarnings('ignore', category=UserWarning)

ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

from src.train.sklearn_utils import get_positive_proba
from src.features.regime_features import compute_regime_features, REGIME_COLS

try:
    from train_us_tech_buy_agent import fetch_all_stock_data, calculate_features, FEATURE_COLS, BENCHMARK
except ImportError as e:
    print(f"âŒ ç„¡æ³•å¾ train_us_tech_buy_agent.py è¼‰å…¥å…±ç”¨é‚è¼¯: {e}")
    sys.exit(1)


def parse_args():
    parser = argparse.ArgumentParser(description="Predict 'Today's Buy Decision' single ticker approach")
    
    # åŸ·è¡Œèˆ‡æª”æ¡ˆæ¨¡å¼
    parser.add_argument("--model-path", type=str, default=None, 
                        help="é¸å¡«ï¼šæä¾›å·²é å…ˆè¨“ç·´å¥½ä¹‹æ¨¡å‹æª”æ¡ˆ(è·¯å¾‘éœ€å« {ticker} ä½”ä½ç¬¦)ã€‚è‹¥ç„¡ï¼Œå‰‡é€²è¡Œç•¶æ—¥ç¨ç«‹è¨“ç·´ã€‚")
    parser.add_argument("--output-dir", type=str, default="output_daily", help="ç•¶æ—¥æ¨¡å‹èˆ‡çµæœå„²å­˜æ ¹ç›®éŒ„")
    
    # æ¨è–¦èˆ‡ç›®æ¨™åƒæ•¸
    parser.add_argument("--tickers", nargs="+", 
                        default=["NVDA", "MSFT", "AAPL", "AMZN", "META", "AVGO", "GOOGL", "TSLA", "NFLX", "PLTR"],
                        help="è¦é æ¸¬çš„ç›®æ¨™è‚¡ç¥¨åˆ—è¡¨")
    parser.add_argument("--target-days", type=int, default=120, help="é æ¸¬æœªä¾†çš„äº¤æ˜“å¤©æ•¸ (é è¨­ 120)")
    parser.add_argument("--target-return", type=float, default=0.20, help="ç›®æ¨™æœ€é«˜åƒ¹æ¼²å¹…é–€æª» (é è¨­ 0.20)")
    
    # æ¨¡å‹æ¨æ–·è¨­å®š
    parser.add_argument("--window-years", type=int, default=3, help="è¨“ç·´çª—æ ¼å¤§å° (é è¨­ 3 å¹´)")
    parser.add_argument("--lookback-years", type=int, default=8, help="ä¸‹è¼‰è³‡æ–™æ‰€éœ€æ¨å‰å¹´æ•¸ä¾› MA/ç‰¹å¾µæš–æ©Ÿç”¨ (é è¨­ 8 å¹´)")
    parser.add_argument("--pct-lookback-days", type=int, default=252, help="åˆ†ä½æ•¸è¨ˆç®—æ¨æ–·çš„æ¨£æœ¬å¤©æ•¸ (é è¨­ 252 äº¤æ˜“æ—¥)")
    parser.add_argument("--topk-threshold-pct", type=float, default=0.90, help="æ­£å¸¸å¸‚å ´ä¸‹ä½œç‚ºè²·å…¥æ¨™æº–ä¹‹æ­·å²åˆ†ä½æ•¸ (é è¨­ 0.90 -> Top 10%)")
    parser.add_argument("--risk-threshold-pct", type=float, default=0.95, help="é«˜é¢¨éšªå¸‚å ´ä¸‹åš´æ ¼åŒ–ä¹‹åˆ†ä½æ•¸é–€æª» (é è¨­ 0.95 -> Top 5%)")
    parser.add_argument("--use-regime-features", type=str, default="true", choices=["true", "false"])
    
    parser.add_argument("--force-retrain", action="store_true", help="å¼·åˆ¶é‡æ–°è¨“ç·´æ–°æ¨¡å‹å¿½ç•¥ç•¶æ—¥å¿«å–")
    parser.add_argument("--no-cache", action="store_true", help="å¼·åˆ¶é‡æ–°æ“·å–/è¨ˆç®—ç›¤å¾Œç‰¹å¾µä¸è®€å–æ­·å²æš«å­˜æª”")
    return parser.parse_args()


def load_model_and_predict(model_path, model_type, X_input):
    if model_type == "ppo":
        from stable_baselines3 import PPO
        import torch
        model = PPO.load(model_path, device="cpu")
        with torch.no_grad():
            obs_tensor = torch.tensor(X_input, dtype=torch.float32, device="cpu")
            distribution = model.policy.get_distribution(obs_tensor)
            proba = distribution.distribution.probs[:, 1].cpu().numpy()[0]
        return proba
        
    elif model_type in ["sklearn", "daily"]:
        model = joblib.load(model_path)
        y_proba, _, _ = get_positive_proba(model, pd.DataFrame(X_input), positive_label=1)
        # Handle single vs batch predictions
        if len(y_proba) == 1:
            return float(y_proba[0])
        else:
            return y_proba
    else:
        raise ValueError(f"ä¸èªå¾—çš„æ¨¡å‹æ ¼å¼: {model_type}")

def evaluate_regime_risk(benchmark_df):
    """åˆ¤æ–·å¸‚å ´æ˜¯å¦è™•æ–¼é«˜é¢¨éšªç‹€æ…‹ (Proxy é¢¨æ§æ©Ÿåˆ¶)"""
    regime_df = compute_regime_features(benchmark_df)
    if len(regime_df) == 0: return "NORMAL", False
    
    latest_regime = regime_df.iloc[-1]
    
    bm_above_ma200 = latest_regime.get('REGIME_BM_ABOVE_MA200', 1.0)
    hv20_pctl = latest_regime.get('REGIME_BM_HV20_PCTL', 0.0)
    ret_120 = latest_regime.get('REGIME_BM_RET_120', 0.0)
    
    is_risk = False
    reasons = []
    
    if bm_above_ma200 == 0 and hv20_pctl > 0.8:
        is_risk = True
        reasons.append("MA200 Below & HV20_Pctl > 0.8")
        
    if ret_120 < 0 and hv20_pctl > 0.8:
        is_risk = True
        reasons.append("120d Return < 0 & HV20_Pctl > 0.8")
        
    if is_risk:
        return f"HIGH_RISK ({'|'.join(reasons)})", True
    return "NORMAL", False


def main():
    args = parse_args()
    
    print("====================================================================")
    print("ğŸš€ US Tech Stock - Daily Train & Predictor (Single Ticker Rank)")
    print("====================================================================")
    
    today_str = datetime.today().strftime("%Y%m%d")
    output_daily_dir = os.path.join(args.output_dir, today_str)
    os.makedirs(output_daily_dir, exist_ok=True)
    
    # 1. æŠ“å–è³‡æ–™
    start_date = (datetime.today() - timedelta(days=args.lookback_years*365)).strftime("%Y-%m-%d")
    print(f"ğŸ“¥ æ­£åœ¨å¾ Yahoo Finance ç²å–/æ›´æ–°æœ€æ–°è‚¡åƒ¹ (è‡ª {start_date} èµ·)...")
    try:
         all_data = fetch_all_stock_data(start_date=start_date)
         benchmark_df = all_data.get(BENCHMARK)
         if benchmark_df is None: raise ValueError(f"ç„¡æ³•è¼‰å…¥åŸºæº– {BENCHMARK}")
    except Exception as e:
         print(f"âŒ {e}")
         sys.exit(1)
         
    use_regime = (args.use_regime_features == "true")
    active_cols = FEATURE_COLS + (REGIME_COLS if use_regime else [])
    target_col = f"Next_{args.target_days}d_Max"
    
    # 2. Proxy Risk è¨ˆç®—
    risk_status_text, is_high_risk = evaluate_regime_risk(benchmark_df)
    regime_df = compute_regime_features(benchmark_df) if use_regime else None
    
    # æ±ºå®šä»Šå¤©ç”¨çš„ Threshold
    active_threshold_pct = args.risk_threshold_pct if is_high_risk else args.topk_threshold_pct
    
    print(f"  [é¢¨æ§ç‹€æ…‹] {risk_status_text} | é è¨ˆä½¿ç”¨é–€æª»: åˆ†ä½æ•¸ >= {active_threshold_pct*100:g}%")
    
    results = [] # output row dictionaries
    run_summary = {
        "run_date": today_str,
        "target_days": args.target_days,
        "target_return": args.target_return,
        "window_years": args.window_years,
        "tickers": args.tickers,
        "global_risk_state": "High Risk" if is_high_risk else "Normal",
        "ticker_summaries": {}
    }
    
    # 3. é€æª”é–‹å§‹ Train & Predict æµç¨‹
    for tk in args.tickers:
        raw_df = all_data.get(tk)
        if raw_df is None or len(raw_df) == 0:
            print(f"âš ï¸ {tk}: ç„¡æ³•å–å¾—è¶³å¤ å ±åƒ¹ï¼Œè·³éã€‚")
            continue
            
        print(f"\nâš™ï¸ è™•ç†è‚¡ç¥¨ [{tk}] ...")
        
        # A) ç‰¹å¾µè£é…
        feat_df = calculate_features(raw_df, benchmark_df, ticker=tk, use_cache=not args.no_cache)
        if target_col not in feat_df.columns:
            print(f"  {tk}: æœªæ‰¾åˆ°æŒ‡å®šçš„ Target Col {target_col}ï¼Œè·³éã€‚")
            continue
            
        feat_df = feat_df.reset_index()
        if 'Date' in feat_df.columns: feat_df.rename(columns={'Date': 'date'}, inplace=True)
        elif 'index' in feat_df.columns: feat_df.rename(columns={'index': 'date'}, inplace=True)
        feat_df['date'] = pd.to_datetime(feat_df['date'])
        
        if use_regime:
            feat_df['date_str'] = feat_df['date'].dt.strftime('%Y-%m-%d')
            feat_df = pd.merge(feat_df, regime_df, left_on='date_str', right_on='date', how='inner', suffixes=('', '_regime'))
        
        # B) æ¨¡å‹è·¯å¾‘èˆ‡è¨“ç·´
        ticker_model_dir = os.path.join(output_daily_dir, tk)
        os.makedirs(ticker_model_dir, exist_ok=True)
        
        is_legacy_mode = (args.model_path is not None)
        if is_legacy_mode:
             model_path = args.model_path.replace("{ticker}", tk)
             model_type = "ppo" if ".zip" in model_path else "sklearn"
        else:
             model_path = os.path.join(ticker_model_dir, "model.joblib")
             model_type = "daily"
             
        # C) è³‡æ–™åˆ‡ç‰‡ [Today - 3y, Today] (çµ¦ Daily Train ç”¨ï¼ŒLegacy ä¹Ÿè¦ç®—å‡ºå¯¦éš›ç¯„åœä»¥ä¾›è¨˜éŒ„)
        train_end = feat_df['date'].max()
        train_start = train_end - pd.DateOffset(years=args.window_years)
        
        mask_train = (feat_df['date'] >= train_start) & (feat_df['date'] <= train_end)
        train_slice = feat_df[mask_train].dropna(subset=active_cols + [target_col]).copy()
        
        n_train = len(train_slice)
        if n_train == 0:
             print(f"  {tk}: è³‡æ–™å›  NA æˆ–éçŸ­è€Œæ¸…ç©ºï¼Œç„¡æ³•å»ºç«‹æ¨¡å‹é æ¸¬ã€‚")
             run_summary["ticker_summaries"][tk] = {"status": "Error: Insufficient Data"}
             continue
             
        train_slice['y'] = (train_slice[target_col] >= args.target_return).astype(int)
        pos_rate = train_slice['y'].mean()
        
        if not is_legacy_mode:
            if os.path.exists(model_path) and not args.force_retrain:
                print(f"  [{tk}] æ¨¡å‹å·²å¿«å–ï¼Œçœç•¥è¨“ç·´ã€‚")
            else:
                from sklearn.ensemble import HistGradientBoostingClassifier
                model = HistGradientBoostingClassifier(random_state=42)
                model.fit(train_slice[active_cols], train_slice['y'])
                joblib.dump(model, model_path)
                print(f"  [{tk}] å–®æª”æ¨¡å‹è¨“ç·´å®Œç•¢ (Train size: {n_train}, Pos Rate: {pos_rate*100:.2f}%)")
        
        # D) æ¨è«–ä»Šå¤© p_today
        # ä»Šå¤©æ˜¯åŒ…å«åœ¨ feat_df æœ€å¾Œä¸€ç­† (å› ç‚º target_col NaNs ä¹Ÿè¢«ç®—é€² calculate_features)
        # å¿…é ˆæ‰‹å‹•å– feat_df æœ€å¾Œä¸€ç­†ä¸¦ç¢ºä¿ active_cols ç„¡ NaN
        latest_feat = feat_df.iloc[-1:].copy()
        latest_date_str = latest_feat['date'].iloc[0].strftime("%Y-%m-%d")
        
        if latest_feat[active_cols].isnull().any().any():
             print(f"  [{tk}] æœ€æ–°ä¸€ç­†è³‡æ–™({latest_date_str})ç‰¹å¾µå­˜åœ¨ç©ºå€¼ï¼Œé€€å‡ºã€‚")
             # æˆ–è¨±éƒ¨åˆ† regime é‚„æœªæ›´æ–°æ‰€ä»¥æœ€å¾Œä¸€å¤©ç©ºå€¼ï¼Œå®‰å…¨èµ·è¦‹æˆ‘å€‘å– feat_df.dropna(subset=active_cols).iloc[-1:] 
             # ä½†é€™æœ€ç¬¦åˆä½¿ç”¨è€…æ‰€èªçŸ¥çš„ã€Œä»Šå¤©(æˆ–æœ€æ–°ä¸€ç­†æœ‰æ•ˆæ—¥)ã€ä¹‹æ¢ä»¶
             latest_feat = feat_df.dropna(subset=active_cols).iloc[-1:]
             if len(latest_feat) == 0: continue
             latest_date_str = latest_feat['date'].iloc[0].strftime("%Y-%m-%d")
             
        X_today = pd.DataFrame(latest_feat[active_cols])
        if model_type == 'ppo': X_today = X_today.values.astype(np.float32)
        p_today = load_model_and_predict(model_path, model_type, X_today)
        
        # E) è¨ˆç®—æ­·å²åˆ†ä½æ•¸ p_history (pct_lookback_days)
        # å–æœ‰æ•ˆç‰¹å¾µçš„æ­·å²è³‡æ–™
        valid_history_df = feat_df.dropna(subset=active_cols).copy()
        # åˆ‡éå» pct_lookback_days ç­† (ä¸å«ä»Šå¤©è‡ªå·±ï¼Œæˆ–è€…å«ä¹Ÿå¯ä»¥ï¼Œä¸å½±éŸ¿å¤§å±€)
        base_lookback_df = valid_history_df.iloc[-(args.pct_lookback_days+1):-1]
        
        if len(base_lookback_df) < (args.pct_lookback_days // 2): 
             # Fallback
             pct_rank_today = np.nan
             print(f"  [{tk}] æ­·å²å¯ç”¨ç´€éŒ„ {len(base_lookback_df)} å¤©éçŸ­ï¼Œç„¡æ³•è¨ˆç®—å¯é çš„ Percentile (>50% required)ã€‚")
        else:
             X_hist = pd.DataFrame(base_lookback_df[active_cols])
             if model_type == 'ppo': X_hist = X_hist.values.astype(np.float32)
             p_hist_array = load_model_and_predict(model_path, model_type, X_hist)
             # scipy percentileofscore [0, 100]
             pct_rank_today = percentileofscore(p_hist_array, p_today) / 100.0
             
        # F) æ±ºç­–åˆ¤æ–·
        action = "WATCH"
        position_scale = 0.0
        
        if np.isnan(pct_rank_today):
             action = "WATCH_INSUFFICIENT_DATA"
        elif pct_rank_today >= active_threshold_pct:
             if is_high_risk:
                 action = "BUY_REDUCED"
                 position_scale = 0.5
             else:
                 action = "BUY"
                 position_scale = 1.0
        else:
             if is_high_risk:
                 action = "SKIP_RISK"
        
        print(f"  [{tk}] P({args.target_days}): {p_today*100:.2f}% | PctRank(252d): {pct_rank_today*100 if not np.isnan(pct_rank_today) else np.nan:.1f}% => {action}")
        
        # ç´€éŒ„æª”ä¿å­˜
        results.append({
             "date": latest_date_str,
             "ticker": tk,
             "p_today": float(p_today),
             "pct_rank_today": float(pct_rank_today),
             "action": action,
             "position_scale": float(position_scale),
             "is_high_risk": is_high_risk,
             "threshold_pct_used": float(active_threshold_pct),
             "train_start_requested": train_start.strftime('%Y-%m-%d'),
             "train_end_requested": train_end.strftime('%Y-%m-%d'),
             "train_start_actual": train_slice['date'].min().strftime('%Y-%m-%d') if len(train_slice) > 0 else "N/A",
             "train_end_actual": train_slice['date'].max().strftime('%Y-%m-%d') if len(train_slice) > 0 else "N/A",
             "n_train": n_train,
             "pos_rate_train": float(pos_rate)
        })
        
        run_summary["ticker_summaries"][tk] = {
             "model_path": model_path,
             "n_train": n_train,
             "pos_rate": pos_rate,
             "valid_history_days": len(base_lookback_df)
        }

    # 4. CSV èˆ‡ JSON å¯«æª”
    csv_path = os.path.join(output_daily_dir, "predictions.csv")
    json_path = os.path.join(output_daily_dir, "run_summary.json")
    
    if len(results) > 0:
        df_out = pd.DataFrame(results)
        df_out.to_csv(csv_path, index=False)
        
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(run_summary, f, indent=4)
        
    # --- å ±è¡¨è¼¸å‡º ---
    print("\nğŸ“Š ä»Šæ—¥æ¨è«–çµæœ (Single Ticker Approach)")
    print("-" * 88)
    print(f"{'Ticker':<8} | {'Latest Date':<12} | {'Score(p)':<10} | {'PctRank':<8} | {'Act Thresh':<10} | {'Action':<15} | {'Pos Scale'}")
    print("-" * 88)
    for r in results:
        pct_str = f"{r['pct_rank_today']*100:.1f}%" if not np.isnan(r['pct_rank_today']) else "N/A"
        print(f"{r['ticker']:<8} | {r['date']:<12} | {r['p_today']*100:6.2f}%    | {pct_str:<8} | >={r['threshold_pct_used']*100:g}%     | {r['action']:<15} | x{r['position_scale']}")
        
    print("-" * 88)
    print(f"ğŸ“ å ±å‘Šè¼¸å‡ºå®Œæˆæ–¼: {output_daily_dir}")
    print(f"âœ… predictions.csv èˆ‡ run_summary.json å·²æ›´æ–°æª”æ¡ˆ")
    print("====================================================================\n")

if __name__ == "__main__":
    main()
