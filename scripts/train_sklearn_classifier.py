#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
================================================================================
US Tech Stock - Sklearn Binary Classifier Training Script
================================================================================
Áî®ÊñºË®ìÁ∑¥‰∫åÂàÜÂàÜÈ°ûÊ®°ÂûãÔºåÈ†êÊ∏¨Êú™‰æÜ 20 ÂÄã‰∫§ÊòìÊó•ÂÖßÊòØÂê¶‰∏äÊº≤Ë∂ÖÈÅé 10%„ÄÇ
Â∞áÈáçÁî® train_us_tech_buy_agent.py ÂÖßÁöÑ fetch_all_stock_data Ëàá calculate_features„ÄÇ

ÊîØÊè¥Ê®°Âûã: RandomForest, AdaBoost, HistGradientBoosting
================================================================================
"""

import os
import sys
import json
import joblib
import argparse
from datetime import datetime
import numpy as np
import pandas as pd

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, HistGradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, 
                             roc_auc_score, average_precision_score, confusion_matrix)
from sklearn.utils.class_weight import compute_class_weight
from sklearn.inspection import permutation_importance

# Â∞áÂ∞àÊ°àÊ†πÁõÆÈåÑÂä†Âà∞ sys.pathÔºå‰ª•‰æø import ÂÖ±Áî®Ê®°ÁµÑ
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

from src.train.sklearn_utils import get_positive_proba, apply_class_balancing, get_model, calc_metrics, get_feature_importances

try:
    from train_us_tech_buy_agent import (fetch_all_stock_data, calculate_features, 
                                         FEATURE_COLS, TRAIN_RANGES, VAL_RANGE, BENCHMARK)
except ImportError as e:
    print(f"‚ùå ÁÑ°Ê≥ïÂæû train_us_tech_buy_agent.py ËºâÂÖ•ÂÖ±Áî®ÈÇèËºØ: {e}")
    print("Ë´ãÁ¢∫‰øùËÖ≥Êú¨ÊúâÊîæÁΩÆÂú®Ê≠£Á¢∫ÁöÑÊ†πÁõÆÈåÑ‰∏ãÂ±§ scripts Ë≥áÊñôÂ§æÂÖß„ÄÇ")
    sys.exit(1)


def parse_args():
    parser = argparse.ArgumentParser(description="Train Sklearn binary classifier for Buy Agent (Next 20d Max >= 10%)")
    
    # Ë≥áÊñôÂèÉÊï∏
    parser.add_argument("--tickers", nargs="+", 
                        default=["NVDA", "MSFT", "AAPL", "AMZN", "META", "AVGO", "GOOGL", "TSLA", "NFLX", "PLTR", "TSM"],
                        help="ÁõÆÊ®ôËÇ°Á•®‰ª£Á¢ºÂàóË°®")
    parser.add_argument("--train-ranges", nargs="*", 
                        help="Ë®ìÁ∑¥Ë≥áÊñôÂçÄÈñì„ÄÇÊ†ºÂºè: YYYY-MM-DD:YYYY-MM-DD (ÂèØÊèê‰æõÂ§öÊÆµÔºåÁ©∫ÁôΩÂàÜÈöî)")
    parser.add_argument("--val-start-date", type=str, help="È©óË≠âÂçÄÈñìËµ∑ÂßãÊó•")
    parser.add_argument("--val-end-date", type=str, help="È©óË≠âÂçÄÈñìÁµêÊùüÊó•")
    
    # Ë®ìÁ∑¥ÂèÉÊï∏
    parser.add_argument("--model", choices=["rf", "adaboost", "hgb"], default="rf",
                        help="ÈÅ∏ÊìáË®ìÁ∑¥Ê®°ÂûãÁ®ÆÈ°û")
    parser.add_argument("--target-days", type=int, default=20, help="È†êÊ∏¨Êú™‰æÜÁöÑ‰∫§ÊòìÂ§©Êï∏ (e.g. 20, 60, 120)")
    parser.add_argument("--target-return", type=float, default=0.10, help="ÁõÆÊ®ôÊúÄÈ´òÂÉπÊº≤ÂπÖÈñÄÊ™ª (e.g. 0.10, 0.20)")
    parser.add_argument("--balance-train", choices=["none", "undersample_50_50", "class_weight_balanced"], 
                        default="none", help="È°ûÂà•‰∏çÂπ≥Ë°°ËôïÁêÜÊñπÂºè (ÂÉÖ‰ΩúÁî®ÊñºË®ìÁ∑¥ÈõÜ)")
    parser.add_argument("--seed", type=int, default=42, help="Èö®Ê©üÁ®ÆÂ≠ê")
    
    # Áí∞Â¢ÉËàáËº∏Âá∫ÂèÉÊï∏
    parser.add_argument("--output-dir", default=os.path.join(ROOT_DIR, "output_sklearn"), 
                        help="Ê®°ÂûãËàáÊåáÊ®ôËº∏Âá∫ÁõÆÈåÑ")
    parser.add_argument("--no-cache", action="store_true", help="ÈóúÈñâÁâπÂæµÂø´Âèñ")
    parser.add_argument("--dry-run", action="store_true", 
                        help="Âè™Ê™¢Êü•Ë≥áÊñôËàáÂàáÂàÜÔºå‰∏çÈÄ≤Ë°åÂØ¶ÈöõË®ìÁ∑¥")
    
    return parser.parse_args()


def get_positive_proba(model, X, positive_label=1) -> tuple:
    """
    ÂèñÂæóÈ†êÊ∏¨ÁÇ∫ positive_label (È†êË®≠ÁÇ∫ 1) ÁöÑÊ©üÁéáÈô£Âàó„ÄÇ
    ÈÅøÂÖç‰æùË≥¥ hard-coded ÁöÑ [:, 1]ÔºåÊîπÁî± model.classes_ ÂãïÊÖãÂ∞ãÊâæ„ÄÇ
    """
    if not hasattr(model, "predict_proba"):
        raise ValueError(f"Ê®°Âûã {type(model).__name__} ‰∏çÊîØÊè¥ predict_proba()")
        
    proba_all = model.predict_proba(X)
    classes = list(model.classes_)
    
    if positive_label not in classes:
        raise ValueError(f"Ê®ôÁ±§ {positive_label} ‰∏çÂ≠òÂú®Êñº model.classes_ {classes} ‰∏≠„ÄÇ")
        
    pos_idx = classes.index(positive_label)
    return proba_all[:, pos_idx], classes, pos_idx


def parse_date_ranges(train_ranges_arg):
    if not train_ranges_arg:
        return TRAIN_RANGES
    parsed = []
    for r in train_ranges_arg:
        parts = r.split(':')
        if len(parts) == 2:
            parsed.append((parts[0], parts[1]))
        else:
            raise ValueError(f"Ë®ìÁ∑¥ÂçÄÈñìÊ†ºÂºèÈåØË™§: {r} (ÊáâÁÇ∫ YYYY-MM-DD:YYYY-MM-DD)")
    return parsed


def prepare_data(args, train_ranges, val_range):
    """
    ËºâÂÖ•‰∏¶ËôïÁêÜË≥áÊñôÔºåËº∏Âá∫Ë®ìÁ∑¥ÈõÜËàáÈ©óË≠âÈõÜ
    """
    all_raw_data = fetch_all_stock_data()
    benchmark_df = all_raw_data.get(BENCHMARK)
    if benchmark_df is None:
        raise ValueError(f"ÁÑ°Ê≥ïËºâÂÖ• benchmark {BENCHMARK} ÁöÑË≥áÊñô„ÄÇ")
        
    use_cache = not args.no_cache
    target_col = f"Next_{args.target_days}d_Max"
    
    train_dfs = []
    val_dfs = []
    
    print(f"\nüîç Ê≠£Âú®ÁîüÊàê/ËºâÂÖ•ÁâπÂæµ... (ÁõÆÊ®ô: {target_col} >= {args.target_return*100:g}%)")
    for ticker in args.tickers:
        if ticker not in all_raw_data:
            print(f"  ‚ö†Ô∏è Êâæ‰∏çÂà∞ {ticker} ÂéüÂßãË≥áÊñôÔºåË∑≥ÈÅé„ÄÇ")
            continue
            
        df_raw = all_raw_data[ticker]
        df_features = calculate_features(df_raw, benchmark_df, ticker=ticker, use_cache=use_cache)
        
        # 1. Á¢∫‰øùÁõÆÊ®ôÊ¨Ñ‰ΩçÂ≠òÂú®‰∏¶ÈÅéÊøæ NaN
        if target_col not in df_features.columns:
            print(f"  ‚ö†Ô∏è Êâæ‰∏çÂà∞ÁâπÂæµÊ¨Ñ‰Ωç {target_col}ÔºåË´ãÁ¢∫ÂÆö calculate_features Â∑≤ÊîØÊè¥Ë©≤Â§©Êï∏„ÄÇ")
            continue
        df_features = df_features.dropna(subset=[target_col])
        
        # 2. Âä†ÂÖ• date Ëàá ticker
        df_features['ticker'] = ticker
        df_features['date'] = df_features.index.strftime('%Y-%m-%d')
        
        # 3. Âª∫Á´ãÊ®ôÁ±§ y
        df_features['y'] = (df_features[target_col] >= args.target_return).astype(int)
        
        # 4. ÊôÇÈñìÂàáÂàÜ (Walk-forward Split)
        # Ë®ìÁ∑¥ÈõÜ
        train_mask = pd.Series(False, index=df_features.index)
        for start, end in train_ranges:
            train_mask |= (df_features.index >= pd.Timestamp(start)) & (df_features.index <= pd.Timestamp(end))
        df_train_tick = df_features[train_mask]
        
        # È©óË≠âÈõÜ
        val_start, val_end = val_range
        val_mask = (df_features.index >= pd.Timestamp(val_start)) & (df_features.index <= pd.Timestamp(val_end))
        df_val_tick = df_features[val_mask]
        
        train_dfs.append(df_train_tick)
        val_dfs.append(df_val_tick)
        
    df_train = pd.concat(train_dfs, ignore_index=True) if train_dfs else pd.DataFrame()
    df_val = pd.concat(val_dfs, ignore_index=True) if val_dfs else pd.DataFrame()
    
    return df_train, df_val


def print_data_stats(df_train, df_val, tickers):
    """Âç∞Âá∫Ë≥áÊñôÈõÜÁµ±Ë®àË≥áË®ä"""
    print("\nüìä Ë≥áÊñôÂàáÂàÜËàáÈ°ûÂà•ÊØî‰æãÁµ±Ë®à")
    print("-" * 60)
    print(f"{'Ticker':<8} | {'Train (N)':<10} | {'Train Pos%':<10} | {'Val (N)':<10} | {'Val Pos%':<10}")
    print("-" * 60)
    
    for tk in tickers:
        d_tr = df_train[df_train['ticker'] == tk]
        d_va = df_val[df_val['ticker'] == tk]
        tr_len = len(d_tr)
        va_len = len(d_va)
        tr_pos = d_tr['y'].mean() if tr_len > 0 else 0
        va_pos = d_va['y'].mean() if va_len > 0 else 0
        print(f"{tk:<8} | {tr_len:<10} | {tr_pos*100:6.2f}%    | {va_len:<10} | {va_pos*100:6.2f}%")
        
    print("-" * 60)
    tot_tr_len = len(df_train)
    tot_va_len = len(df_val)
    tot_tr_pos = df_train['y'].mean() if tot_tr_len > 0 else 0
    tot_va_pos = df_val['y'].mean() if tot_va_len > 0 else 0
    print(f"{'TOTAL':<8} | {tot_tr_len:<10} | {tot_tr_pos*100:6.2f}%    | {tot_va_len:<10} | {tot_va_pos*100:6.2f}%")
    print("-" * 60)


def main():
    args = parse_args()
    
    # Ê±∫ÂÆöÂàáÂâ≤ÂçÄÊÆµ
    train_ranges = parse_date_ranges(args.train_ranges)
    val_range = (
        args.val_start_date if args.val_start_date else VAL_RANGE[0],
        args.val_end_date if args.val_end_date else VAL_RANGE[1]
    )
    
    print("=" * 60)
    print("üöÄ Sklearn Binary Classifier Training")
    print("=" * 60)
    print(f"  Model       : {args.model}")
    print(f"  Target      : Next_{args.target_days}d_Max >= {args.target_return*100:g}%")
    print(f"  Tickers     : {', '.join(args.tickers)}")
    print(f"  Train Ranges: {train_ranges}")
    print(f"  Val Range   : {val_range}")
    print(f"  Balance Mode: {args.balance_train}")
    print(f"  Dry Run     : {args.dry_run}")
    print("=" * 60)
    
    # 1. Ê∫ñÂÇôË≥áÊñô
    df_train, df_val = prepare_data(args, train_ranges, val_range)
    
    if len(df_train) == 0 or len(df_val) == 0:
        print("‚ùå Ë®ìÁ∑¥ÊàñÈ©óË≠âË≥áÊñôÈõÜÁÇ∫Á©∫ÔºåË´ãÊ™¢Êü•Êó•ÊúüËàáË≥áÊñô‰∏ãËºâÁãÄÊÖã„ÄÇ")
        sys.exit(1)
        
    # Âç∞Âá∫È†êË®≠Áµ±Ë®àÂàÜÂ∏É
    print_data_stats(df_train, df_val, args.tickers)
    
    # Â¶ÇÊûúÊòØ Dry-run Â∞±Áõ¥Êé•ÁµêÊùü
    if args.dry_run:
        print("\n‚úÖ Dry-Run Ê®°ÂºèÁµêÊùü„ÄÇ")
        sys.exit(0)
    
    # 2. È°ûÂà•Âπ≥Ë°° (ÂÉÖÂú®Ë®ìÁ∑¥ÈöéÊÆµËôïÁêÜ)
    df_train_b = apply_class_balancing(df_train, args.balance_train, args.seed)
    
    X_train = df_train_b[FEATURE_COLS]
    y_train = df_train_b['y']
    
    X_val = df_val[FEATURE_COLS]
    y_val = df_val['y']
    
    # 3. Ê∫ñÂÇôÊ®°Âûã
    model, needs_sample_weight = get_model(args.model, args.balance_train, args.seed)
    
    sample_weight = None
    if needs_sample_weight:
        cw = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        w_dict = dict(zip(np.unique(y_train), cw))
        sample_weight = np.array([w_dict[y] for y in y_train])
        
    print(f"\n‚öôÔ∏è  ÈñãÂßãË®ìÁ∑¥ {args.model.upper()} ...")
    if sample_weight is not None:
        model.fit(X_train, y_train, sample_weight=sample_weight)
    else:
        model.fit(X_train, y_train)
        
    # 4. È†êÊ∏¨ËàáË®àÁÆóÊåáÊ®ô
    print("üìà Ê≠£Âú®Â∞ç Validation Subset ÈÄ≤Ë°åË©ï‰º∞...")
    try:
        y_proba_val, clz_list, pos_idx = get_positive_proba(model, X_val, positive_label=1)
    except Exception as e:
        print(f"‚ùå ÂèñÂæóÈ†êÊ∏¨Ê©üÁéáÂ§±Êïó: {e}")
        sys.exit(1)
        
    y_pred_val = model.predict(X_val)
    
    # Ë®àÁÆóÊ≠£Ë≤†Ê®£Êú¨ÁöÑÂπ≥ÂùáÊ©üÁéá (Sanity Check)
    mask_pos = (y_val == 1)
    mask_neg = (y_val == 0)
    mean_pos_proba = y_proba_val[mask_pos].mean() if mask_pos.sum() > 0 else 0.0
    mean_neg_proba = y_proba_val[mask_neg].mean() if mask_neg.sum() > 0 else 0.0
    
    proba_direction_warning = False
    if mean_pos_proba < mean_neg_proba:
        print(f"  ‚ö†Ô∏è [WARNING] Ê≠£Ê®£Êú¨ÁöÑÂπ≥ÂùáÈ†êÊ∏¨Ê©üÁéá ({mean_pos_proba:.4f}) Â∞èÊñº Ë≤†Ê®£Êú¨ ({mean_neg_proba:.4f})ÔºÅ")
        print("     ÈÄôÂèØËÉΩÊöóÁ§∫ÂàÜÈ°ûÂô®ÁöÑÂ≠∏ÁøíÁµêÊûúÊñπÂêëÁõ∏ÂèçÔºåÊàñÊ≠£È°ûË¢´ÈåØË™§Â∞çÊáâ„ÄÇROC-AUC ÂèØËÉΩ < 0.5„ÄÇ")
        proba_direction_warning = True
    
    metrics = calc_metrics(y_val, y_proba_val, y_pred_val, prefix="Pooled Overall")
    metrics['Sanity Check'] = {
        'mean_pos_proba': float(mean_pos_proba),
        'mean_neg_proba': float(mean_neg_proba),
        'proba_direction_warning': proba_direction_warning
    }
    
    # Ë®àÁÆó Feature Importances
    importances = get_feature_importances(model, args.model, X_val, y_val, FEATURE_COLS)
    metrics['Feature Importances'] = importances
    
    # 5. Ëº∏Âá∫ÂÑ≤Â≠ò
    run_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(args.output_dir, f"run_{args.model}_{args.target_days}d_{run_ts}")
    os.makedirs(run_dir, exist_ok=True)
    
    # (a) Model joblib
    joblib.dump(model, os.path.join(run_dir, "model.joblib"))
    
    # (b) Params Json
    params = {
        "cli_args": vars(args),
        "target_definition": f"Next_{args.target_days}d_Max >= {args.target_return}",
        "actual_train_ranges": train_ranges,
        "actual_val_range": val_range,
        "train_samples_raw": len(df_train),
        "train_samples_balanced": len(df_train_b),
        "val_samples": len(df_val),
        "impl_details": {
            "balance_application": "sample_weight passed to fit" if sample_weight is not None else ("class_weight arg passed" if args.balance_train == "class_weight_balanced" else args.balance_train),
            "model_classes": [int(c) for c in clz_list],
            "positive_class_index": int(pos_idx)
        }
    }
    with open(os.path.join(run_dir, "params.json"), "w", encoding="utf-8") as f:
        json.dump(params, f, indent=4, ensure_ascii=False)
        
    # (c) Metrics Json
    with open(os.path.join(run_dir, "metrics.json"), "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=4, ensure_ascii=False)
        
    # (d) Prediction CSV
    df_val_export = df_val[['date', 'ticker']].copy()
    df_val_export['y_true'] = y_val
    df_val_export['y_proba'] = y_proba_val
    df_val_export['y_pred'] = y_pred_val
    df_val_export.to_csv(os.path.join(run_dir, "val_predictions.csv"), index=False)
    
    print("\n‚úÖ Ë®ìÁ∑¥ÂÆåÊàêÔºÅ")
    print("-" * 60)
    print(f"  [Validation Metrics (Pooled / Micro)]")
    print(f"  Accuracy : {metrics['Accuracy']:.4f}")
    print(f"  ROC-AUC  : {metrics['ROC-AUC']:.4f}" if metrics['ROC-AUC'] else "  ROC-AUC  : N/A")
    print(f"  PR-AUC   : {metrics['PR-AUC']:.4f}" if metrics['PR-AUC'] else "  PR-AUC   : N/A")
    print(f"  Precision: {metrics['Precision']:.4f}")
    print(f"  Recall   : {metrics['Recall']:.4f}")
    print(f"  F1-Score : {metrics['F1']:.4f}")
    print(f"\nüìÇ ÁµêÊûúÂ∑≤ÂÑ≤Â≠òÊñº: {run_dir}")

if __name__ == "__main__":
    main()
