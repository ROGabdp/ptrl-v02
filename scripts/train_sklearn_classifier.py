#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
================================================================================
US Tech Stock - Sklearn Binary Classifier Training Script
================================================================================
ç”¨æ–¼è¨“ç·´äºŒåˆ†åˆ†é¡æ¨¡å‹ï¼Œé æ¸¬æœªä¾† 20 å€‹äº¤æ˜“æ—¥å…§æ˜¯å¦ä¸Šæ¼²è¶…é 10%ã€‚
å°‡é‡ç”¨ train_us_tech_buy_agent.py å…§çš„ fetch_all_stock_data èˆ‡ calculate_featuresã€‚

æ”¯æ´æ¨¡å‹: RandomForest, AdaBoost, HistGradientBoosting
================================================================================
"""

import os
import sys
import json
import joblib
import argparse
from datetime import datetime
import numpy as np
import pandas as pd

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, HistGradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, 
                             roc_auc_score, average_precision_score, confusion_matrix)
from sklearn.utils.class_weight import compute_class_weight
from sklearn.inspection import permutation_importance

# å°‡å°ˆæ¡ˆæ ¹ç›®éŒ„åŠ åˆ° sys.pathï¼Œä»¥ä¾¿ import å…±ç”¨æ¨¡çµ„
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if ROOT_DIR not in sys.path:
    sys.path.append(ROOT_DIR)

try:
    from train_us_tech_buy_agent import (fetch_all_stock_data, calculate_features, 
                                         FEATURE_COLS, TRAIN_RANGES, VAL_RANGE, BENCHMARK)
except ImportError as e:
    print(f"âŒ ç„¡æ³•å¾ train_us_tech_buy_agent.py è¼‰å…¥å…±ç”¨é‚è¼¯: {e}")
    print("è«‹ç¢ºä¿è…³æœ¬æœ‰æ”¾ç½®åœ¨æ­£ç¢ºçš„æ ¹ç›®éŒ„ä¸‹å±¤ scripts è³‡æ–™å¤¾å…§ã€‚")
    sys.exit(1)


def parse_args():
    parser = argparse.ArgumentParser(description="Train Sklearn binary classifier for Buy Agent (Next 20d Max >= 10%)")
    
    # è³‡æ–™åƒæ•¸
    parser.add_argument("--tickers", nargs="+", 
                        default=["NVDA", "MSFT", "AAPL", "AMZN", "META", "AVGO", "GOOGL", "TSLA", "NFLX", "PLTR"],
                        help="ç›®æ¨™è‚¡ç¥¨ä»£ç¢¼åˆ—è¡¨")
    parser.add_argument("--train-ranges", nargs="*", 
                        help="è¨“ç·´è³‡æ–™å€é–“ã€‚æ ¼å¼: YYYY-MM-DD:YYYY-MM-DD (å¯æä¾›å¤šæ®µï¼Œç©ºç™½åˆ†éš”)")
    parser.add_argument("--val-start-date", type=str, help="é©—è­‰å€é–“èµ·å§‹æ—¥")
    parser.add_argument("--val-end-date", type=str, help="é©—è­‰å€é–“çµæŸæ—¥")
    
    # è¨“ç·´åƒæ•¸
    parser.add_argument("--model", choices=["rf", "adaboost", "hgb"], default="rf",
                        help="é¸æ“‡è¨“ç·´æ¨¡å‹ç¨®é¡")
    parser.add_argument("--balance-train", choices=["none", "undersample_50_50", "class_weight_balanced"], 
                        default="none", help="é¡åˆ¥ä¸å¹³è¡¡è™•ç†æ–¹å¼ (åƒ…ä½œç”¨æ–¼è¨“ç·´é›†)")
    parser.add_argument("--seed", type=int, default=42, help="éš¨æ©Ÿç¨®å­")
    
    # ç’°å¢ƒèˆ‡è¼¸å‡ºåƒæ•¸
    parser.add_argument("--output-dir", default=os.path.join(ROOT_DIR, "output_sklearn"), 
                        help="æ¨¡å‹èˆ‡æŒ‡æ¨™è¼¸å‡ºç›®éŒ„")
    parser.add_argument("--no-cache", action="store_true", help="é—œé–‰ç‰¹å¾µå¿«å–")
    parser.add_argument("--dry-run", action="store_true", 
                        help="åªæª¢æŸ¥è³‡æ–™èˆ‡åˆ‡åˆ†ï¼Œä¸é€²è¡Œå¯¦éš›è¨“ç·´")
    
    return parser.parse_args()


def parse_date_ranges(train_ranges_arg):
    if not train_ranges_arg:
        return TRAIN_RANGES
    parsed = []
    for r in train_ranges_arg:
        parts = r.split(':')
        if len(parts) == 2:
            parsed.append((parts[0], parts[1]))
        else:
            raise ValueError(f"è¨“ç·´å€é–“æ ¼å¼éŒ¯èª¤: {r} (æ‡‰ç‚º YYYY-MM-DD:YYYY-MM-DD)")
    return parsed


def prepare_data(args, train_ranges, val_range):
    """
    è¼‰å…¥ä¸¦è™•ç†è³‡æ–™ï¼Œè¼¸å‡ºè¨“ç·´é›†èˆ‡é©—è­‰é›†
    """
    all_raw_data = fetch_all_stock_data()
    benchmark_df = all_raw_data.get(BENCHMARK)
    if benchmark_df is None:
        raise ValueError(f"ç„¡æ³•è¼‰å…¥ benchmark {BENCHMARK} çš„è³‡æ–™ã€‚")
        
    use_cache = not args.no_cache
    
    train_dfs = []
    val_dfs = []
    
    print("\nğŸ” æ­£åœ¨ç”Ÿæˆ/è¼‰å…¥ç‰¹å¾µ...")
    for ticker in args.tickers:
        if ticker not in all_raw_data:
            print(f"  âš ï¸ æ‰¾ä¸åˆ° {ticker} åŸå§‹è³‡æ–™ï¼Œè·³éã€‚")
            continue
            
        df_raw = all_raw_data[ticker]
        df_features = calculate_features(df_raw, benchmark_df, ticker=ticker, use_cache=use_cache)
        
        # 1. ç¢ºä¿ç›®æ¨™æ¬„ä½å­˜åœ¨ä¸¦éæ¿¾ NaN
        if 'Next_20d_Max' not in df_features.columns:
            continue
        df_features = df_features.dropna(subset=['Next_20d_Max'])
        
        # 2. åŠ å…¥ date èˆ‡ ticker
        df_features['ticker'] = ticker
        df_features['date'] = df_features.index.strftime('%Y-%m-%d')
        
        # 3. å»ºç«‹æ¨™ç±¤ y
        df_features['y'] = (df_features['Next_20d_Max'] >= 0.10).astype(int)
        
        # 4. æ™‚é–“åˆ‡åˆ† (Walk-forward Split)
        # è¨“ç·´é›†
        train_mask = pd.Series(False, index=df_features.index)
        for start, end in train_ranges:
            train_mask |= (df_features.index >= pd.Timestamp(start)) & (df_features.index <= pd.Timestamp(end))
        df_train_tick = df_features[train_mask]
        
        # é©—è­‰é›†
        val_start, val_end = val_range
        val_mask = (df_features.index >= pd.Timestamp(val_start)) & (df_features.index <= pd.Timestamp(val_end))
        df_val_tick = df_features[val_mask]
        
        train_dfs.append(df_train_tick)
        val_dfs.append(df_val_tick)
        
    df_train = pd.concat(train_dfs, ignore_index=True) if train_dfs else pd.DataFrame()
    df_val = pd.concat(val_dfs, ignore_index=True) if val_dfs else pd.DataFrame()
    
    return df_train, df_val


def print_data_stats(df_train, df_val, tickers):
    """å°å‡ºè³‡æ–™é›†çµ±è¨ˆè³‡è¨Š"""
    print("\nğŸ“Š è³‡æ–™åˆ‡åˆ†èˆ‡é¡åˆ¥æ¯”ä¾‹çµ±è¨ˆ")
    print("-" * 60)
    print(f"{'Ticker':<8} | {'Train (N)':<10} | {'Train Pos%':<10} | {'Val (N)':<10} | {'Val Pos%':<10}")
    print("-" * 60)
    
    for tk in tickers:
        d_tr = df_train[df_train['ticker'] == tk]
        d_va = df_val[df_val['ticker'] == tk]
        tr_len = len(d_tr)
        va_len = len(d_va)
        tr_pos = d_tr['y'].mean() if tr_len > 0 else 0
        va_pos = d_va['y'].mean() if va_len > 0 else 0
        print(f"{tk:<8} | {tr_len:<10} | {tr_pos*100:6.2f}%    | {va_len:<10} | {va_pos*100:6.2f}%")
        
    print("-" * 60)
    tot_tr_len = len(df_train)
    tot_va_len = len(df_val)
    tot_tr_pos = df_train['y'].mean() if tot_tr_len > 0 else 0
    tot_va_pos = df_val['y'].mean() if tot_va_len > 0 else 0
    print(f"{'TOTAL':<8} | {tot_tr_len:<10} | {tot_tr_pos*100:6.2f}%    | {tot_va_len:<10} | {tot_va_pos*100:6.2f}%")
    print("-" * 60)


def apply_class_balancing(df_train, balance_method, seed):
    """æ ¹æ“š balance_method è™•ç†è¨“ç·´é›†çš„ Class Imbalance"""
    if balance_method == 'undersample_50_50':
        pos_df = df_train[df_train['y'] == 1]
        neg_df = df_train[df_train['y'] == 0]
        min_len = min(len(pos_df), len(neg_df))
        if min_len == 0:
            return df_train
            
        pos_sample = pos_df.sample(n=min_len, random_state=seed)
        neg_sample = neg_df.sample(n=min_len, random_state=seed)
        
        # ç¢ºä¿é †åºä¸è¢«æ‰“äº‚æˆ–è€…é‡æ’
        balanced_df = pd.concat([pos_sample, neg_sample]).sort_index()
        print(f"\nâš–ï¸  [Undersample 50/50] é‡æ–°å–æ¨£å¾Œ Train Size: {len(balanced_df)} (Pos: {len(pos_sample)}, Neg: {len(neg_sample)})")
        return balanced_df
        
    return df_train


def get_model(model_name, balance_method, seed):
    """å›å‚³æŒ‡å®šæ¨¡å‹èˆ‡æ˜¯å¦éœ€è¦åœ¨ fit() ä¸­ä½¿ç”¨ sample_weight"""
    class_weight = 'balanced' if balance_method == 'class_weight_balanced' else None
    
    if model_name == 'rf':
        model = RandomForestClassifier(n_estimators=100, max_depth=10, 
                                       random_state=seed, class_weight=class_weight, n_jobs=-1)
        return model, False # RF built-in handles class_weight

    elif model_name == 'adaboost':
        # DecisionTreeClassifier æ”¯æ´ class_weight
        base = DecisionTreeClassifier(max_depth=2, class_weight=class_weight, random_state=seed)
        model = AdaBoostClassifier(estimator=base, n_estimators=100, random_state=seed)
        return model, False

    elif model_name == 'hgb':
        # HistGradientBoostingClassifier é›–ç„¶ä¸ç›´æ¥æ”¯æ´ class_weight='balanced'
        # åœ¨ sklearn ä¸­å¯ä»¥æ”¹ç”± class_weight parameter (åœ¨ 1.3+) æˆ–æ˜¯ä½¿ç”¨ fit å‚³é sample_weight
        try:
            model = HistGradientBoostingClassifier(max_iter=100, max_depth=10, 
                                                   random_state=seed, class_weight=class_weight)
            return model, False
        except TypeError:
            # Fallback for older scikit-learn versions
            model = HistGradientBoostingClassifier(max_iter=100, max_depth=10, random_state=seed)
            return model, (class_weight == 'balanced')
            
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹ç¨®é¡: {model_name}")


def calc_metrics(y_true, y_proba, y_pred, prefix="Overall"):
    """è¨ˆç®—ä¸¦å›å‚³é©—è­‰é›†çš„å„ç¨®æŒ‡æ¨™"""
    metrics = {}
    
    # é¿å… y_true å…¨ 0 æˆ–å…¨ 1 å°è‡´ auc å¤±æ•—
    has_mixed_classes = len(np.unique(y_true)) > 1
    
    metrics['Accuracy'] = float(accuracy_score(y_true, y_pred))
    metrics['Precision'] = float(precision_score(y_true, y_pred, zero_division=0))
    metrics['Recall'] = float(recall_score(y_true, y_pred, zero_division=0))
    metrics['F1'] = float(f1_score(y_true, y_pred, zero_division=0))
    
    metrics['ROC-AUC'] = float(roc_auc_score(y_true, y_proba)) if has_mixed_classes else None
    metrics['PR-AUC'] = float(average_precision_score(y_true, y_proba)) if has_mixed_classes else None
    
    metrics['Confusion Matrix'] = confusion_matrix(y_true, y_pred).tolist()
    
    # Precision@k (Top 1%, 5%, 10%)
    sort_idx = np.argsort(y_proba)[::-1]
    sorted_y_true = np.array(y_true)[sort_idx]
    
    for k_pct in [0.01, 0.05, 0.10]:
        k = max(1, int(len(y_true) * k_pct))
        top_k_y_true = sorted_y_true[:k]
        metrics[f'Precision@{int(k_pct*100)}%'] = float(np.mean(top_k_y_true))
        
    # Threshold sweep
    metrics['Threshold Sweep'] = {}
    for th in [0.5, 0.6, 0.7, 0.8, 0.9]:
        y_pred_th = (y_proba >= th).astype(int)
        metrics['Threshold Sweep'][f'Threshold={th}'] = {
            'Precision': float(precision_score(y_true, y_pred_th, zero_division=0)),
            'Recall': float(recall_score(y_true, y_pred_th, zero_division=0)),
            'F1': float(f1_score(y_true, y_pred_th, zero_division=0))
        }
        
    return metrics


def get_feature_importances(model, model_name, X_val, y_val):
    """è¨ˆç®—ä¸¦å›å‚³ç‰¹å¾µé‡è¦æ€§"""
    importances_dict = {}
    print("\nğŸ” æ­£åœ¨è¨ˆç®— Feature Importances (Top 30)...")
    
    if model_name == 'rf':
        try:
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1]
            for i in indices[:30]:
                importances_dict[FEATURE_COLS[i]] = float(importances[i])
        except AttributeError:
            pass
    
    # å°å…¶ä»–æ¨¡å‹ä½¿ç”¨ permutation importance (é‡å° Validation Subset å–æ¨£ä»¥æ±‚æ•ˆç‡)
    if not importances_dict:
        n_samples = min(50000, len(X_val))
        idx = np.random.choice(len(X_val), n_samples, replace=False)
        X_sub = X_val.iloc[idx]
        y_sub = y_val.iloc[idx]
        
        result = permutation_importance(model, X_sub, y_sub, n_repeats=5, random_state=42, n_jobs=-1)
        importances = result.importances_mean
        indices = np.argsort(importances)[::-1]
        
        for i in indices[:30]:
            importances_dict[FEATURE_COLS[i]] = float(importances[i])
            
    return importances_dict


def main():
    args = parse_args()
    
    # æ±ºå®šåˆ‡å‰²å€æ®µ
    train_ranges = parse_date_ranges(args.train_ranges)
    val_range = (
        args.val_start_date if args.val_start_date else VAL_RANGE[0],
        args.val_end_date if args.val_end_date else VAL_RANGE[1]
    )
    
    print("=" * 60)
    print("ğŸš€ Sklearn Binary Classifier Training")
    print("=" * 60)
    print(f"  Model       : {args.model}")
    print(f"  Target      : Next_20d_Max >= 10%")
    print(f"  Tickers     : {', '.join(args.tickers)}")
    print(f"  Train Ranges: {train_ranges}")
    print(f"  Val Range   : {val_range}")
    print(f"  Balance Mode: {args.balance_train}")
    print(f"  Dry Run     : {args.dry_run}")
    print("=" * 60)
    
    # 1. æº–å‚™è³‡æ–™
    df_train, df_val = prepare_data(args, train_ranges, val_range)
    
    if len(df_train) == 0 or len(df_val) == 0:
        print("âŒ è¨“ç·´æˆ–é©—è­‰è³‡æ–™é›†ç‚ºç©ºï¼Œè«‹æª¢æŸ¥æ—¥æœŸèˆ‡è³‡æ–™ä¸‹è¼‰ç‹€æ…‹ã€‚")
        sys.exit(1)
        
    # å°å‡ºé è¨­çµ±è¨ˆåˆ†å¸ƒ
    print_data_stats(df_train, df_val, args.tickers)
    
    # å¦‚æœæ˜¯ Dry-run å°±ç›´æ¥çµæŸ
    if args.dry_run:
        print("\nâœ… Dry-Run æ¨¡å¼çµæŸã€‚")
        sys.exit(0)
    
    # 2. é¡åˆ¥å¹³è¡¡ (åƒ…åœ¨è¨“ç·´éšæ®µè™•ç†)
    df_train_b = apply_class_balancing(df_train, args.balance_train, args.seed)
    
    X_train = df_train_b[FEATURE_COLS]
    y_train = df_train_b['y']
    
    X_val = df_val[FEATURE_COLS]
    y_val = df_val['y']
    
    # 3. æº–å‚™æ¨¡å‹
    model, needs_sample_weight = get_model(args.model, args.balance_train, args.seed)
    
    sample_weight = None
    if needs_sample_weight:
        cw = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)
        w_dict = dict(zip(np.unique(y_train), cw))
        sample_weight = np.array([w_dict[y] for y in y_train])
        
    print(f"\nâš™ï¸  é–‹å§‹è¨“ç·´ {args.model.upper()} ...")
    if sample_weight is not None:
        model.fit(X_train, y_train, sample_weight=sample_weight)
    else:
        model.fit(X_train, y_train)
        
    # 4. é æ¸¬èˆ‡è¨ˆç®—æŒ‡æ¨™
    print("ğŸ“ˆ æ­£åœ¨å° Validation Subset é€²è¡Œè©•ä¼°...")
    y_proba_val = model.predict_proba(X_val)[:, 1]
    y_pred_val = model.predict(X_val)
    
    metrics = calc_metrics(y_val, y_proba_val, y_pred_val, prefix="Pooled Overall")
    
    # è¨ˆç®— Feature Importances
    importances = get_feature_importances(model, args.model, X_val, y_val)
    metrics['Feature Importances'] = importances
    
    # 5. è¼¸å‡ºå„²å­˜
    run_ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    run_dir = os.path.join(args.output_dir, f"run_{args.model}_{run_ts}")
    os.makedirs(run_dir, exist_ok=True)
    
    # (a) Model joblib
    joblib.dump(model, os.path.join(run_dir, "model.joblib"))
    
    # (b) Params Json
    params = {
        "cli_args": vars(args),
        "actual_train_ranges": train_ranges,
        "actual_val_range": val_range,
        "train_samples_raw": len(df_train),
        "train_samples_balanced": len(df_train_b),
        "val_samples": len(df_val),
        "impl_details": {
            "balance_application": "sample_weight passed to fit" if sample_weight is not None else ("class_weight arg passed" if args.balance_train == "class_weight_balanced" else args.balance_train)
        }
    }
    with open(os.path.join(run_dir, "params.json"), "w", encoding="utf-8") as f:
        json.dump(params, f, indent=4, ensure_ascii=False)
        
    # (c) Metrics Json
    with open(os.path.join(run_dir, "metrics.json"), "w", encoding="utf-8") as f:
        json.dump(metrics, f, indent=4, ensure_ascii=False)
        
    # (d) Prediction CSV
    df_val_export = df_val[['date', 'ticker']].copy()
    df_val_export['y_true'] = y_val
    df_val_export['y_proba'] = y_proba_val
    df_val_export['y_pred'] = y_pred_val
    df_val_export.to_csv(os.path.join(run_dir, "val_predictions.csv"), index=False)
    
    print("\nâœ… è¨“ç·´å®Œæˆï¼")
    print("-" * 60)
    print(f"  [Validation Metrics (Pooled / Micro)]")
    print(f"  Accuracy : {metrics['Accuracy']:.4f}")
    print(f"  ROC-AUC  : {metrics['ROC-AUC']:.4f}" if metrics['ROC-AUC'] else "  ROC-AUC  : N/A")
    print(f"  PR-AUC   : {metrics['PR-AUC']:.4f}" if metrics['PR-AUC'] else "  PR-AUC   : N/A")
    print(f"  Precision: {metrics['Precision']:.4f}")
    print(f"  Recall   : {metrics['Recall']:.4f}")
    print(f"  F1-Score : {metrics['F1']:.4f}")
    print(f"\nğŸ“‚ çµæœå·²å„²å­˜æ–¼: {run_dir}")

if __name__ == "__main__":
    main()
