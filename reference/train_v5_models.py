#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
================================================================================
V5 æ¨¡å‹è¨“ç·´è…³æœ¬ - Buy Agent å°ç¨±çå‹µç‰ˆæœ¬
================================================================================
æœ¬è…³æœ¬åŸ·è¡Œ V5 è¨“ç·´æµç¨‹ï¼š
1. æ¸…é™¤èˆŠçš„ç‰¹å¾µå¿«å–
2. åŸ·è¡Œé è¨“ç·´ (Pre-training)
3. åŸ·è¡Œå¾®èª¿ (Fine-tuning)

V5 èˆ‡ V4 çš„å·®ç•°ï¼š
- Buy Agent ä½¿ç”¨å°ç¨±çå‹µçµæ§‹ï¼š
  - è²·å° (action=1, æ¼²å¹…â‰¥10%): +1.0
  - è²·éŒ¯ (action=1, æ¼²å¹…<10%): 0.0
  - éŒ¯éå¥½æ©Ÿæœƒ (action=0, æ¼²å¹…â‰¥10%): 0.0
  - æ­£ç¢ºè¿´é¿ (action=0, æ¼²å¹…<10%): +1.0
- Sell Agent ç¶­æŒèˆ‡ V4 ç›¸åŒ

ä½œè€…ï¼šPhil Liang (Generated by Gemini)
æ—¥æœŸï¼š2025-12-27
================================================================================
"""

import os
import sys
import glob
import shutil
import pickle
import multiprocessing

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import pandas as pd

# =============================================================================
# è¨­å®š
# =============================================================================
PROJECT_PATH = os.path.dirname(os.path.abspath(__file__))
CACHE_DIR = os.path.join(PROJECT_PATH, "data", "processed")
DATA_PATH = os.path.join(PROJECT_PATH, "data", "raw")
V5_MODELS_PATH = os.path.join(PROJECT_PATH, "models_hybrid_v5")
V5_RESULTS_PATH = os.path.join(PROJECT_PATH, "results_hybrid_v5")
# V6.02: ä¸é€£çºŒçš„è¨“ç·´é›† + ç¨ç«‹é©—è­‰é›†
TRAIN_RANGES = [
    ("2000-01-01", "2017-10-15"),
    ("2023-10-16", "2025-12-31")
]
VAL_RANGE = ("2017-10-16", "2023-10-15")

# V5: è¨“ç·´æ­¥æ•¸è¨­å®š
PRETRAIN_BUY_STEPS = 10_000_000
PRETRAIN_SELL_STEPS = 10_000_000  # [User Request] Increase to 1M
FINETUNE_BUY_STEPS = 10_000_000
FINETUNE_SELL_STEPS = 10_000_000  # [User Request] Increase to 1M


def filter_by_ranges(df, ranges):
    """æ ¹æ“šæ—¥æœŸç¯„åœåˆ—è¡¨éæ¿¾ DataFrame (V6.02)"""
    mask = pd.Series(False, index=df.index)
    for start, end in ranges:
        mask |= (df.index >= pd.Timestamp(start)) & (df.index <= pd.Timestamp(end))
    return df[mask]


def clear_cache():
    print("\n" + "=" * 60)
    print("Step A: Clear Cache")
    print("=" * 60)
    if not os.path.exists(CACHE_DIR):
        print(f"[Info] Cache dir not found: {CACHE_DIR}")
        return
    pkl_files = glob.glob(os.path.join(CACHE_DIR, "*.pkl"))
    if not pkl_files:
        print("[Info] No .pkl cache files found")
        return
    print(f"[Info] Found {len(pkl_files)} cache files")
    for f in pkl_files:
        try:
            os.remove(f)
            print(f"  Deleted: {os.path.basename(f)}")
        except Exception as e:
            print(f"  Failed: {os.path.basename(f)} - {e}")
    print("[Done] Cache cleared")


def run_pretraining_v5():
    """V5 é è¨“ç·´ï¼šBuy Agent ä½¿ç”¨ BuyEnvHybridV5"""
    print("\n" + "=" * 60)
    print("Step B: Pre-training (V5 - Symmetric Reward)")
    print("=" * 60)
    print(f"  Buy Agent:  {PRETRAIN_BUY_STEPS:,} steps (BuyEnvHybridV5)")
    print(f"  Sell Agent: {PRETRAIN_SELL_STEPS:,} steps")
    print(f"  Output:     {V5_MODELS_PATH}")
    print("=" * 60)
    
    os.makedirs(V5_MODELS_PATH, exist_ok=True)
    os.makedirs(DATA_PATH, exist_ok=True)
    
    import ptrl_hybrid_system as hybrid
    import torch
    from stable_baselines3 import PPO
    from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
    
    # [v6.0] å¼·åˆ¶ä½¿ç”¨ CPU (å°å‹ MLP + å¤šç’°å¢ƒ PPO åœ¨ CPU æ›´å¿«)
    device = "cpu"
    print(f"[System] Device: {device} (forced for multi-env PPO)")
    
    # [v6.0] LSTM å·²ç§»é™¤ï¼Œä¸å†è¼‰å…¥ LSTM æ¨¡å‹
    
    print("\n[Data] Downloading global index data...")
    # V6.02: ä¸‹è¼‰å…¨éƒ¨è³‡æ–™ï¼Œä¸é™åˆ¶ end_date
    raw_data = hybrid.fetch_index_data(DATA_PATH, start_date="2000-01-01")
    
    print("\n[Data] Computing features...")
    train_data = {}
    benchmark_df = raw_data.get("^TWII")
    for ticker, df in raw_data.items():
        try:
            processed = hybrid.calculate_features(df, benchmark_df, ticker, use_cache=False)
            # V6.02: åªä¿ç•™è¨“ç·´é›†ç¯„åœå…§çš„è³‡æ–™
            filtered = filter_by_ranges(processed, TRAIN_RANGES)
            if len(filtered) > 100:
                train_data[ticker] = filtered
                print(f"  OK {ticker}: {len(filtered)} rows (filtered from {len(processed)})")
        except Exception as e:
            print(f"  WARN {ticker}: {e}")
    
    # æª¢æŸ¥ Buy å’Œ Sell Base æ¨¡å‹
    buy_done = os.path.exists(os.path.join(V5_MODELS_PATH, "ppo_buy_base.zip"))
    sell_done = os.path.exists(os.path.join(V5_MODELS_PATH, "ppo_sell_base.zip"))
    
    print(f"\n[Training] Starting pre-training...")
    print(f"   Buy Agent (V5):  {'â­ï¸ SKIP' if buy_done else 'âœ… Train'}")
    print(f"   Sell Agent:      {'â­ï¸ SKIP' if sell_done else 'âœ… Train'}")
    
    tensorboard_log = "./tensorboard_logs/"
    os.makedirs(tensorboard_log, exist_ok=True)
    os.makedirs(os.path.join(V5_MODELS_PATH, "best_pretrain"), exist_ok=True)
    
    n_envs = min(8, max(1, multiprocessing.cpu_count() - 1))
    
    ppo_params = {
        "learning_rate": 0.0001,
        "n_steps": max(128, 2048 // n_envs),
        "batch_size": 512,
        "ent_coef": 0.01,
        "device": device,
        "policy_kwargs": dict(net_arch=[64, 64, 64]),
        "verbose": 1,
        "tensorboard_log": tensorboard_log
    }
    
    # =========================================================================
    # Buy Agent V5
    # =========================================================================
    if not buy_done:
        print("\nğŸ›’ Training Buy Agent V5 (Symmetric Reward - Base Model)...")
        
        # ä½¿ç”¨ BuyEnvHybridV5
        buy_env = make_vec_env(hybrid.BuyEnvHybridV5, n_envs=n_envs, vec_env_cls=SubprocVecEnv,
                               env_kwargs={'data_dict': train_data, 'is_training': True})
        
        eval_buy_env = make_vec_env(hybrid.BuyEnvHybridV5, n_envs=1, vec_env_cls=DummyVecEnv,
                                    env_kwargs={'data_dict': train_data, 'is_training': False})
        
        buy_model = PPO("MlpPolicy", buy_env, **ppo_params)
        
        buy_callbacks = CallbackList([
            CheckpointCallback(save_freq=80000, save_path=V5_MODELS_PATH, name_prefix="ppo_buy_base"),
            EvalCallback(eval_buy_env, best_model_save_path=os.path.join(V5_MODELS_PATH, "best_pretrain", "buy"),
                         log_path="./logs/", eval_freq=10000, n_eval_episodes=50, deterministic=True)
        ])
        
        buy_model.learn(total_timesteps=PRETRAIN_BUY_STEPS, callback=buy_callbacks, tb_log_name="buy_v5_pretrain")
        
        best_buy_path = os.path.join(V5_MODELS_PATH, "best_pretrain", "buy", "best_model.zip")
        buy_base_path = os.path.join(V5_MODELS_PATH, "ppo_buy_base.zip")
        if os.path.exists(best_buy_path):
            shutil.copy(best_buy_path, buy_base_path)
            print(f"[Pre-train] âœ… Buy Agent V5: Copied BEST model to {buy_base_path}")
        else:
            buy_model.save(os.path.join(V5_MODELS_PATH, "ppo_buy_base"))
            print(f"[Pre-train] âš ï¸ Buy Agent V5: Best model not found, saved last step model")
        
        buy_env.close()
        eval_buy_env.close()
    else:
        print("\n[Skip] Buy Agent V5 pre-training (already done)")
    
    # =========================================================================
    # Sell Agent (èˆ‡ V4 ç›¸åŒ)
    # =========================================================================
    if not sell_done:
        print("\nğŸ’° Training Sell Agent (Base Model)...")
        sell_env = make_vec_env(hybrid.SellEnvHybrid, n_envs=n_envs, vec_env_cls=SubprocVecEnv,
                                env_kwargs={'data_dict': train_data})
        
        eval_sell_env = make_vec_env(hybrid.SellEnvHybrid, n_envs=1, vec_env_cls=DummyVecEnv,
                                     env_kwargs={'data_dict': train_data})
        
        sell_model = PPO("MlpPolicy", sell_env, **ppo_params)
        
        sell_callbacks = CallbackList([
            CheckpointCallback(save_freq=80000, save_path=V5_MODELS_PATH, name_prefix="ppo_sell_base"),
            EvalCallback(eval_sell_env, best_model_save_path=os.path.join(V5_MODELS_PATH, "best_pretrain"),
                         log_path="./logs/", eval_freq=10000, n_eval_episodes=50, deterministic=True)
        ])
        
        sell_model.learn(total_timesteps=PRETRAIN_SELL_STEPS, callback=sell_callbacks, tb_log_name="sell_v5_pretrain")
        
        best_sell_path = os.path.join(V5_MODELS_PATH, "best_pretrain", "best_model.zip")
        sell_base_path = os.path.join(V5_MODELS_PATH, "ppo_sell_base.zip")
        if os.path.exists(best_sell_path):
            shutil.copy(best_sell_path, sell_base_path)
            print(f"[Pre-train] âœ… Sell Agent: Copied BEST model to {sell_base_path}")
        else:
            sell_model.save(os.path.join(V5_MODELS_PATH, "ppo_sell_base"))
            print(f"[Pre-train] âš ï¸ Sell Agent: Best model not found, saved last step model")
        
        sell_env.close()
        eval_sell_env.close()
    else:
        print("\n[Skip] Sell Agent pre-training (already done)")
    
    print("[System] Pre-training Completed.")


def run_finetuning_v5(train_buy=True, train_sell=True):
    """V5 å¾®èª¿ï¼šBuy Agent ä½¿ç”¨ BuyEnvHybridV5"""
    print("\n" + "=" * 60)
    print("Step C: Fine-tuning (V5 - Symmetric Reward)")
    print("=" * 60)
    print(f"  Buy Agent:  {FINETUNE_BUY_STEPS:,} steps {'âœ…' if train_buy else 'â­ï¸ SKIP'}")
    print(f"  Sell Agent: {FINETUNE_SELL_STEPS:,} steps {'âœ…' if train_sell else 'â­ï¸ SKIP'}")
    print(f"  Output:     {V5_MODELS_PATH}")
    print("=" * 60)
    
    import ptrl_hybrid_system as hybrid
    import torch
    from datetime import datetime
    from stable_baselines3 import PPO
    from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv
    from stable_baselines3.common.logger import configure
    from stable_baselines3.common.utils import get_schedule_fn
    
    # [v6.0] å¼·åˆ¶ä½¿ç”¨ CPU (å°å‹ MLP + å¤šç’°å¢ƒ PPO åœ¨ CPU æ›´å¿«)
    device = "cpu"
    print(f"[System] Device: {device} (forced for multi-env PPO)")
    
    # [v6.0] LSTM å·²ç§»é™¤ï¼Œä¸å†è¼‰å…¥ LSTM æ¨¡å‹
    
    print("\n[Data] Preparing ^TWII data...")
    cache_path = os.path.join(CACHE_DIR, "_TWII_features.pkl")
    
    if os.path.exists(cache_path):
        print(f"[Cache] Loading ^TWII features...")
        with open(cache_path, 'rb') as f:
            twii_full_df = pickle.load(f)
    else:
        print("[Compute] Loading ^TWII from local CSV...")
        twii_raw = hybrid._load_local_twii_data(start_date="2000-01-01")
        twii_full_df = hybrid.calculate_features(twii_raw, twii_raw, ticker="^TWII", use_cache=True)
    
    print(f"[Data] ^TWII data: {len(twii_full_df)} rows")
    
    # V6.02: è¨“ç·´é›† = TRAIN_RANGES, é©—è­‰é›† = VAL_RANGE
    twii_finetune_df = filter_by_ranges(twii_full_df, TRAIN_RANGES)
    val_start, val_end = VAL_RANGE
    twii_eval_df = twii_full_df[
        (twii_full_df.index >= pd.Timestamp(val_start)) & 
        (twii_full_df.index <= pd.Timestamp(val_end))
    ]
    
    print(f"  Fine-tuning set: {len(twii_finetune_df)} rows (TRAIN_RANGES)")
    print(f"  Eval set:        {len(twii_eval_df)} rows ({val_start} ~ {val_end})")
    
    finetune_data = {'^TWII': twii_finetune_df}
    eval_data = {'^TWII': twii_eval_df}
    
    tensorboard_log = "./tensorboard_logs/"
    os.makedirs(tensorboard_log, exist_ok=True)
    os.makedirs(os.path.join(V5_MODELS_PATH, "best_tuned"), exist_ok=True)
    os.makedirs("./logs/", exist_ok=True)
    
    n_envs = min(4, max(1, multiprocessing.cpu_count() - 1))
    
    finetune_params = {
        "learning_rate": 5e-6,  # [Fix] Reduce from 1e-5 for stable fine-tune
        "n_steps": 256,
        "batch_size": 128,
        "ent_coef": 0.01,  # [Fix] Increase from 0.005 to prevent premature convergence
        "device": device,
        "verbose": 1
    }
    
    # =========================================================================
    # Fine-tune Buy Agent V5
    # =========================================================================
    if train_buy:
        print("\n[Fine-tune] Loading ppo_buy_base and fine-tuning for ^TWII (V5)...")
        
        buy_base_path = os.path.join(V5_MODELS_PATH, "ppo_buy_base.zip")
        if not os.path.exists(buy_base_path):
            print(f"[Error] Base model not found: {buy_base_path}")
            return None, None
        
        # ä½¿ç”¨ BuyEnvHybridV5
        buy_env = make_vec_env(hybrid.BuyEnvHybridV5, n_envs=n_envs, vec_env_cls=SubprocVecEnv,
                               env_kwargs={'data_dict': finetune_data, 'is_training': True})
        
        eval_buy_env = make_vec_env(hybrid.BuyEnvHybridV5, n_envs=1, vec_env_cls=DummyVecEnv,
                                    env_kwargs={'data_dict': eval_data, 'is_training': False})
        
        buy_model = PPO.load(buy_base_path, env=buy_env, device=device)
        # å»ºç«‹ç¨ç«‹çš„ TensorBoard Logger (æ¯æ¬¡è¨“ç·´é–‹æ–°è³‡æ–™å¤¾)
        buy_log_name = f"buy_v5_finetune_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        buy_logger = configure(os.path.join(tensorboard_log, buy_log_name), ["tensorboard", "stdout"])
        buy_model.set_logger(buy_logger)
        buy_logger = configure(os.path.join(tensorboard_log, buy_log_name), ["tensorboard", "stdout"])
        buy_model.set_logger(buy_logger)
        buy_model.learning_rate = finetune_params["learning_rate"]
        buy_model.lr_schedule = get_schedule_fn(buy_model.learning_rate)  # [Fix] Update schedule
        buy_model.ent_coef = finetune_params["ent_coef"]
        
        buy_callbacks = CallbackList([
            CheckpointCallback(save_freq=100000, save_path=V5_MODELS_PATH, name_prefix="ppo_buy_finetune"),
            EvalCallback(eval_buy_env, best_model_save_path=os.path.join(V5_MODELS_PATH, "best_tuned", "buy"),
                         log_path="./logs/", eval_freq=10000, n_eval_episodes=100, deterministic=True)  # [Fix] 30->100
        ])
        
        print(f"[Fine-tune] Training Buy Agent V5 for {FINETUNE_BUY_STEPS:,} steps (log: {buy_log_name})")
        buy_model.learn(total_timesteps=FINETUNE_BUY_STEPS, callback=buy_callbacks,
                        reset_num_timesteps=False)
        
        best_buy_path = os.path.join(V5_MODELS_PATH, "best_tuned", "buy", "best_model.zip")
        buy_final_path = os.path.join(V5_MODELS_PATH, "ppo_buy_twii_final.zip")
        if os.path.exists(best_buy_path):
            shutil.copy(best_buy_path, buy_final_path)
            print(f"[Fine-tune] âœ… Buy Agent V5: Copied BEST model to {buy_final_path}")
        else:
            buy_model.save(os.path.join(V5_MODELS_PATH, "ppo_buy_twii_final"))
            print(f"[Fine-tune] âš ï¸ Buy Agent V5: Best model not found, saved last step model")
        
        buy_env.close()
        eval_buy_env.close()
    else:
        print("\n[Skip] Buy Agent V5 fine-tuning (train_buy=False)")
    
    # =========================================================================
    # Fine-tune Sell Agent (èˆ‡ V4 ç›¸åŒ)
    # =========================================================================
    if train_sell:
        print("\n[Fine-tune] Loading ppo_sell_base and fine-tuning for ^TWII...")
        
        sell_base_path = os.path.join(V5_MODELS_PATH, "ppo_sell_base.zip")
        sell_env = make_vec_env(hybrid.SellEnvHybrid, n_envs=n_envs, vec_env_cls=SubprocVecEnv,
                                env_kwargs={'data_dict': finetune_data})
        
        eval_sell_env = make_vec_env(hybrid.SellEnvHybrid, n_envs=1, vec_env_cls=DummyVecEnv,
                                     env_kwargs={'data_dict': eval_data})
        
        sell_model = PPO.load(sell_base_path, env=sell_env, device=device)
        # å»ºç«‹ç¨ç«‹çš„ TensorBoard Logger (æ¯æ¬¡è¨“ç·´é–‹æ–°è³‡æ–™å¤¾)
        sell_log_name = f"sell_v5_finetune_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        sell_logger = configure(os.path.join(tensorboard_log, sell_log_name), ["tensorboard", "stdout"])
        sell_model.set_logger(sell_logger)
        sell_logger = configure(os.path.join(tensorboard_log, sell_log_name), ["tensorboard", "stdout"])
        sell_model.set_logger(sell_logger)
        sell_model.learning_rate = finetune_params["learning_rate"]
        sell_model.lr_schedule = get_schedule_fn(sell_model.learning_rate)  # [Fix] Update schedule
        sell_model.ent_coef = finetune_params["ent_coef"]
        
        sell_callbacks = CallbackList([
            CheckpointCallback(save_freq=50000, save_path=V5_MODELS_PATH, name_prefix="ppo_sell_finetune"),
            EvalCallback(eval_sell_env, best_model_save_path=os.path.join(V5_MODELS_PATH, "best_tuned", "sell"),
                         log_path="./logs/", eval_freq=10000, n_eval_episodes=100, deterministic=True)  # [Fix] 30->100
        ])
        
        print(f"[Fine-tune] Training Sell Agent for {FINETUNE_SELL_STEPS:,} steps (log: {sell_log_name})")
        sell_model.learn(total_timesteps=FINETUNE_SELL_STEPS, callback=sell_callbacks,
                         reset_num_timesteps=False)
        
        best_sell_path = os.path.join(V5_MODELS_PATH, "best_tuned", "sell", "best_model.zip")
        sell_final_path = os.path.join(V5_MODELS_PATH, "ppo_sell_twii_final.zip")
        if os.path.exists(best_sell_path):
            shutil.copy(best_sell_path, sell_final_path)
            print(f"[Fine-tune] âœ… Sell Agent: Copied BEST model to {sell_final_path}")
        else:
            sell_model.save(os.path.join(V5_MODELS_PATH, "ppo_sell_twii_final"))
            print(f"[Fine-tune] âš ï¸ Sell Agent: Best model not found, saved last step model")
        
        sell_env.close()
        eval_sell_env.close()
    else:
        print("\n[Skip] Sell Agent fine-tuning (train_sell=False)")
    
    print("\n[System] Fine-tuning Completed!")


def run_backtesting_v5():
    print("\n" + "=" * 60)
    print("Step D: Backtesting (V5)")
    print("=" * 60)
    print(f"  Output: {V5_RESULTS_PATH}")
    print("=" * 60)
    
    os.makedirs(V5_RESULTS_PATH, exist_ok=True)
    
    import ptrl_hybrid_system as hybrid
    from stable_baselines3 import PPO
    
    # [v6.0] LSTM å·²ç§»é™¤ï¼Œä¸å†è¼‰å…¥ LSTM æ¨¡å‹
    
    print("\n[Data] Preparing ^TWII data...")
    print("[Compute] Loading ^TWII from local CSV (always recalculate)...")
    twii_raw = hybrid._load_local_twii_data(start_date="2000-01-01")
    twii_full_df = hybrid.calculate_features(twii_raw, twii_raw, ticker="^TWII", use_cache=False)
    
    # V6.02: å›æ¸¬ä½¿ç”¨é©—è­‰é›†ç¯„åœ
    val_start, val_end = VAL_RANGE
    twii_backtest_df = twii_full_df[
        (twii_full_df.index >= pd.Timestamp(val_start)) & 
        (twii_full_df.index <= pd.Timestamp(val_end))
    ]
    
    print(f"[Data] Backtest period: {twii_backtest_df.index[0].strftime('%Y-%m-%d')} ~ {twii_backtest_df.index[-1].strftime('%Y-%m-%d')}")
    print(f"[Data] Backtest data: {len(twii_backtest_df)} rows")
    
    print("\n[Model] Loading V5 Fine-tuned models...")
    buy_model = PPO.load(os.path.join(V5_MODELS_PATH, "ppo_buy_twii_final.zip"))
    sell_model = PPO.load(os.path.join(V5_MODELS_PATH, "ppo_sell_twii_final.zip"))
    
    print("\n[Backtest] Running...")
    metrics = hybrid.run_backtesting(twii_backtest_df, buy_model, sell_model, V5_RESULTS_PATH, twii_full_df)
    
    print(f"\n[Done] Backtest complete! Results saved to {V5_RESULTS_PATH}/")
    return metrics


def check_pretrain_buy_complete():
    return os.path.exists(os.path.join(V5_MODELS_PATH, "ppo_buy_base.zip"))

def check_pretrain_sell_complete():
    return os.path.exists(os.path.join(V5_MODELS_PATH, "ppo_sell_base.zip"))

def check_pretrain_complete():
    return check_pretrain_buy_complete() and check_pretrain_sell_complete()

def check_finetune_buy_complete():
    return os.path.exists(os.path.join(V5_MODELS_PATH, "ppo_buy_twii_final.zip"))

def check_finetune_sell_complete():
    return os.path.exists(os.path.join(V5_MODELS_PATH, "ppo_sell_twii_final.zip"))

def check_finetune_complete():
    return check_finetune_buy_complete() and check_finetune_sell_complete()

def check_backtest_complete():
    return os.path.exists(os.path.join(V5_RESULTS_PATH, "final_performance.png"))


def print_next_steps():
    print("\n" + "=" * 60)
    print("âœ… V5 æ¨¡å‹è¨“ç·´æµç¨‹å·²å®Œæˆï¼")
    print("=" * 60)
    print(f"""
ğŸ“ æ¨¡å‹å„²å­˜ä½ç½®:
   {V5_MODELS_PATH}/ppo_buy_twii_final.zip
   {V5_MODELS_PATH}/ppo_sell_twii_final.zip

ğŸ“Š V5 ç‰¹é» (å°ç¨±çå‹µç‰ˆ):
   - Buy Agent çå‹µçµæ§‹:
     * è²·å° (æ¼²å¹…â‰¥10%): +1.0
     * è²·éŒ¯ (æ¼²å¹…<10%): 0.0
     * éŒ¯é (æ¼²å¹…â‰¥10%): 0.0
     * æ­£ç¢ºè¿´é¿ (æ¼²å¹…<10%): +1.0
""")


def main():
    print("=" * 60)
    print("V5 Model Training Script (Symmetric Reward)")
    print(f"   Buy Fine-tune: {FINETUNE_BUY_STEPS:,} steps | Sell Fine-tune: {FINETUNE_SELL_STEPS:,} steps")
    print("=" * 60)
    
    # æª¢æŸ¥ Buy å’Œ Sell Base æ¨¡å‹
    buy_base_done = check_pretrain_buy_complete()
    sell_base_done = check_pretrain_sell_complete()
    
    if not (buy_base_done and sell_base_done):
        clear_cache()
    else:
        print("\n[Skip] Step A: Cache clearing (pre-training done)")
    
    if buy_base_done and sell_base_done:
        print("\n" + "=" * 60)
        print("[Skip] Step B: Pre-training complete")
        print(f"   Buy Base:  {V5_MODELS_PATH}/ppo_buy_base.zip âœ…")
        print(f"   Sell Base: {V5_MODELS_PATH}/ppo_sell_base.zip âœ…")
        print("=" * 60)
    else:
        print("\n" + "=" * 60)
        print("[Check] Step B: Pre-training status")
        print(f"   Buy Base:  {'âœ… Done' if buy_base_done else 'âŒ Missing'}")
        print(f"   Sell Base: {'âœ… Done' if sell_base_done else 'âŒ Missing'}")
        print("=" * 60)
        run_pretraining_v5()
    
    # æª¢æŸ¥ Fine-tune
    buy_done = check_finetune_buy_complete()
    sell_done = check_finetune_sell_complete()
    
    if buy_done and sell_done:
        print("\n" + "=" * 60)
        print("[Skip] Step C: Fine-tuning complete")
        print(f"   Buy Final:  {V5_MODELS_PATH}/ppo_buy_twii_final.zip âœ…")
        print(f"   Sell Final: {V5_MODELS_PATH}/ppo_sell_twii_final.zip âœ…")
        print("=" * 60)
    else:
        print("\n" + "=" * 60)
        print("[Check] Step C: Fine-tuning status")
        print(f"   Buy Final:  {'âœ… Done' if buy_done else 'âŒ Missing'}")
        print(f"   Sell Final: {'âœ… Done' if sell_done else 'âŒ Missing'}")
        print("=" * 60)
        run_finetuning_v5(train_buy=not buy_done, train_sell=not sell_done)
    
    if check_backtest_complete():
        print("\n" + "=" * 60)
        print("[Skip] Step D: Backtest complete")
        print(f"   Result: {V5_RESULTS_PATH}/final_performance.png")
        print("=" * 60)
    else:
        run_backtesting_v5()
    
    print_next_steps()


if __name__ == "__main__":
    main()
